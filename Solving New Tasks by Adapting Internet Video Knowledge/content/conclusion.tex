\vspace{-0.2em}
\section{Conclusion and Future Work}

In this work, we investigate how internet video knowledge can be adapted with a small amount of in-domain video demonstrations to achieve novel task generalization for downstream robotics, which we generally acronymize as Adapt2Act.  We explore several methods through which internet-scale video models may be adapted to model the appearance and dynamics of novel environments, and subsequently utilized to perform new behaviors or accomplish unseen tasks conditioned on natural language.  Our considered adaptation techniques vary in their data and resource requirements.  We have conducted extensive evaluations on MetaWorld and DeepMind Control Suite tasks under both policy supervision and visual planning setups.  In particular, our proposed \textit{Inverse Probabilistic Adaptation} approach demonstrates strong generalization capabilities across different task settings, and remains robust when only suboptimal demonstrations are available. Our findings highlight the promise of leveraging internet-scale text-to-video priors to model domain-specific robotic behaviors through data-efficient adaptation, which enables further advancements on text-conditioned generalization for embodied intelligence.


\textbf{Limitations and Future Work.} When the same adapted model is utilized to solve the tasks under different setups, performance differences that are difficult to explain may arise. For example, while inverse probabilistic adaptation outperforms its inverse version significantly under the policy supervision setup, both achieve similar success rates under the video planning setup. Additionally, a natural next step to further study is the use of suboptimal data, which can be collected cheaply by running random policies on tasks of interest, or iteratively augmented from on-policy observations.

\textbf{Acknowledgments.} This work is supported by Samsung and NASA. Our research was conducted using computational resources at the Center for Computation and Visualization at Brown University. Chen would like to thank Mia for inspiration.

\section{Reproducibility}
All adaptation techniques in our work are implemented using available open-sourced components. As mentioned in Section~\ref{sec:experimental_setup}, we use publicly available checkpoints for pretrained large models, such as AnimateDiff~\citep{guo2023animatediff} as well as StableDiffusion~\citep{rombach2022high}. We utilize DreamBooth~\citep{ruiz2023dreambooth} for subject customization. Furthermore, we reuse the codebase provided by the authors of AVDC~\citep{ko2024avdc} for in-domain model training, with minimal adjustments to enable the latent diffusion. For policy learning, we follow Video-TADPoLe~\citep{luo2024text} framework, which itself is built off of publicly available AnimateDiff and TDMPC~\citep{hansen2022temporal}. Furthermore, we release detailed hyperparameter settings and implementation details in Appendix~\ref{sec:implementation_details}, as well as elaborate on techniques on how they were discovered or selected in Appendix~\ref{sec:policy_discrimination}.  We believe that the simplicity of our approach, along with the utilization of open-sourced checkpoints, makes this work highly reproducible.  We also commit to open-sourcing our code, to support further reproducibility efforts in the community.

