\section{Experiments}

\subsection{Experimental Setup and Evaluation}
\label{sec:experimental_setup}


\textbf{Benchmarks:} We evaluate to what degree adapted video models can facilitate downstream robotic behavior generalization across a variety of environments and tasks, spanning robotic manipulation to continuous control. We focus the bulk of our explorations on MetaWorld-v2~\citep{yu2020meta}, which offers a suite of robotic manipulation tasks with different levels of complexity. This benchmark allows us to thoroughly assess the generalization capabilities of adaptation methods across a wide selection of tasks. To study the effectiveness of adaptation techniques in a \textit{low data} regime, we curate a small dataset of in-domain examples from 7 MetaWorld tasks (denoted with an asterisk in Table~\ref{table:text_prompts}) to adapt pretrained video models. For each task, we utilize 25 expert videos for direct finetuning and probabilistic adaptation, while sampling a small set of non-consecutive observations for subject customization. During inference, we evaluate the adapted video models on 9 tasks, 7 of which are novel tasks that are not exposed during adaptation (denoted with no asterisk in Table~\ref{table:text_prompts}). 

Additionally, we extend our evaluation to Humanoid and Dog environments from the DeepMind Control Suite~\citep{tassa2018deepmind}. We select ``Dog walking" and ``Humanoid walking", which offer quantitative evaluation through ground-truth rewards, as tasks where we collect 20 demonstrations for adaptation. Following Video-TADPoLe~\citep{luo2024text}, we evaluate the adapted models on walking as well as other behavior achievement tasks specified by novel text prompts. We provide a detailed list of text prompts used for both adaptation and task evaluation in Table~\ref{table:text_prompts}.

\textbf{Implementation details of adaptation:} In our experiments, we use AnimateDiff~\citep{guo2023animatediff} ($\sim$1.5B parameters) as our pretrained text-to-video model, which combines StableDiffusion with a motion module pretrained on WebVid-10M~\citep{bain2021webvid} for high-quality video generation. To perform direct finetuning on AnimateDiff, we follow the training pipeline provided by the authors and only update the parameters of its motion module with a small in-domain dataset. For subject customization, we utilize 20 static images to finetune StableDiffusion with DreamBooth for each environment. In addition, we adopt DreamBooth LoRA~\citep{hu2022lora} for lower memory usage and better training efficiency. In probabilistic adaptation, we implement our small in-domain video model based on AVDC~\citep{ko2024avdc}, a text-to-video model that diffuses over pixel space; implemented using $\sim$109M parameters, this is comparable in size to that of the small models used in prior work~\citep{yang2023probabilistic}. To enable direct score composition between the in-domain model and AnimateDiff, we modify the AVDC model to diffuse over the same latent space used by StableDiffusion. We include detailed hyperparameters for in-domain model training in Appendix~\ref{sec:implementation_details}.

\textbf{Evaluation metrics:} For robotic manipulation tasks in MetaWorld, we report the ``success rate'', computed as the proportion of evaluation rollouts in which the agent successfully completes the given task. For Dog and Humanoid evaluation, we follow the setup in Video-TADPoLe~\citep{luo2024text} to report quantitative performance for walking tasks, which have ground-truth reward functions, while providing qualitative results for novel behavior achievement where ground-truth reward functions are unavailable. As in prior work~\citep{yang2023probabilistic}, we also use FVD~\citep{thomas2018fvd} to measure the visual quality of videos generated by adapted video models.




\begin{table*}
\centering
\small

\setlength{\tabcolsep}{4.8pt}
\resizebox{\textwidth}{!}{\begin{tabular}{lccccccc}\toprule
Episode Return & Vanilla AnimateDiff & In-Domain-Only & Direct Finetuning & Subject Customization & Prob. Adaptation & Inverse Prob. Adaptation \\
\midrule
Humanoid Walking & 145.8 $\pm$ 48.2 & 2.4 $\pm$ 0.3 & 111.5 $\pm$ 106.4 & 174.7 $\pm$ 42.7  & 1.8 $\pm$ 0.2 & 92.6 $\pm$ 51.1  \\
Dog Walking & 60.2 $\pm$ 8.8 & 76.2 $\pm$ 29.5 & 44.6 $\pm$ 44.3 & 117.9 $\pm$ 49.7 & 11.3 $\pm$ 0.9 & 88.7 $\pm$ 9.0 \\
\midrule
Overall & 103 & 39.3 & 78.1 & \textbf{146.3} & 6.5 & 90.7  \\
\bottomrule
\end{tabular}}
\vspace{-5pt}
\caption[]{\textbf{Policy Supervision on Continuous Control.} We report ground-truth episode return achieved by policies optimized using the listed adapted video models, aggregated over 5 seeds.  We observe that direct finetuning produces marginal improvement over a vanilla AnimateDiff model, and surprisingly, despite adaptation on just static images, subject customization is able to substantially improve continuous locomotion performance over the base pretrained video model.}
\label{table:dmc_videotadpole}
\vspace{-5pt}
\end{table*}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/dog_jumping_figure.pdf}
    \vspace{-15pt}
    \caption{\textbf{Novel Text-Conditioned Generalization.}  In the top row, we visualize a free-form video generation from a directly finetuned AnimateDiff model for the novel text prompt ``a dog jumping‚Äù.  This was a behavior unseen during adaptation.  When using this adapted video model for policy supervision, we showcase that it can successfully supervise a downstream Dog agent to behave according to novel text specifications in a zero-shot manner (policy rollout shown in bottom row).}
    \label{fig:dog_jumping_rollout}
    \vspace{-15pt}
\end{figure}

\subsection{Policy Supervision}

\begin{table*}[t]
\centering
\small

\setlength{\tabcolsep}{4.8pt}
  \scalebox{0.9}{\begin{tabular}{lccccc}\toprule
Success Rate (\%) w/ & Door Close$^*$ & Door Open & Window Close & Window Open & Drawer Close  \\
\midrule
In-Domain-Only & 100.0 $\pm$ 0.0  & 31.1 $\pm$ 44.0  & 0.0 $\pm$ 0.0  & 33.3 $\pm$ 47.1  & 74.4 $\pm$ 36.2    \\
Vanilla AnimateDiff & 100.0 $\pm$ 0.0  & 0.0 $\pm$ 0.0  & 33.3 $\pm$ 47.1  & 31.1 $\pm$ 44.0   & 98.9 $\pm$ 1.5    \\
\midrule
Direct Finetuning  & 100.0 $\pm$ 0.0 & 0.0 $\pm$ 0.0 & 0.0 $\pm$ 0.0  & 47.8 $\pm$ 41.4  & 95.6 $\pm$ 7.7    \\
Subject Customization  & 100.0 $\pm$ 0.0  & 0.0 $\pm$ 0.0 & 100.0 $\pm$ 0.0  & 60.0 $\pm$ 42.5  & 100.0 $\pm$ 0.0   \\
Prob. Adaptation & 95.6 $\pm$ 7.7  & 30.0 $\pm$ 52.0 & 33.3 $\pm$ 57.7  & 100.0 $\pm$ 0.0 & 100.0 $\pm$ 0.0   \\
Inverse Prob. Adaptation & 97.8 $\pm$ 3.8 & 65.6 $\pm$ 56.8  & 98.9 $\pm$ 1.9  & 98.9 $\pm$ 1.9  & 100.0 $\pm$ 0.0    \\
\midrule
\midrule
Success Rate (\%) w/ & Drawer Open & Coffee Push$^*$ & Soccer & Button Press &  \textbf{Overall}  \\
\midrule
In-Domain-Only & 0.0 $\pm$ 0.0 & 0.0 $\pm$ 0.0 & 0.0 $\pm$ 0.0 & 33.3 $\pm$ 47.1  & 30.2    \\
Vanilla AnimateDiff & 33.3 $\pm$ 47.1 & 28.9 $\pm$ 23.2  & 0.0 $\pm$ 0.0  & 33.3 $\pm$ 47.1  &  39.8    \\
\midrule
Direct Finetuning & 0.0 $\pm$ 0.0 & 30.0 $\pm$ 26.0 & 5.6 $\pm$ 9.6  & 0.0 $\pm$ 0.0 &  31.0    \\
Subject Customization & 0.0 $\pm$ 0.0 & 15.6 $\pm$ 22.0  & 20.0 $\pm$ 17.8 & 0.0 $\pm$ 0.0 &  44.0    \\
Prob. Adaptation & 0.0 $\pm$ 0.0 & 1.1 $\pm$ 1.9  & 2.2 $\pm$ 3.8  & 0.0 $\pm$ 0.0  &  40.2     \\
Inverse Prob. Adaptation & 66.7 $\pm$ 57.7 & 13.3 $\pm$ 23.0 & 6.7 $\pm$ 6.7 & 66.7 $\pm$ 57.7  &  \textbf{68.3}   \\
\bottomrule
\end{tabular}}
\vspace{-5pt}
\caption[]{\textbf{Policy Supervision on MetaWorld.} We report the mean success rate across 9 manipulation tasks in MetaWorld, over 3 seeds.
``$*$'' denotes seen tasks during adaptation.  We observe that inverse probabilistic adaptation achieves the highest overall performance, both in averaged success rate over the entire task suite, as well as successful generalization to the highest number of novel tasks.  Subject customization also achieves surprisingly high aggregate success given its cheap data cost.}
\label{table:mw_videotadpole}
\vspace{-2em}
\end{table*}

We implement video models as policy supervisors following the setup in Video-TADPoLe~\citep{luo2024text}.  We reuse the same TDMPC~\citep{hansen2022temporal} backbone for policy optimization, along with the same hyperparameter settings and training steps, which are tabulated in Section~\ref{sec:implementation_details}.

\textbf{Continuous Control:} In Table~\ref{table:dmc_videotadpole}, we report walking performance as evaluated by the ground-truth reward function across all adaptation techniques, in comparison to Video-TADPoLe using default AnimateDiff. In terms of Video-TADPoLe parameters, we utilize context window size of 8, stride length of 4 for both Humanoid and Dog experiments. We also list noise levels used for different adaptation techniques in Table~\ref{table:videotadpole_noise_level_dmc}. These settings were discovered in an offline manner using \textbf{\textit{policy discrimination}}, which is described in Appendix~\ref{sec:policy_discrimination}.  Utilized text prompts can be found in Table~\ref{table:text_prompts}.

We first observe that Direct Finetuning shows a slight decrease compared to vanilla AnimateDiff, demonstrating the challenges of capturing complex in-domain dynamics through small-scale finetuning. We also observe that the small in-domain model performs poorly on walking tasks. Consequently, both probabilistic adaptation and its inverse further show degradation on average performance when integrating vanilla AnimateDiff with the in-domain model. However, Inverse Probabilistic Adaptation not only surpasses the in-domain model on Humanoid Walking by a large margin, but also achieves stronger Dog Walking performance than both pretrained and in-domain models. We conjecture that while our small in-domain model has difficulty modeling Humanoid and Dog motions, increasing in-domain model capacity and applying proper adaptation with pretrained models yields improvements through Video-TADPoLe. We further discover that Subject Customization performs better on default walking behavior across both environments, with significant improvement in the case of Dog.  This is a striking result, as Subject Customization only utilizes static images of the scene for adaptation and reuses the default motion module pretrained from AnimateDiff.  


However, for novel text-conditioned generalization to new poses, we find that direct finetuning performs the best.  For a novel text prompt and motion, such as ``a dog jumping'', a directly finetuned video model is able to supervise the learning of an associated policy.  We provide a visual of the achieved policy rollout in Figure~\ref{fig:dog_jumping_rollout}.  These results indicate that for continuous locomotion settings, direct finetuning may be the best balance in terms of preserving performance but also enabling interesting text-conditioned generalization; subject customization, on the other hand, is a low-cost yet performant approach to consider. Finally, Inverse Probabilistic Adaptation enables competitive continuous control performance without finetuning the large pretrained model.

\textbf{Robotic Manipulation: }In Table~\ref{table:mw_videotadpole}, we report the average success rate on MetaWorld tasks across adaptation techniques, using a standardized context window of 8, stride length of 4, and noise level of 700.  We discover that inverse probabilistic adaptation has the best performance.  It is able to achieve non-zero success rate over all 9 tasks, with the highest average success rate of 68.3\%.  By default, utilizing vanilla AnimateDiff through Video-TADPoLe is able to achieve decent performance, highlighting the default text-conditioned generalization capabilities of large pretrained models. We also believe that integrating it with an in-domain model that supervises the motion of the particular dynamics of the environment, enables better generalization on more challenging tasks (e.g. Door Open).  However, the default probabilistic adaptation formulation may heavily rely on the in-domain text-conditioned score, which may be inaccurate when handling novel task prompts due to its small training scale.  We therefore hypothesize that this explains why inverse probabilistic adaptation is more performant; it may be more robust to novel text-conditioning, as more weight is put on leveraging textual priors from the pretrained model.

\vspace{-5pt}
















\subsection{Visual Planning}

We implement video models as visual planners following the framework in UniPi~\citep{du2024learning}.  To generate a plan, we synthesize a sequence of 8 future frames conditioned on both the current visual observation from the environment and the text prompt specifying the task.  This is then translated into an executable action sequence via an inverse dynamics model.  To mitigate the potential error accumulation problem, we evaluate our visual planner in a closed-loop manner, in which we only execute the first inferred action for every environment step.  We provide detailed hyperparameters for video planning, and the implementation of the inverse dynamics model, in Appendix~\ref{sec:implementation_details}.


\begin{table*}
\centering
\small


\setlength{\tabcolsep}{4.8pt}
\scalebox{0.8}{\begin{tabular}{lccccc}\toprule
Success Rate (\%) w/     & Door Close$^*$ & Door Open & Window Close & Window Open & Drawer Close  \\
\midrule
In-Domain-Only & 93.3 $\pm$ 14.9 & 0.0 $\pm$ 0.0 & 53.3 $\pm$ 29.8 & 6.7 $\pm$ 14.9 & 20.0 $\pm$ 29.8  \\
Vanilla AnimateDiff         & 100.0 $\pm$ 0.0 & 0.0 $\pm$ 0.0 & 13.3 $\pm$ 18.3  & 40.0 $\pm$ 27.9 & 46.7 $\pm$ 29.8   \\
\midrule
Direct Finetuning        & 100.0 $\pm$ 0.0 & 0.0 $\pm$ 0.0 & 0.0 $\pm$ 0.0 & 6.7 $\pm$ 14.9 & 80.0 $\pm$ 29.8   \\
Subject Customization    & 100.0 $\pm$ 0.0 & 0.0 $\pm$ 0.0 & 0.0 $\pm$ 0.0 & 20.0 $\pm$ 29.8 & 25.0 $\pm$ 31.9     \\
Prob. Adaptation         & 100.0 $\pm$ 0.0 & 0.0 $\pm$ 0.0 & 73.3 $\pm$ 27.9 & 13.3 $\pm$ 18.3 & 40.0 $\pm$ 43.5 \\
Inverse Prob. Adaptation & 100.0 $\pm$ 0.0 & 0.0 $\pm$ 0.0 & 53.3 $\pm$ 18.3 & 0.0 $\pm$ 0.0  & 53.3 $\pm$ 38.0  \\
\midrule
\midrule
Success Rate (\%) w/     & Drawer Open & Coffee Push$^*$ & Soccer & Button Press & \textbf{Overall} \\
\midrule
In-Domain-Only               & 0.0 $\pm$ 0.0  & 0.0 $\pm$ 0.0 & 0.0 $\pm$ 0.0 & 40.0 $\pm$ 14.9  & 23.7  \\
Vanilla AnimateDiff         & 0.0 $\pm$ 0.0 & 0.0 $\pm$ 0.0 & 0.0 $\pm$ 0.0 & 0.0 $\pm$ 0.0 & 22.2   \\
\midrule
Direct Finetuning       & 0.0 $\pm$ 0.0 & 13.3 $\pm$ 18.2 & 6.7 $\pm$ 14.9 & 0.0 $\pm$ 0.0 & 23.0  \\
Subject Customization    & 0.0 $\pm$ 0.0 & 13.3 $\pm$ 29.8 & 0.0 $\pm$ 0.0 & 0.0 $\pm$ 0.0 & 17.6  \\
Prob. Adaptation         & 6.7 $\pm$ 14.9 & 6.7 $\pm$ 14.9 & 0.0 $\pm$ 0.0 & 33.3 $\pm$ 23.6  & \textbf{30.4}  \\
Inverse Prob. Adaptation & 0.0 $\pm$ 0.0 & 0.0 $\pm$ 0.0 & 0.0 $\pm$ 0.0 & 26.7 $\pm$ 27.9 & 25.9  \\
\midrule
\end{tabular}}
\vspace{-5pt}
\caption[]{\textbf{Visual Planning on MetaWorld.} We report the mean success rate via visual planning across 9 tasks, aggregated over $5$ seeds each.  We discover that both probabilistic adaptation and its inverse are able to act as performant visual planners, and substantially improve over vanilla AnimateDiff; notably, probabilistic adaptation achieves success on more unseen tasks than alternatives.}
\label{table:mw_videoplanning}
\vspace{-15pt}
\end{table*}


\textbf{Robotic Manipulation:} We evaluate visual planners with different adaptation techniques across 9 selected tasks in MetaWorld, of which 7 are unseen during adaptation, and report the success rates in Table~\ref{table:mw_videoplanning}.
Among all evaluated adaptation techniques, we observe that probabilistic adaptation and its inverse version achieve the highest overall performance, with probabilistic adaptation achieving the highest number of non-zero performance on novel tasks. We find that using only static images for adaptation through subject customization fails to produce useful in-domain visual plans. Meanwhile, direct finetuning only brings marginal improvement over Vanilla AnimateDiff and performs on par with the small in-domain model, struggling to generalize well on unseen tasks due to the small scale of demonstration data being considered. For such settings with cheap amounts of in-domain data, we discover that probabilistic adaptation and its inverse appear comparatively more promising.

\textbf{Additional Metrics:} Visual planning relies on synthesizing coherent high-quality video plans that can be accurately interpreted by the inverse dynamics model.  Therefore, we may be interested in measuring the quality of visual plans created by adapted video models in a free-form manner, in other words, generated from pure noise.  In Table~\ref{table:fvd_scores_freeform}, we report FVD scores of free-form generation for both seen and unseen tasks in MetaWorld benchmark. In each setup, we generate 1,000 synthetic videos over 7 robotic manipulation tasks. We observe that probabilistic adaptation and its inverse have strong FVD scores compared to other adaptation techniques.  At the same time, high FVD scores achieved by direct finetuning fails to translate to high task success rate through visual planning; this suggests that while its plans are of high visual quality, they may not necessarily model the correct motions for solving tasks in a text-conditioned manner. This further suggests that beyond just gauging visual metrics alone, using downstream robotic tasks can provide meaningful and additional insights to evaluate adaptation performance of video models. %





\begin{table}[ht]
\centering
\setlength{\tabcolsep}{4.8pt}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}lcccccc@{}}
\toprule
FVD Scores (MetaWorld) & Vanilla AnimateDiff & In-Domain-Only & Direct Finetuning & Subject Customization & Prob. Adaptation & Inverse Prob. Adaptation \\ \midrule
Seen                   & 4625.3              & 2987.2         & 946.0             & 2212.8                & \textbf{848.3}            & 928.6                    \\
Unseen                 & 4469.7              & 3080.7         & \textbf{915.0}             & 2316.0                & 1237.6           & 1250.3                   \\ \bottomrule
\end{tabular}
}
\caption[]{\textbf{FVD Scores with Free-form Generation.} We report FVD scores for videos of MetaWorld tasks, produced by Free-form Generation via the video generative models of interest. This is computed for both seen and unseen task sets, each with 7 tasks, aggregated over 1000 synthetic videos.}
\label{table:fvd_scores_freeform}
\end{table}



\textbf{Studying Data Quality:} In probabilistic adaptation, our in-domain model is trained solely on limited expert demonstrations, which can still be prohibitively expensive to collect in some scenarios. By combining with pretrained video models through adaptation, we provide further investigation on whether the knowledge (e.g. motion priors) obtained from large-scale pretraining can bridge the gap between the suboptimality of in-domain data and task evaluation performance.



\begin{table}[h]
\centering
\small

\setlength{\tabcolsep}{4.8pt}
\scalebox{0.9}{\begin{tabular}{lccccc}\toprule

Success Rate (\%) w/     & Door Close$^*$ & Door Open & Window Close & Window Open & Drawer Close  \\
\midrule
In-Domain-Only                & 93.3 $\pm$ 14.9 & 0.0 $\pm$ 0.0 & 40.0 $\pm$ 27.9 & 0.0 $\pm$ 0.0 & 33.3 $\pm$ 23.6  \\
\midrule
Prob. Adaptation         & 100.0 $\pm$ 0.0 & 0.0 $\pm$ 0.0 & 60.0 $\pm$ 36.5 & 13.3 $\pm$ 18.3 & 46.7 $\pm$ 29.8  \\
Inverse Prob. Adaptation & 93.3 $\pm$ 14.9 & 0.0 $\pm$ 0.0 & 53.3 $\pm$ 29.8 & 0.0 $\pm$ 0.0  & 93.3 $\pm$ 14.9  \\
\midrule
\midrule
Success Rate (\%) w/     & Drawer Open & Coffee Push$^*$ & Soccer & Button Press & \textbf{Overall} \\
\midrule
In-Domain-Only               & 0.0 $\pm$ 0.0  & 0.0 $\pm$ 0.0 & 0.0 $\pm$ 0.0 & 13.3 $\pm$ 18.3  & 20.0  \\
\midrule
Prob. Adaptation         & 6.7 $\pm$ 14.9 & 0.0 $\pm$ 0.0 & 0.0 $\pm$ 0.0 & 0.0 $\pm$ 0.0  & 25.2  \\
Inverse Prob. Adaptation & 6.7 $\pm$ 14.9 & 0.0 $\pm$ 0.0 & 0.0 $\pm$ 0.0 & 0.0 $\pm$ 0.0 & \textbf{27.4}  \\
\midrule

\end{tabular}}
\caption[]{\textbf{Probabilistic Adaptation with Suboptimal Data for Video Planning.} We report the mean success rate via visual planning across 9 tasks, aggregated over $5$ seeds each. Compared to results in Table~\ref{table:mw_videoplanning}, we discover that the overall performance of the in-domain model and probabilistic adaptation is severely impacted by the suboptimality of the training data. In contrast, the performance of inverse probabilistic adaptation remains robust under the suboptimal setup.}
\label{table:mw_videoplanning_suboptimal}
\vspace{-1em}
\end{table}

In Table~\ref{table:mw_videoplanning_suboptimal}, we perform planning with probabilistic adaptation and its inverse, where the available adaptation data is produced by a suboptimal agent.  The suboptimal agent takes an expert action only 30\% of the time, and a random action 70\% of the time.  In consistency with the previous setup, we collect 25 (now suboptimal) demonstrations from the same 7 tasks denoted with asterisks in Table~\ref{table:text_prompts}. We observe that the performance of probabilistic adaptation decreases when integrating with a suboptimal in-domain model. Surprisingly, the overall average task success rate remains robust for inverse probabilistic adaptation. Overall, both adaptation techniques outperform the in-domain only baseline.
This is a promising sign that in adapting large-scale text-to-video models for robotic downstream tasks, expert demonstrations may not be explicitly needed.  This potentially opens up opportunities for applying large-scale video models to novel robotic tasks, where only random or suboptimal demonstrations are tractably available.























