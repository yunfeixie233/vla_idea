\section{Method}

Video models pretrained on internet-scale data exhibit strong zero-shot generalization capabilities across diverse visual scenarios, which make them attractive to leverage for downstream robotic tasks.  However, the general nature of their pretraining may not inherently enable them to understand domain-specific nuances of the environment within which we would like to learn robotic behavior.  We investigate how this can be addressed by integrating in-domain information into large-scale text-to-video models; in Section \ref{subsec:adaptation_techniques} we describe three adaptation approaches of interest, each with separate requirements on in-domain training data and optimization cost.  Then, in Section \ref{subsec:avm_task_generalization}, we describe two techniques through which we can evaluate the novel task generalization capabilities of these adapted video models in a standardized manner.

\subsection{Adaptation Techniques}
\label{subsec:adaptation_techniques}

We aim to adapt pretrained large video models to in-domain data, while maintaining and generalizing their internet-scale knowledge to solving downstream robotic tasks. We utilize an AnimateDiff~\citep{guo2023animatediff} checkpoint as our large video model, which is designed to effectively animate pretrained text-to-image diffusion models such as StableDiffusion~\citep{rombach2022high}. At its core, AnimateDiff features a motion module pretrained on a large-scale video dataset, providing powerful motion priors to guide video generation. We investigate three different techniques for in-domain adaptation: \textbf{\textit{direct finetuning}}, \textbf{\textit{subject customization}}, and \textbf{\textit{probabilistic adaptation}}.

\subsubsection{Direct Finetuning}
Directly finetuning a generally-pretrained text-to-video model is one of the most straightforward ways to mitigate potential domain gaps. Given a video $\tau_0$ sampled from in-domain data distribution $p(\tau_0)$, a randomly sampled Gaussian noise $\epsilon \sim \mathcal{N}(\boldsymbol{0}, \textbf{I})$ and a schedule of noise levels $\beta_t$ that are indexed by timestep $t \in [0, T]$, a text-conditioned video diffusion model can be trained or finetuned as a denoising function $\epsilon_{\theta}(\cdot)$ by optimizing the denoising objective~\citep{ho2022ddpm} below:
\begin{align}
    \mathcal{L}_{\text{denoise}}({\theta}) = \mathbb{E}_{\tau_0,\epsilon,t}[|| \epsilon - \epsilon_{\theta}(\tau_t, t \mid \text{text}) ||^2]
    \label{eq:ddpm}
\end{align}
in which $\tau_t$ is a noise corrupted video obtained by perturbing $\tau_0$ with sampled Gaussian noise $\epsilon$ and noise level $t$, and training is done with text-conditioning dropout to implement classifier-free guidance~\citep{ho2022classifier}.
AnimateDiff is implemented as a trained motion module built on top of pretrained StableDiffusion components; in our study we keep these reused parts unchanged and only adjust the motion module.  Direct finetuning involves additional training with labelled pairs of video demonstrations collected from the domain of interest, allowing the model to update the parameters of its motion module to arbitrary extents and shift its output space towards the target domain.  However, direct finetuning of large video models may cause issues such as model collapse or catastrophic forgetting to emerge, especially when demonstration samples are limited.  %

\subsubsection{Subject Customization}

Performing direct finetuning on pretrained large video models can often be a computationally expensive endeavour.  Furthermore, it requires labelled in-domain video demonstrations, the ready availability of which cannot always be assumed in adaptation scenarios of interest for downstream robotic tasks.  We therefore investigate cheaper alternatives for adaptation with respect to data and optimization cost.  Customized generation~\citep{ruiz2023dreambooth, rinon2023textualinv, wei2024dreamvideo} has been widely used for synthesizing subjects and scenes that accommodate user preferences, utilizing only a few static images of a subject. In this work, we also explore how this technique can be used to inject subject-centric information into pretrained video models, potentially enabling them to better supervise robotic task behavior.  We use DreamBooth~\citep{ruiz2023dreambooth} to customize the generation process due to its simplicity and data efficiency. This method binds a unique text identifier to a specific subject, examples of which are provided using still images paired with text captions \textit{without} motion information, enabling novel view synthesis of the subject contextualized in different scenes.

Following DreamBooth~\citep{ruiz2023dreambooth}, we design special prompts by using a rare token (e.g. ``[D]'') as the unique identifier for each in-domain subject (e.g.``a photo of [D] robot arm''). We finetune StableDiffusion with DreamBooth on static images of the subject paired with this special text prompt. We then instantiate AnimateDiff with the DreamBooth-finetuned U-Net and text encoder for subject-informed video generation. Unlike direct finetuning which requires labelled video data, this approach performs few-shot customization just through the use of static images; the adaptation is possible without requiring expert video demonstrations, when only still observations of the environment and its subject are available.  Since this adaptation technique will not expose any subject motions to the video model, it also allows us to study whether the pretrained video model can directly transfer its \textit{motion prior}, which is obtained from domain-agnostic pre-training, onto arbitrary in-domain subjects of interest and facilitate generalization over downstream robotic tasks.

\subsubsection{Probabilistic Adaptation}

Under some circumstances, the large-scale pretrained model is available for inference, but adjusting it in any way may not be feasible or desirable.  In such scenarios, we consider Probabilistic Adaptation~\citep{yang2023probabilistic}, where a small sample of demonstrations is utilized to train an in-domain video model, which can be flexibly parameterized to accommodate available modeling resource constraints.  Adaptation is then performed by combining the predicted scores from the pretrained, frozen large-scale video model $\epsilon_{\text{pretrained}}(\tau_t, t \mid \text{text})$ with those of the lightweight domain-specific video model $\epsilon_{\theta}(\tau_t, t \mid \text{text})$ during inference.  We use low-temperature sampling~\citep{yang2023probabilistic} to compute the adapted score following the denoising function below:
\begin{align}
    \tilde{\epsilon} = \epsilon_{\theta}(\tau_t, t) + \alpha\Big(\epsilon_{\theta}(\tau_t, t \mid \text{text}) + \gamma \epsilon_{\text{pretrained}}(\tau_t, t \mid \text{text}) - \epsilon_{\theta}(\tau_t, t)\Big)
    \label{eq:probadap}
\end{align}
where $\gamma$ is the prior strength, %
and $\alpha$ is the guidance scale of text-conditioning. This method only requires the training of a small component with limited in-domain data, and allows the pretrained large video model to serve as a probabilistic prior which guides the generation process of the domain-specific model through score composition.

Moreover, we extend Equation \ref{eq:probadap} to its inverse version, in which the adaptation direction between $\epsilon_{\theta}(\tau_t, t \mid \text{text})$ and $\epsilon_{\text{pretrained}}(\tau_t, t \mid \text{text})$ is inverted:
\begin{align}
    \tilde{\epsilon}_{\text{inv}} = \epsilon_{\text{pretrained}}(\tau_t, t) + \alpha\Big(\epsilon_{\text{pretrained}}(\tau_t, t \mid \text{text}) + \gamma \epsilon_{\theta}(\tau_t, t \mid \text{text}) - \epsilon_{\text{pretrained}}(\tau_t, t)\Big).
    \label{eq:inv_probadap}
\end{align}
In inverse probabilistic adaptation, the pretrained video model controls the generation process, while consulting the small model for domain-specific information. Both probabilistic adaptation formulations allow more flexible and low-cost adaptation in the video space compared to direct finetuning; empirically, we find that one direction may work better than the other in certain circumstances.



\subsection{Evaluating Task Generalization Capabilities of Video Models}
\label{subsec:avm_task_generalization}
To measure the quality of adaptation, samples from the adapted video models can be judged with respect to in-domain examples in terms of Fréchet Video Distance (FVD) scores~\citep{yang2023probabilistic, thomas2018fvd}.  However, beyond simply assessing surface-level visual style, we propose further evaluating adaptation quality via their ability to facilitate downstream robotic performance.  For tasks with predefined evaluation schemes, this provides a quantifiable metric in terms of achieved performance and success, and can deliver additional insights into the capabilities of video models beyond appealing visual content generation.  In this work we consider two approaches, depicted in Figure~\ref{fig:enter-label}, for applying video models to decision making -- \textbf{\textit{visual planning}} and \textbf{\textit{policy supervision}}.  Under both scenarios, we can measure to what degree downstream robotic performance and text-conditioned generalization may be enabled via different adaptation techniques.

\subsubsection{Video Models as Visual Planners} %


One intuitive approach to use video generative models for decision-making is to use synthesized videos as a visual plan of the future, which then can be converted into executable actions.  Concretely, a planning model learns $p(\boldsymbol{s’} \mid \boldsymbol{s})$, for state $\boldsymbol{s}$ and subsequent state $\boldsymbol{s’}$; in the case of visual planning, $\boldsymbol{s}$ takes the form of RGB frames and $p(\boldsymbol{s’} \mid \boldsymbol{s})$ is a video generative model.  Synthesized video plans are then translated into actions to execute in the environment through a separately learned inverse dynamics model $p(\boldsymbol{a} \mid \boldsymbol{s}, \boldsymbol{s’})$.  Applying text-guided video generation for task planning has been successfully applied in prior work~\citep{du2024learning, du2024video, ajaj2023hip}.  However, as the performance of video planning can be highly dependent on both the visual quality of the imagined plan and the robustness of the inverse dynamics model, prior work has only utilized video models trained on \textit{in-domain demonstrations}.

In this work we identify and exploit the fact that the planning model $p(\boldsymbol{s’} \mid \boldsymbol{s})$ does not require any action information in its training, and is therefore a promising component through which information from large-scale data and pretraining can be introduced.  Through the lens of visual planning, we explicitly examine integrating large-scale video information into the planner $p(\boldsymbol{s’} \mid \boldsymbol{s})$ by directly leveraging a video generative model pretrained on internet-scale data.  

A set of action-labeled trajectories is still required to train an inverse dynamics model; in this work we also utilize a small set of such demonstration data to temper the large-scale pretrained video model to generate coherent in-domain visual plans while preserving priors summarized from large-scale pretraining.  We therefore investigate whether generally-pretrained models can capture environment-specific dynamics through cheap adaptation to in-domain data while leveraging preserved text-conditioned generalization capabilities to facilitate performant in-domain visual planning even for \textit{novel} tasks of interest unseen during adaptation.


\subsubsection{Video Models as Policy Supervisors} %
\label{subsec:policy_supervision_method}
In addition to their use as planners, video generative models can be used as policy supervisors.  In this approach, the video model is utilized to evaluate frames achieved by the agent during interaction in a discriminative manner; these signals can then be converted into rewards with which to optimize the policy.
While many prior works require video models that are trained solely on environment-specific demonstrations of expert quality~\citep{huang2023diffusion, alejandro2023viper}, with rewards computed against a summarized notion of ``expertness", here we seek to extract accurate text-conditioned rewards from text-to-video models.
Video-TADPole~\citep{luo2024text} measures text-alignment of robotic trajectories by noise-corrupting achieved pixel observations, and evaluating how likely a large-scale pretrained text-to-video model would reconstruct the video interactions conditioned on the provided natural language prompt (additional details provided in Appendix~\ref{sec:videotadpole_equations}).  We train a text-conditioned policy by maximizing cumulative Video-TADPoLe rewards through reinforcement learning.  Successful optimization of such a policy enables us to evaluate the ability of adapted large-scale text-to-video models in facilitating novel task generalization, specified by natural language.


After adapting to limited data samples, synthesizing coherent high-quality in-domain videos can still be challenging for video models.  This may pose issues in visual planning, where the generated plan must appear sufficiently in-domain for an inverse dynamics model to be able to accurately translate into meaningful actions.  On the contrary, video models do not necessarily need the ability to create high-quality in-domain videos from scratch to behave as effective policy supervisors; they simply need to be able to critique the quality of achieved in-domain frames.  Thus, expressing adapted knowledge through rewards may allow the detachment of downstream policy performance from demands on video generation quality.  Conversely, a potential drawback of this approach in comparison to visual planning is the high variance commonly observed in the policy learning process.
