\section{Experiments}
\label{sec:experiments}
\input{figures/figure_4_benchmarktasks}
In this section, we evaluate the effectiveness of RS-CL across diverse training scenarios. 
In Section~\ref{subsec:finetuning}, we examine its impact when applied on top of large-scale pre-trained state-of-the-art Vision-Language-Action (VLA) models on challenging multitask manipulation benchmarks: RoboCasa-Kitchen~\citep{robocasa} and LIBERO~\citep{libero}. 
We also demonstrate its applicability to real-world tasks using a 7-DoF manipulator.
In Section~\ref{subsec:fromscratch}, we further validate RS-CL in the setting where a VLA model is trained from scratch, starting from a pre-trained VLM.
For an overview of the benchmark tasks and real-robot experiments, see Fig.~\ref{figure:tasks}.


\vspace{0.01in}
\textbf{Implementation and training details.}~
We adopt GR00T N1.5~\citep{gr00tn1_5} as our baseline VLA framework and, unless otherwise specified, we follow the training and inference settings from the original implementation.
For the contrastive regularization path, the projection head $g_{\psi}$ is a 2-layer MLP with hidden dimension 2048 and projection dimension 128. 
The weighting coefficient $\lambda$ for $\mathcal{L}_{\textrm{RS-CL}}$ is initialized to 1.0 and decayed to 0 using a cosine schedule, such that representation refinement is emphasized in early training while accurate action prediction becomes the main focus later. 
For proprioceptive inputs, we primarily use the end-effector position $(x,y,z)$, 6D rotation, and gripper state. 
In the real-world tasks, we additionally explore the use of absolute joint positions of the 7-DoF manipulator to examine variations in proprioceptive configurations.
Further training details for each experiment are provided in Appendix~\ref{appendix:trainingdetails}.


\vspace{0.01in}
\textbf{Baselines.}~
We primarily validate RS-CL on top of the GR00T N1.5 training pipeline, a state-of-the-art VLA model trained with large-scale robot trajectories. 
To provide context on the benchmarks, we also report the performance of representative VLA models, including $\pi_0$~\citep{pi0}, $\pi_0$-FAST~\citep{pi0fast}, and GR00T N1~\citep{gr00tn1}. 
For reproduced performance of $\pi_0$-FAST and $\pi_0$ on RoboCasa-Kitchen, we train for 30K and 60K gradient steps, respectively, with a global batch size of 64, following the original settings as closely as possible. 
In Section~\ref{subsec:fromscratch}, we include as a baseline further-training the VLM with various instructions curated with robotics data, and then fine-tuning for action prediction.
We make use of state-of-the-art embodied reasoning models such as RoboBrain~\citep{robobrain20}, VeBrain~\citep{vebrain}, and Cosmos-Reason1~\citep{cosmosreason}, as well as models trained for discretized action prediction~\citep{nora}.

% comparison of RS-CL with approaches that further train the VLM on curated instruction dataset with robotics data, before fine-tuning for action prediction. 

 

% \input{tables/table_main}

\subsection{Fine-tuning Experiments}
\label{subsec:finetuning}
\input{tables/table_1_robocasa}
\input{tables/table_2_libero}
We first evaluate RS-CL in a fine-tuning scenario, where it is integrated into a state-of-the-art pre-trained VLA model. 
This setup tests whether RS-CL can yield additional gains on weights already optimized for large-scale action prediction, demonstrating its ability to further enhance strong pretrained policies.
We adopt RoboCasa-Kitchen~\citep{robocasa} and LIBERO~\citep{libero}, two multitask benchmarks as our simulation experiments.
To further validate the effectiveness of our method beyond simulation, we conduct real-robot experiments on a Franka Research 3 arm, covering both in-domain and generalization performance.

\textbf{Setup.}~
RoboCasa-Kitchen consists of 24 atomic manipulation tasks in a simulated kitchen environment with three camera views (2 exterior, 1 wrist camera). 
We evaluate RS-CL under varying numbers of demonstrations (30, 100, 300) using the publicly available dataset generated by MimicGen~\citep{mimicgen}.
LIBERO is also a multitask simulation benchmark comprising four task suites: spatial, object, goal, and long (each with 10 tasks and 50 demonstrations per task), utilizing two camera views (1 exterior, 1 wrist camera).
For LIBERO, we utilize the filtered dataset from \citet{openvla} and jointly train the four task suites (see Appendix~\ref{appendix:simulation} for details).
To further assess whether RS-CL leads to more precise actions in task execution, we design our real-robot experiments primarily around pick-and-place tasks, which require accurate positioning during grasping and placing. 
We also introduce a challenging close-lid task, where the lid has a small handle that is more difficult to grasp than other objects. 
Once grasped, the wrist camera view becomes occluded, requiring placement to rely mainly using the exterior camera (see Fig.~\ref{figure:tasks}, right).
We collect and train each method with 60 expert demonstrations for 4 pick-and-place tasks across diverse objects (teddy bear, sponge, cup, cube) and environments (box, bowl, plate, basket), and the close-lid task, utilizing two camera views (1 exterior, 1 wrist camera)
(see Appendix~\ref{appendix:realworld} for details).



\vspace{0.01in}
\textbf{Simulation results.}~
Table \ref{maintable:robocasanocl} summarizes the performance of RS-CL on RoboCasa-Kitchen. 
Across all dataset sizes, RS-CL consistently outperforms the original GR00T N1.5 fine-tuning framework. 
In particular, pick-and-place tasks exhibit a substantial improvement, with success rates rising from 30.3\% to 41.5\% (+11.2\%). 
We attribute this gain to RS-CLâ€™s ability to generate more accurate actions during execution, which is particularly beneficial for pick-and-place tasks requiring precise positioning during grasping and placing.
We further validate this in our following real-world experiments.
RS-CL also improves performance on LIBERO (Table~\ref{maintable:liberonocl}), confirming its robustness across different benchmarks.

\input{figures/figure_5_realworld}
\input{figures/figure_6_qualitativeresults}
\vspace{0.01in}
\textbf{Real-robot experiment results.}~
RS-CL consistently improves performance across real-robot tasks (see Fig.~\ref{subfig:realexperiments:a}).
In particular, for the close-lid task, RS-CL brings improvements not only in partial success (\emph{i.e.}, lifting the lid) but also larger gains in complete success (\emph{i.e.}, accurately closing the pot) even under occluded viewpoints (see Fig.~\ref{figure:qualitative}).
We attribute this effect to two factors: (i) proprioceptive supervision enables more accurate positioning, and (ii) the proposed \textit{view cutoff} augmentation promotes view-invariant representations, thereby improving robustness to partial occlusion.
In addition, our generalization experiments show that RS-CL maintains strong generalization performance of VLAs across visual, physical shifts, and in the terms of language grounding (see Fig.~\ref{subfig:realexperiments:b}).


\subsection{From-Scratch Experiments}
\label{subsec:fromscratch}
In this section, we evaluate the impact of RS-CL in a from-scratch training scenario, where we train a VLA model on top of general-purpose pre-trained VLM backbones of Qwen2.5-VL~\citep{qwen2_5vl}, GR00T N1.5 VLM~\citep{gr00tn1_5} and SigLIP2~\citep{siglip2}.
This setup directly aligns with our motivation that pre-trained VLM representations lack sensitivity to robotic signals, and allows us to validate whether explicitly aligning them to proprioceptive information yields performance gains. 
Furthermore, we compare the effect on RS-CL against baselines obtained by further training VLMs on robotics datasets.


\vspace{0.01in}
\textbf{Setup.}~
We adopt RoboCasa-Kitchen as our main benchmark, and use 300 demonstrations for training all models.
For the VLA training framework, we attach a randomly initialized action decoder to various pre-trained VLMs, with a lightweight adapter module $f_\phi$ in between.
We freeze the VLM and train the adapter to refine condition representations, except for SigLIP2, where we experiment with an unfrozen VLM setting either to study how RS-CL interacts with different numbers of trainable backbone parameters.
For the action decoder, we adopt a 16-layer DiT with 0.5B parameters.
For the further-trained VLM baselines, we utilize RoboBrain~\citep{robobrain20}, VeBrain~\citep{vebrain}, and Cosmos-Reason1~\citep{cosmosreason}, which are high-performing baselines further trained from Qwen2.5-VL on embodied reasoning with robotics dataset, and NORA~\citep{nora}, which is trained on the Open-X-Embodiment~\citep{openx} dataset to predict FAST~\citep{pi0fast} tokenized actions (see Appendix \ref{appendix:fromscratch} for details).


\vspace{0.01in}
\textbf{Results on general-purpose VLM backbones.}~
Fig.~\ref{figure:vlmtrainingmethod} summarizes the effect of RS-CL when training VLA models from different pre-trained VLMs.
Across all backbones, RS-CL consistently improves success rates, demonstrating that our representation regularization generalizes beyond a particular backbone model.
On SigLIP2, RS-CL yields larger improvements from 4.0\% to 14.1\% when the backbone is unfrozen, indicating that RS-CL benefits from increased trainable capacity.



\input{figures/figure_7_fromscratch}
\vspace{0.01in}
\textbf{Comparison to VLM training strategy.}~
Fig.~\ref{figure:vlmtrainingmethod} compares RS-CL with VLMs that are further trained on robotics datasets for tasks such as visual grounding, embodied reasoning, and discretized action prediction.
While such further-trained VLMs, when used as conditioning models, provide only limited and often inconsistent gains across backbone families, RS-CL consistently delivers larger improvements.
It achieves higher success rates than any of these adapted models on both Qwen2.5-VL-3B and 7B, and further enhances their benefits when combined with them. 
Even for GR00T N1.5, which is derived from Eagle 2.5 VLM~\citep{eagle2_5} with enhanced grounding and reasoning capabilities, RS-CL provides additional gains. 
These results suggest that robotics-specific training alone may not fully close the gap between general-purpose VLM representations and the control signals required for action generation, while RS-CL effectively bridges much of this gap.

\subsection{Ablation Study} 
\label{subsec:ablationstudy}
\input{tables/table_3_ablations}
\vspace{0.01in}
\textbf{Effect of soft-label supervision target.}~
In Table~\ref {tab:ablation_softlabel}, we observe that standard InfoNCE improves over the baseline without contrastive learning, demonstrating the effectiveness of our training framework, namely contrastive representation regularization for VLA models (see Appendix~\ref{appendix:clvsrscl} for further analysis).
However, alternative supervision signals (see Appendix~\ref{appendix:analysisdetail} for distance definition of targets) such as next action distances fall below vanilla InfoNCE. 
A plausible reason is that the next action itself serves as the prediction target, making it difficult to use as a reliable alignment signal.
In contrast, the robot proprioceptive state provides a stable cue for representation alignment.
\input{figures/figure_8_analysis}

\vspace{0.01in}
\textbf{Effect of representation augmentation strategy.}~
In Table~\ref {tab:ablation_augmentation}, we observe limited improvements from similar representation-level cutoff operations~\citep{cutoff}, while our proposed view cutoff achieves the highest success rate. 
This shows that simulating viewpoint variation is particularly beneficial for robust representation learning in multi-view robotic manipulation settings.
This is in line with prior works, addressing the effects of utilizing multi-view data for representation learning~\citep{croco, mvmwm}.


\vspace{0.01in}
\textbf{Quantitative analysis of representation alignment.}~
We further measure how RS-CL improves the alignment of VLM representations with robotic signals with CKNNA~\citep{cknna}. 
As shown in Fig.~\ref{fig:cka}, RS-CL increases representation similarity between learned embeddings and proprioceptive features, indicating that \mbox{RS-CL} successfully reshapes the embedding space toward capturing control-relevant signals.
Details are described in Appendix~\ref{appendix:analysisdetail}.



