\newpage
\appendix

\section{Implemented Neural Architectures and Lifelong Learning Algorithms}

\begin{table}[h!]
    \centering
    \begin{tabular}{ l l}
    \toprule
    \multirow{3}{*}{Neural Policy Arch.} &  \bcrnn{}  \\
                                         &  \bct{}  \\
                                         &  \bcvilt{}  \\
    \midrule 
    \multirow{5}{*}{Lifelong Learning Algo.} &  \seql{}  \\
                                             &  \ewc{}~\citep{kirkpatrick2017overcoming}  \\
                                             &  \er{}~\citep{chaudhry2019tiny}  \\
                                             &  \packnet{}~\citep{mallya2018packnet}  \\
                                             &  \mtl{}  \\
                                    
\bottomrule
    \end{tabular}
    \caption{The implemented neural policy architectures and the lifelong learning algorithms in \lb{}.}
    \label{tab:arch-policy}
\end{table}
\subsection{Neural Architectures}
\label{appendix:neural}

In Section~\ref{sec:method-architecture}, we outlined the neural network architectures utilized in our experiments, namely \bcrnn{}, \bct{}, and \bcvilt{}. The specifics of each architecture are illustrated in Figure~\ref{fig:architectures}. Furthermore, Table~\ref{tab:rnn-param}, \ref{tab:transformer-param}, and \ref{tab:vilt-param} display the hyperparameters for the architectures used throughout all of our experiments.

% \bcrnn{}: Talbe \ref{tab:rnn-param} ResNet [...]. LSTM [..., number of layers, number of hidden units in each layer]

% \bct{}: ResNet [...], transformer decoder [number of heads, number of layers, token size, and number of hidden units in each mlp component]

% \bcvilt{}: ViT [number of heads, number of layers, token size, and number of hidden units in each mlp component], transformer decoder [number of heads, number of layers, token size, and number of hidden units in each mlp component]


\begin{figure*}[h!]
    \centering
    \includegraphics[width=\textwidth]{figures/libero_arch_3.png}
    \caption{We provide visualizations of the architectures for \bcrnn{}, \bct{}, and \bcvilt{}, respectively. It is worth noting that each model architecture incorporates language embedding in distinct ways.}
    \label{fig:architectures}
\end{figure*}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/libero_arch_encoder.png}
    \caption{The image encoders: ResNet-based encoder and the vision transformer-based encoder.}
    \label{fig:encoders}
\end{figure}

\section{Computation}
For all experiments, we use a single Nvidia A100 GPU or a single Nvidia A40 GPU (CUDA 11.7) with 8~16 CPUs for training and evaluation.

\newpage

\begin{minipage}{\textwidth}
\begin{minipage}[h!]{0.48\textwidth}
\makeatletter\def\@captype{table}
\centering
\begin{tabular}{cc}
    \toprule
    Variable &  Value \\
    \midrule
 resnet\_image\_embed\_size & 64 \\
 text\_embed\_size & 32 \\
 rnn\_hidden\_size & 1024 \\
 rnn\_layer\_num & 2 \\
 rnn\_dropout & 0.0 \\
    \bottomrule
\end{tabular}
    \caption{Hyper parameters of \bcrnn{}.}
    \label{tab:rnn-param}
\end{minipage}
\begin{minipage}[t]{0.48\textwidth}
\makeatletter\def\@captype{table}
\centering
\begin{tabular}{cc}
    \toprule
    Variable &  Value \\
    \midrule
extra\_info\_hidden\_size & 128 \\
img\_embed\_size & 64 \\
transformer\_num\_layers & 4 \\
transformer\_num\_heads & 6 \\
transformer\_head\_output\_size & 64 \\
transformer\_mlp\_hidden\_size & 256 \\
transformer\_dropout & 0.1 \\
transformer\_max\_seq\_len & 10 \\
    \bottomrule
\end{tabular}
    \caption{Hyper parameters of \bct{}.}
    \label{tab:transformer-param}
\end{minipage}
\end{minipage}

\begin{table}[h!]
    \centering
    \begin{tabular}{cc}
    \toprule
    Variable &  Value \\
    \midrule
extra\_info\_hidden\_size & 128 \\
img\_embed\_size & 128 \\
spatial\_transformer\_num\_layers & 7 \\
spatial\_transformer\_num\_heads & 8 \\
spatial\_transformer\_head\_output\_size & 120 \\
spatial\_transformer\_mlp\_hidden\_size & 256 \\
spatial\_transformer\_dropout & 0.1 \\
spatial\_down\_sample\_embed\_size & 64 \\
temporal\_transformer\_input\_size & null \\
temporal\_transformer\_num\_layers & 4 \\
temporal\_transformer\_num\_heads & 6 \\
temporal\_transformer\_head\_output\_size & 64 \\
temporal\_transformer\_mlp\_hidden\_size & 256 \\
temporal\_transformer\_dropout & 0.1 \\
temporal\_transformer\_max\_seq\_len & 10 \\
    \bottomrule 
    \end{tabular}
\caption{Hyper parameters of \bcvilt{}.}
\label{tab:vilt-param}
\end{table}

\subsection{Lifelong Learning Algorithms}
\label{appendix:llalgo}
Lifelong learning (LL) is a field of study that aims to understand how an agent can continually acquire and retain knowledge over an infinite sequence of tasks without catastrophically forgetting previous knowledge. Recent literature proposes three main approaches to address the problem of catastrophic forgetting in deep learning: Dynamic Architecture approaches, Regularization-Based approaches, and Rehearsal approaches. Although some recent works explore the combination of different approaches~\citep{ayub2022few,kang2022class,rios2020lifelong} or new strategies~\citep{zhou2022forward,saha2021space,cheung2019superposition}, our benchmark aims to provide an in-depth analysis of these three basic lifelong learning directions to reveal their pros and cons on robot learning tasks.

The dynamic architecture approach gradually expands the learning model to incorporate new knowledge~\citep{rusu2016progressive,yoon2017lifelong,mallya2018packnet,hung2019compacting,wu2020firefly,ben2022lifelong}. Regularization-based methods, on the other hand, regularize the learner to a previous checkpoint when it learns a new task~\citep{kirkpatrick2017overcoming,chaudhry2018riemannian,schwarz2018progress,liu2022continual}. Rehearsal methods save exemplar data from prior tasks and replay them with new data to consolidate the agent's memory~\citep{chaudhry2019tiny,lopez2017gradient,chaudhry2018efficient,buzzega2020dark}. For a comprehensive review of LL methods, we refer readers to surveys \citep{de2021continual, parisi2019continual}.

% Lifelong learning (LL) is the field of study that aims to understand how an agent can continually acquire and retain knowledge over an infinite sequence of tasks without catastrophically forgetting previous knowledge. Three main approaches have been proposed in recent literature to address the problem of catastrophic forgetting in deep learning.
% The dynamic architecture approach gradually grows the learning model to incorporate new knowledge~\citep{rusu2016progressive,yoon2017lifelong,mallya2018packnet,hung2019compacting,wu2020firefly}. Regularization-based methods regularize the learner to a previous checkpoint when it learns on a new task~\citep{kirkpatrick2017overcoming,chaudhry2018riemannian,schwarz2018progress}. Rehearsal methods save exemplar data from prior tasks and replay them with new data to consolidate the agent's memory~\citep{chaudhry2019tiny,lopez2017gradient,chaudhry2018efficient,buzzega2020dark}. We refer the readers to surveys \citep{de2021continual,parisi2019continual} for a comprehensive review of LL methods.

The following paragraphs provide details on the three lifelong learning algorithms that we have implemented.


\paragraph{\er{}} Experience Replay (\er{})~\citep{chaudhry2019tiny} is a \textbf{rehearsal-based} approach that maintains a memory buffer of samples from previous tasks and leverages it to learn new tasks. After the completion of policy learning for a task, \er{} stores a portion of the data into a storage memory. When training a new task, \er{} samples data from the memory and combines it with the training data from the current task so that the training data approximately represents the empirical distribution of all-task data. In our implementation, we use a replay buffer to store a portion of the training data (up to 1000 trajectories) after training each task. For every training iteration during the training of a new task, we uniformly sample a fixed number of replay data from the memory (32 trajectories) along with each batch of training data from the new task.


\paragraph{\ewc{}} Elastic Weight Consolidation(\ewc{})~\citep{kirkpatrick2017overcoming} is a \textbf{regularization-based} approach that add a regularization term that constraints neural network update to the original single-task learning objective. Specifically, \ewc{} uses the Fisher information matrix that quantify the importance of every neural netwrk parameter. The loss function for task $k$ is:
$$
\mathcal{L}_{k}^{EWC}(\theta)=\mathcal{L}_K^{BC}(\theta)+\sum_i \frac{\lambda}{2} F_i\left(\theta_i-\theta_{{k-1}, i}^*\right)^2,
$$
where $\lambda$ is a penalty hyperparameter, and the coefficient $F_i$ is the diagonal of the Fisher information matrix: $F_k=\mathbb{E}_{s \sim \mathcal{D}_k} \mathbb{E}_{a \sim p_\theta(\cdot \mid s)}\left(\nabla_{\theta_k} \log p_{\theta_k}(a | s)\right)^2$. In this work, we use the online update version of \ewc{} that updates the Fisher information matrix using  exponential moving average along the lifelong learning process, and use the empirical estimation of above Fisher information matrix to stabilize the estimation. Formally, the actually used estimation of Fisher Information Matrix is $\tilde{F_k}=\gamma F_{k-1} + (1-\gamma) F_k$, where $F_k=\mathbb{E}_{(s,a) \sim \mathcal{D}_k} \left(\nabla_{\theta_k} \log p_{\theta_k}(a | s)\right)^2$ and $k$ is the task number. We set $\gamma=0.9$ and $\lambda=5 \cdot 10^4$.

\paragraph{\packnet{}}
\packnet{}~\citep{mallya2018packnet} is a \textbf{dynamic architecture-based} approach that aims to prevent changes to parameters that are important for previous tasks in lifelong learning. To achieve this, \packnet{} iteratively trains, prunes, fine-tunes, and freezes parts of the network. The method theoretically completely avoids catastrophic forgetting, but for each new task, the number of available parameters shrinks. The pruning process in \packnet{} involves two stages. First, the network is trained, and at the end of the training, a fixed proportion of the most important parameters (25\% in our implementation) are chosen, and the rest are pruned. Second, the selected part of the network is fine-tuned and then frozen. In our implementation, we follow the original paper \citep{mallya2018packnet} and do not train all biases and normalization layers. We perform the same number of fine-tuning epochs as for training (50 epochs in our implementation). Note that all evaluation metrics are calculated \textit{before} the fine-tuning stage.

\clearpage

\section{LIBERO Task Suite Designs}
\label{appendix:task}

\subsection{Task Suites}
We visualize all the tasks from the four task suites in Figure~\ref{fig:libero-spatial}-~\ref{fig:libero-100}. Figure~\ref{fig:libero-spatial} visualizes the initial states since the task goals are always the same. All the figures visualize the goal states of tasks except for Figure~\ref{fig:libero-spatial}, which visualizes the initial states since the task goals are always the same. 


\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/libero_spatial.png}
    \caption{\liberospatial{}}
    \label{fig:libero-spatial}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/libero_object.png}
    \caption{\liberoobject{}}
    \label{fig:libero-object}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/libero_goal.png}
    \caption{\liberogoal{}}
    \label{fig:libero-goal}
\end{figure}


\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/libero_100.png}
    \caption{\liberohundred{}}
    \label{fig:libero-100}
\end{figure}

~\\
~\\
~\\
~\\
~\\
~\\
~\\

\clearpage
\subsection{PDDL-based Scene Description File}
Here we visualize the whole content of an example scene description file based on PDDL. This file corresponds to the task shown in Figure~\ref{fig:libero-procedural-generation}.

\begin{center}
    \textbf{Example task:}\qquad \textcolor{purple}{\textit{Open the top drawer of the cabinet and put the bowl in it}}.
\end{center}

\begin{lstlisting}[caption={},label=lst:pddl-example]
(define (problem LIBERO_Kitchen_Tabletop_Manipulation)
  (:domain robosuite)
  (:language open the top drawer of the cabinet and put the bowl in it)
    (:regions
      (wooden_cabinet_init_region
          (:target kitchen_table)
          (:ranges (
              (-0.01 -0.31 0.01 -0.29)
            )
          )
          (:yaw_rotation (
              (3.141592653589793 3.141592653589793)
            )
          )
      )
      (akita_black_bowl_init_region
          (:target kitchen_table)
          (:ranges (
              (-0.025 -0.025 0.025 0.025)
            )
          )
          (:yaw_rotation (
              (0.0 0.0)
            )
          )
      )
      (plate_init_region
          (:target kitchen_table)
          (:ranges (
              (-0.025 0.225 0.025 0.275)
            )
          )
          (:yaw_rotation (
              (0.0 0.0)
            )
          )
      )
      (top_side
          (:target wooden_cabinet_1)
      )
      (top_region
          (:target wooden_cabinet_1)
      )
      (middle_region
          (:target wooden_cabinet_1)
      )
      (bottom_region
          (:target wooden_cabinet_1)
      )
    )

  (:fixtures
    kitchen_table - kitchen_table
    wooden_cabinet_1 - wooden_cabinet
  )

  (:objects
    akita_black_bowl_1 - akita_black_bowl
    plate_1 - plate
  )

  (:obj_of_interest
    wooden_cabinet_1
    akita_black_bowl_1
  )

  (:init
    (On akita_black_bowl_1 kitchen_table_akita_black_bowl_init_region)
    (On plate_1 kitchen_table_plate_init_region)
    (On wooden_cabinet_1 kitchen_table_wooden_cabinet_init_region)
  )

  (:goal
    (And (Open wooden_cabinet_1_top_region) 
         (In akita_black_bowl_1 wooden_cabinet_1_top_region)
    )
  )

)
\end{lstlisting}


\newpage

\section{Experimental Setup}
\label{appendix:exp-setting}

We consider five lifelong learning algorithms: \seql{} the sequential learning baseline where the agent learns each task in the sequence directly without any further consideration, \mtl{} the multitask learning baseline where the agent learns all tasks in the sequence simultaneously, the regularization-based method \ewc{}~\citep{kirkpatrick2017overcoming}, the replay-based method \er{}~\citep{chaudhry2019tiny}, and the dynamic architecture-based method \packnet{}~\citep{mallya2018packnet}. \seql{} and \mtl{} can be seen as approximations of the lower and upper bounds respectively for any lifelong learning algorithm. The other three methods represent the three primary categories of lifelong learning algorithms.
%
For the neural architectures, we consider three vision-language policy architectures: \bcrnn{}, \bct{}, \bcvilt{}, which differ in how spatial or temporal information is aggregated (See Appendix~\ref{appendix:neural} for more details).
%
For each task, the agent is trained over 50 epochs on the 50 demonstration trajectories. We evaluate the agent's average success rate over 20 test rollout trajectories of a maximum length of 600 every 5 epochs. We use Adam optimizer~\citep{kingma2014adam} with a batch size of $32$, and a cosine scheduled learning rate from $0.0001$ to $0.00001$ for each task.
Following the convention of~\robomimic{}~\citep{mandlekar2021matters}, we pick the model checkpoint that achieves the best success rate as the final policy for a given task.
After 50 epochs of training, the agent with the best checkpoint is then evaluated on all previously learned tasks, with 20 test rollout trajectories for each task. All policy networks are matched in Floating Point Operations Per Second (FLOPS): all policy architectures have $\sim$13.5G FLOPS. For each combination of algorithm, policy architecture, and task suite, we run the lifelong learning method 3 times with random seeds $\{100, 200, 300\}$ (180 experiments in total). See Table~\ref{tab:arch-policy} for the implemented algorithms and architectures.


\section{Additional Experiment Results}
\label{appendix:additional}


\subsection{Full Results}
\label{appendix:additional-result}

We provide the full results across three different lifelong learning algorithms (e.g., \ewc{}, \er{}, \packnet{}) and three different policy architectures (e.g., \bcrnn{}, \bct{}, \bcvilt{}) on the four task suites in Table~\ref{tab:algorithm}.

\begin{table}[h!]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l l  c c c c c c}
    \toprule
 Algo. &   Policy Arch. & FWT($\uparrow$) & NBT($\downarrow$) & AUC($\uparrow$) & FWT($\uparrow$) & NBT($\downarrow$) & AUC($\uparrow$)\\
    \midrule 
    & & \multicolumn{3}{c}{\liberolong{}} & \multicolumn{3}{c}{\liberospatial{}}  \\
\cmidrule(lr){3-5}\cmidrule(lr){6-8}

\multirow{3}{*}{\seql{}}    & \bcrnn                    &  0.24 \fs {   0.02 } &  0.28  \fs {   0.01 } &  0.07  \fs {  0.01 } &  0.50 \fs {   0.01 } &  0.61  \fs {   0.01 } &  0.14  \fs {  0.01 } \\
                            & \bct                      &  0.54 \fs {   0.01 } &  0.63  \fs {   0.01 } &  0.15  \fs {  0.00 } &  0.72 \fs {   0.01 } &  0.81  \fs {   0.01 } &  0.20  \fs {  0.01 } \\
                            & \bcvilt                   &  0.44 \fs {   0.04 } &  0.50  \fs {   0.05 } &  0.13  \fs {  0.01 } &  0.63 \fs {   0.02 } &  0.76  \fs {   0.01 } &  0.16  \fs {  0.01 } \\
\cmidrule(lr){2-8}

\multirow{3}{*}{\er{}}      & \bcrnn                    &  0.16 \fs {   0.02 } &  0.16  \fs {   0.02 } &  0.08  \fs {  0.01 } &  0.40 \fs {   0.02 } &  0.29  \fs {   0.02 } &  0.29  \fs {  0.01 } \\
                            & \bct                      &  0.48 \fs {   0.02 } &  0.32  \fs {   0.04 } &  \toptwoboxit 0.32  \fs {  0.01 } & 0.65 \fs {   0.03 } & 0.27  \fs {   0.03 } &  \topthreeboxit 0.56  \fs {  0.01 } \\
                            & \bcvilt                   &  0.38 \fs {   0.05 } & 0.29  \fs {   0.06 } &  \topthreeboxit 0.25  \fs {  0.02 } &  0.63 \fs {   0.01 } &  0.29  \fs {   0.02 } &  0.50  \fs {  0.02 } \\
\cmidrule(lr){2-8}

\multirow{3}{*}{\ewc{}}     & \bcrnn                    &  0.02 \fs {   0.00 } &  0.04  \fs {   0.01 } &  0.00  \fs {  0.00 } &  0.14 \fs {   0.02 } &  0.23  \fs {   0.02 } &  0.03  \fs {  0.00 } \\
                            & \bct                      &  0.13 \fs {   0.02 } &  0.22  \fs {   0.03 } &  0.02  \fs {  0.00 } &  0.23 \fs {   0.01 } &  0.33  \fs {   0.01 } &  0.06  \fs {  0.01 } \\
                            & \bcvilt                   &  0.05 \fs {   0.02 } &  0.09  \fs {   0.03 } &  0.01  \fs {  0.00 } &  0.32 \fs {   0.03 } &  0.48  \fs {   0.03 } &  0.06  \fs {  0.01 } \\
\cmidrule(lr){2-8}

\multirow{3}{*}{\packnet{}} & \bcrnn                    &  0.13 \fs {   0.00 } &  0.21  \fs {   0.01 } &  0.03  \fs {  0.00 } &  0.27 \fs {   0.03 } &  0.38  \fs {   0.03 } &  0.06  \fs {  0.01 } \\
                            & \bct                      &  0.22 \fs {   0.01 } &  0.08  \fs {   0.01 } &  \topthreeboxit 0.25  \fs {  0.00 } &  0.55 \fs {   0.01 } &  0.07  \fs {   0.02 } &  \toponeboxit 0.63  \fs {  0.00 } \\
                            & \bcvilt                   &  0.36 \fs {   0.01 } &  0.14  \fs {   0.01 } &   \toponeboxit 0.34  \fs {  0.01 } & 0.57 \fs {   0.04 } &  0.15  \fs {   0.00 } & \toptwoboxit 0.59  \fs {  0.03 } \\
\cmidrule(lr){2-8}

\multirow{3}{*}{\mtl{}}     & \bcrnn                    &                      &                       &  0.20  \fs {  0.01 }  &                      &                       &  0.61  \fs {  0.00 } \\
                            & \bct                      &                      &                       &  0.48  \fs {  0.01 }  &                      &                       &  0.83  \fs {  0.00 } \\
                            & \bcvilt                   &                      &                       &  0.46  \fs {  0.00 }  &                      &                       &  0.79  \fs {  0.01 } \\
\midrule 
    & & \multicolumn{3}{c}{\liberoobject{}} & \multicolumn{3}{c}{\liberogoal{}}\\
\cmidrule(lr){3-5} \cmidrule(lr){6-8}
\multirow{3}{*}{\seql{}}    & \bcrnn                    &  0.48 \fs {   0.03 } &  0.53  \fs {   0.04 } &  0.15  \fs {  0.01 } &  0.61 \fs {   0.01 } &  0.73  \fs {   0.01 } &  0.16  \fs {  0.00 } \\
                            & \bct                      &  0.78 \fs {   0.04 } &  0.76  \fs {   0.04 } &  0.26  \fs {  0.02 } &  0.77 \fs {   0.01 } &  0.82  \fs {   0.01 } &  0.22  \fs {  0.00 } \\
                            & \bcvilt                   &  0.76 \fs {   0.03 } &  0.73  \fs {   0.03 } &  0.27  \fs {  0.02 } &  0.75 \fs {   0.01 } &  0.85  \fs {   0.01 } &  0.20  \fs {  0.01 } \\
                  \cmidrule(lr){2-8}
\multirow{3}{*}{\er{}}      & \bcrnn                    &  0.30 \fs {   0.01 } &  0.27  \fs {   0.05 } &  0.17  \fs {  0.05 } &  0.41 \fs {   0.00 } &  0.35  \fs {   0.01 } &  0.26  \fs {  0.01 } \\
                            & \bct                      &  0.67 \fs {   0.07 } &  0.43  \fs {   0.04 } &  0.44  \fs {  0.06 } &  0.64 \fs {   0.01 } &  0.34  \fs {   0.02 } &  \topthreeboxit 0.49  \fs {  0.02 } \\
                            & \bcvilt                   &  0.70 \fs {   0.02 } &  0.28  \fs {   0.01 } &  \toptwoboxit 0.57  \fs {  0.01 } &  0.57 \fs {   0.00 } &  0.40  \fs {   0.02 } &  0.38  \fs {  0.01 } \\
                  \cmidrule(lr){2-8}
\multirow{3}{*}{\ewc{}}     & \bcrnn                    &  0.17 \fs {   0.04 } &  0.23  \fs {   0.04 } &  0.06  \fs {  0.01 } &  0.16 \fs {   0.01 } &  0.22  \fs {   0.01 } &  0.06  \fs {  0.01 } \\
                            & \bct                      &  0.56 \fs {   0.03 } &  0.69  \fs {   0.02 } &  0.16  \fs {  0.02 } &  0.32 \fs {   0.02 } &  0.48  \fs {   0.03 } &  0.06  \fs {  0.00 } \\
                            & \bcvilt                   &  0.57 \fs {   0.03 } &  0.64  \fs {   0.03 } &  0.23  \fs {  0.00 } &  0.32 \fs {   0.04 } &  0.45  \fs {   0.04 } &  0.07  \fs {  0.01 } \\
                  \cmidrule(lr){2-8}
\multirow{3}{*}{\packnet{}} & \bcrnn                    &  0.29 \fs {   0.02 } &  0.35  \fs {   0.02 } &  0.13  \fs {  0.01 } &  0.32 \fs {   0.03 } &  0.37  \fs {   0.04 } &  0.11  \fs {  0.01 } \\
                          & \bct                      &  0.60 \fs {   0.07 } & 0.17  \fs {   0.05 } &  \toponeboxit 0.60  \fs {  0.05 } & 0.63 \fs {   0.02 } & 0.06  \fs {   0.01 } & \toptwoboxit 0.75  \fs {  0.01 } \\
                          & \bcvilt                   &  0.58 \fs {   0.03 } &  0.18  \fs {   0.02 } &  \topthreeboxit 0.56  \fs {  0.04 } & 0.69 \fs {   0.02 } &  0.08  \fs {   0.01 } &  \toponeboxit 0.76  \fs {  0.02 } \\
                  \cmidrule(lr){2-8}
\multirow{3}{*}{\mtl{}}   & \bcrnn                    &                      &                       &  0.10  \fs {  0.03 }  &                      &                       &  0.59  \fs {  0.00 } \\
                          & \bct                      &                      &                       &  0.54  \fs {  0.02 }  &                      &                       &  0.80  \fs {  0.01 } \\
                          & \bcvilt                   &                      &                       &  0.78  \fs {  0.02 }  &                      &                       &  0.82  \fs {  0.01 } \\
\bottomrule


    \end{tabular}
    }
    \caption{
We present the full results of all networks and algorithms on all four task suites. For each task suite, we highlight the top three AUC scores among the combinations of the three lifelong learning algorithms and the three neural architectures. The best three results are highlighted in \textbf{\textcolor{top1_boxit_color!90}{magenta}} (the best), \textbf{\textcolor{top2_boxit_color!50}{light magenta}} (the second best), and \textbf{\textcolor{top3_boxit_color!20}{super light magenta}} (the third best), respectively.
    }
    \label{tab:algorithm}
\end{table}

To better illustrate the performance of each lifelong learning agent throughout the learning process, we present plots that show how the agent's performance evolves over the stream of tasks. Firstly, we provide plots that compare the performance of the agent using different lifelong learning algorithms while fixing the policy architecture (refer to Figure~\ref{fig:algo_resnet_rnn},\ref{fig:algo_resnet_t}, and \ref{fig:algo_vit_t}). Next, we provide plots that compare the performance of the agent using different policy architectures while fixing the lifelong learning algorithm (refer to Figure\ref{fig:arch_ewc},~\ref{fig:arch_er}, and \ref{fig:arch_packnet})

\begin{figure}[t!]
    \centering
    \includegraphics[width=\textwidth]{figures/algo_resnet_rnn.png}
    \caption{We compare the performance of different algorithms using the \bcrnn{} policy architecture in Figure~\ref{fig:algo_resnet_rnn}. The $y$-axis represents the success rate, and the $x$-axis shows the agent's performance on each of the 10 tasks in a specific task suite over the course of learning. For example, the upper-left plot in the figure displays the agent's performance on the first task as it learns the 10 tasks sequentially.}
    \label{fig:algo_resnet_rnn}
\end{figure}
\begin{figure}[t!]
    \centering
    \includegraphics[width=\textwidth]{figures/algo_resnet_t.png}
    \caption{Comparison of different algorithms using the \bct{} policy architecture. The $y$-axis represents the success rate, while the $x$-axis shows the agent's performance on each of the 10 tasks in a given task suite during the course of learning. For example, the plot in the upper-left corner depicts the agent's performance on the first task as it learns the 10 tasks sequentially.}
    \label{fig:algo_resnet_t}
\end{figure}

\clearpage

\begin{figure}[t!]
    \centering
    \includegraphics[width=\textwidth]{figures/algo_vit_t.png}
    \caption{Comparison of different algorithms using the \bcvilt{} policy architecture. The success rate is represented on the $y$-axis, while the $x$-axis shows the agent's performance on the 10 tasks in a given task suite over the course of learning. For instance, the plot in the upper-left corner illustrates the agent's performance on the first task when learning the 10 tasks sequentially. }
    \label{fig:algo_vit_t}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\textwidth]{figures/arch_ewc.png}
    \caption{Comparison of different architectures with the \ewc{} algorithm. The $y$-axis is the success rate, while the $x$-axis shows the agent's performance on the 10 tasks in a given task suite over the course of learning. For instance, the upper-left plot shows the agent's performance on the first task when learning the 10 tasks sequentially. }
    \label{fig:arch_ewc}
\end{figure}

\clearpage

\begin{figure}[t!]
    \centering
    \includegraphics[width=\textwidth]{figures/arch_er.png}
    \caption{Comparison of different architectures with the \er{} algorithm. The $y$-axis is the success rate, while the $x$-axis shows the agent's performance on the 10 tasks in a given task suite ver the course of learning. For instance, the upper-left plot shows the agent's performance on the first task when learning the 10 tasks sequentially. }
    \label{fig:arch_er}
\end{figure}
\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{figures/arch_packnet.png}
    \caption{Comparison of different architectures with the \packnet{} algorithm. The $y$-axis is the success rate, while the $x$-axis shows the agent's performance on the 10 tasks in a given task suite over the course of learning. For instance, the upper-left plot shows the agent's performance on the first task when learning the 10 tasks sequentially. }
    \label{fig:arch_packnet}
\end{figure}

\clearpage


\subsection{Loss v.s. Success Rates}
\label{appendix:loss-success-rates}
We demonstrate that behavioral cloning loss can be a misleading indicator of task success rate in this section. In supervised learning tasks like image classifications, lower loss often indicates better prediction accuracy. However, this is not, in general, true for decision-making tasks. This is because errors can compound until failures during executing a robot~\citep{ross2011reduction}. Figure~\ref{fig:lsr_resnet_rnn},~\ref{fig:algo_resnet_t} and~\ref{fig:algo_vit_t} plots the training loss and success rates of three lifelong learning methods (\er{}, \ewc{}, and \packnet{}) for comparison. We evaluate the three algorithms on four task suites using three different neural architectures. 

\textcolor{purple}{\textit{Findings:}} We observe that though sometimes \ewc{} has the \textbf{lowest} loss, it did not achieve good success rate. \er{}, on the other hand, can have the highest loss but perform better than \ewc{}. In conclusion, success rates, instead of behavioral cloning loss, should be the right metric to evaluate whether a model checkpoint is good or not.

\clearpage

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{figures/lsr_algo_resnet_rnn.png}
    \caption{Losses and success rates of \er{} (violet), \ewc{} (grey), and \packnet{} (blue) on four task suites with \bcrnn{} policy. The first (second) row shows the loss (success rate) of the agent on task $i$ throughout the \lldm{} procedure.}
    \label{fig:lsr_resnet_rnn}
\end{figure}

\clearpage

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{figures/lsr_algo_resnet_t.png}
    \caption{Losses and success rates of \er{} (violet), \ewc{} (grey), and \packnet{} (blue) on four task suites with \bct{} policy. The first (second) row shows the loss (success rate) of the agent on task $i$ throughout the \lldm{} procedure.}
    \label{fig:lsr_resnet_t}
\end{figure}

\clearpage

\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{figures/lsr_algo_vit_t.png}
    \caption{Losses and success rates of \er{} (violet), \ewc{} (grey), and \packnet{} (blue) on four task suites with \bcvilt{} policy. The first (second) row shows the loss (success rate) of the agent on task $i$ throughout the \lldm{} procedure.}
    \label{fig:lsr_vit_t}
\end{figure}

\clearpage
\subsection{Multitask Success Rate}
\label{appendix:multi-succ}

\begin{table}[h!]
    \centering
\resizebox{\textwidth}{!}{
\begin{tabular}{l c c c}
\toprule
Env Name & \bcrnn{} & \bct{} & \bcvilt{} \\ 
\midrule
KITCHEN SCENE10 close the top drawer of the cabinet & 0.45 & 0.45 & 0.6 \\ 
\midrule
KITCHEN SCENE10 close the top drawer of the cabinet and put the black bowl on top of it & 0.1 & 0.1 & 0.0 \\ 
\midrule
KITCHEN SCENE10 put the black bowl in the top drawer of the cabinet & 0.0 & 0.25 & 0.05 \\ 
\midrule
KITCHEN SCENE10 put the butter at the back in the top drawer of the cabinet and close it & 0.15 & 0.0 & 0.0 \\ 
\midrule
KITCHEN SCENE10 put the butter at the front in the top drawer of the cabinet and close it & 0.2 & 0.0 & 0.0 \\ 
\midrule
KITCHEN SCENE10 put the chocolate pudding in the top drawer of the cabinet and close it & 0.25 & 0.0 & 0.0 \\ 
\midrule
KITCHEN SCENE1 open the bottom drawer of the cabinet & 0.05 & 0.0 & 0.15 \\ 
\midrule
KITCHEN SCENE1 open the top drawer of the cabinet & 0.3 & 0.45 & 0.25 \\ 
\midrule
KITCHEN SCENE1 open the top drawer of the cabinet and put the bowl in it & 0.05 & 0.0 & 0.0 \\ 
\midrule
KITCHEN SCENE1 put the black bowl on the plate & 0.35 & 0.3 & 0.0 \\ 
\midrule
KITCHEN SCENE1 put the black bowl on top of the cabinet & 0.2 & 0.7 & 0.15 \\ 
\midrule
KITCHEN SCENE2 open the top drawer of the cabinet & 0.45 & 0.4 & 0.65 \\ 
\midrule
KITCHEN SCENE2 put the black bowl at the back on the plate & 0.2 & 0.05 & 0.35 \\ 
\midrule
KITCHEN SCENE2 put the black bowl at the front on the plate & 0.1 & 0.35 & 0.0 \\ 
\midrule
KITCHEN SCENE2 put the middle black bowl on the plate & 0.35 & 0.1 & 0.0 \\ 
\midrule
KITCHEN SCENE2 put the middle black bowl on top of the cabinet & 0.5 & 0.1 & 0.6 \\ 
\midrule
KITCHEN SCENE2 stack the black bowl at the front on the black bowl in the middle & 0.25 & 0.05 & 0.25 \\ 
\midrule
KITCHEN SCENE2 stack the middle black bowl on the back black bowl & 0.05 & 0.05 & 0.0 \\ 
\midrule
KITCHEN SCENE3 put the frying pan on the stove & 0.4 & 0.3 & 0.35 \\ 
\midrule
KITCHEN SCENE3 put the moka pot on the stove & 0.15 & 0.0 & 0.4 \\ 
\midrule
KITCHEN SCENE3 turn on the stove & 0.6 & 0.7 & 1.0 \\ 
\midrule
KITCHEN SCENE3 turn on the stove and put the frying pan on it & 0.15 & 0.0 & 0.35 \\ 
\midrule
KITCHEN SCENE4 close the bottom drawer of the cabinet & 0.75 & 0.4 & 0.55 \\ 
\midrule
KITCHEN SCENE4 close the bottom drawer of the cabinet and open the top drawer & 0.2 & 0.0 & 0.05 \\ 
\midrule
KITCHEN SCENE4 put the black bowl in the bottom drawer of the cabinet & 0.25 & 0.3 & 0.25 \\ 
\midrule
KITCHEN SCENE4 put the black bowl on top of the cabinet & 0.8 & 0.6 & 0.85 \\ 
\midrule
KITCHEN SCENE4 put the wine bottle in the bottom drawer of the cabinet & 0.0 & 0.0 & 0.35 \\ 
\midrule
KITCHEN SCENE4 put the wine bottle on the wine rack & 0.05 & 0.0 & 0.2 \\ 
\midrule
KITCHEN SCENE5 close the top drawer of the cabinet & 0.05 & 0.8 & 0.9 \\ 
\midrule
KITCHEN SCENE5 put the black bowl in the top drawer of the cabinet & 0.2 & 0.1 & 0.15 \\ 
\midrule
KITCHEN SCENE5 put the black bowl on the plate & 0.05 & 0.05 & 0.1 \\ 
\midrule
KITCHEN SCENE5 put the black bowl on top of the cabinet & 0.5 & 0.25 & 0.2 \\ 
\midrule
KITCHEN SCENE5 put the ketchup in the top drawer of the cabinet & 0.0 & 0.0 & 0.05 \\ 
\midrule
KITCHEN SCENE6 close the microwave & 0.1 & 0.1 & 0.2 \\ 
\midrule
KITCHEN SCENE6 put the yellow and white mug to the front of the white mug & 0.2 & 0.05 & 0.1 \\ 
\midrule
KITCHEN SCENE7 open the microwave & 0.7 & 0.1 & 0.45 \\ 
\midrule
KITCHEN SCENE7 put the white bowl on the plate & 0.05 & 0.0 & 0.05 \\ 
\midrule
KITCHEN SCENE7 put the white bowl to the right of the plate & 0.05 & 0.05 & 0.15 \\ 
\midrule
KITCHEN SCENE8 put the right moka pot on the stove & 0.15 & 0.0 & 0.1 \\ 
\midrule
KITCHEN SCENE8 turn off the stove & 0.2 & 0.25 & 0.75 \\ 
\midrule
KITCHEN SCENE9 put the frying pan on the cabinet shelf & 0.45 & 0.15 & 0.05 \\ 
\midrule
KITCHEN SCENE9 put the frying pan on top of the cabinet & 0.25 & 0.4 & 0.3 \\ 
\midrule
KITCHEN SCENE9 put the frying pan under the cabinet shelf & 0.15 & 0.45 & 0.15 \\ 
\midrule
KITCHEN SCENE9 put the white bowl on top of the cabinet & 0.1 & 0.1 & 0.15 \\ 
\midrule
KITCHEN SCENE9 turn on the stove & 0.5 & 0.4 & 0.95 \\ 
\midrule
KITCHEN SCENE9 turn on the stove and put the frying pan on it & 0.0 & 0.0 & 0.0 \\ 
\bottomrule
\end{tabular}
}
\end{table}

\clearpage

\begin{table}[h!]
    \centering
\resizebox{\textwidth}{!}{
\begin{tabular}{l c c c}
\toprule
Env Name & \bcrnn{} & \bct{} & \bcvilt{} \\ 
\midrule
LIVING ROOM SCENE1 pick up the alphabet soup and put it in the basket & 0.0 & 0.0 & 0.0 \\ 
\midrule
LIVING ROOM SCENE1 pick up the cream cheese box and put it in the basket & 0.0 & 0.0 & 0.0 \\ 
\midrule
LIVING ROOM SCENE1 pick up the ketchup and put it in the basket & 0.0 & 0.0 & 0.0 \\ 
\midrule
LIVING ROOM SCENE1 pick up the tomato sauce and put it in the basket & 0.0 & 0.0 & 0.0 \\ 
\midrule
LIVING ROOM SCENE2 pick up the alphabet soup and put it in the basket & 0.0 & 0.0 & 0.0 \\ 
\midrule
LIVING ROOM SCENE2 pick up the butter and put it in the basket & 0.0 & 0.0 & 0.0 \\ 
\midrule
LIVING ROOM SCENE2 pick up the milk and put it in the basket & 0.0 & 0.05 & 0.0 \\ 
\midrule
LIVING ROOM SCENE2 pick up the orange juice and put it in the basket & 0.0 & 0.0 & 0.0 \\ 
\midrule
LIVING ROOM SCENE2 pick up the tomato sauce and put it in the basket & 0.0 & 0.05 & 0.0 \\ 
\midrule
LIVING ROOM SCENE3 pick up the alphabet soup and put it in the tray & 0.0 & 0.05 & 0.0 \\ 
\midrule
LIVING ROOM SCENE3 pick up the butter and put it in the tray & 0.0 & 0.3 & 0.0 \\ 
\midrule
LIVING ROOM SCENE3 pick up the cream cheese and put it in the tray & 0.0 & 0.25 & 0.0 \\ 
\midrule
LIVING ROOM SCENE3 pick up the ketchup and put it in the tray & 0.0 & 0.0 & 0.0 \\ 
\midrule
LIVING ROOM SCENE3 pick up the tomato sauce and put it in the tray & 0.0 & 0.25 & 0.0 \\ 
\midrule
LIVING ROOM SCENE4 pick up the black bowl on the left and put it in the tray & 0.0 & 0.4 & 0.2 \\ 
\midrule
LIVING ROOM SCENE4 pick up the chocolate pudding and put it in the tray & 0.0 & 0.2 & 0.25 \\ 
\midrule
LIVING ROOM SCENE4 pick up the salad dressing and put it in the tray & 0.0 & 0.0 & 0.1 \\ 
\midrule
LIVING ROOM SCENE4 stack the left bowl on the right bowl and place them in the tray & 0.0 & 0.0 & 0.0 \\ 
\midrule
LIVING ROOM SCENE4 stack the right bowl on the left bowl and place them in the tray & 0.0 & 0.0 & 0.05 \\ 
\midrule
LIVING ROOM SCENE5 put the red mug on the left plate & 0.0 & 0.0 & 0.05 \\ 
\midrule
LIVING ROOM SCENE5 put the red mug on the right plate & 0.15 & 0.0 & 0.0 \\ 
\midrule
LIVING ROOM SCENE5 put the white mug on the left plate & 0.1 & 0.15 & 0.05 \\ 
\midrule
LIVING ROOM SCENE5 put the yellow and white mug on the right plate & 0.35 & 0.05 & 0.05 \\ 
\midrule
LIVING ROOM SCENE6 put the chocolate pudding to the left of the plate & 0.1 & 0.65 & 0.0 \\ 
\midrule
LIVING ROOM SCENE6 put the chocolate pudding to the right of the plate & 0.05 & 0.55 & 0.0 \\ 
\midrule
LIVING ROOM SCENE6 put the red mug on the plate & 0.0 & 0.2 & 0.0 \\ 
\midrule
LIVING ROOM SCENE6 put the white mug on the plate & 0.0 & 0.2 & 0.0 \\ 
\midrule
STUDY SCENE1 pick up the book and place it in the front compartment of the caddy & 0.0 & 0.0 & 0.05 \\ 
\midrule
STUDY SCENE1 pick up the book and place it in the left compartment of the caddy & 0.2 & 0.05 & 0.0 \\ 
\midrule
STUDY SCENE1 pick up the book and place it in the right compartment of the caddy & 0.0 & 0.1 & 0.0 \\ 
\midrule
STUDY SCENE1 pick up the yellow and white mug and place it to the right of the caddy & 0.0 & 0.3 & 0.35 \\ 
\midrule
STUDY SCENE2 pick up the book and place it in the back compartment of the caddy & 0.35 & 0.3 & 0.0 \\ 
\midrule
STUDY SCENE2 pick up the book and place it in the front compartment of the caddy & 0.25 & 0.1 & 0.0 \\ 
\midrule
STUDY SCENE2 pick up the book and place it in the left compartment of the caddy & 0.25 & 0.45 & 0.05 \\ 
\midrule
STUDY SCENE2 pick up the book and place it in the right compartment of the caddy & 0.0 & 0.2 & 0.0 \\ 
\midrule
STUDY SCENE3 pick up the book and place it in the front compartment of the caddy & 0.2 & 0.0 & 0.0 \\ 
\midrule
STUDY SCENE3 pick up the book and place it in the left compartment of the caddy & 0.4 & 0.45 & 0.15 \\ 
\midrule
STUDY SCENE3 pick up the book and place it in the right compartment of the caddy & 0.0 & 0.05 & 0.05 \\ 
\midrule
STUDY SCENE3 pick up the red mug and place it to the right of the caddy & 0.0 & 0.2 & 0.05 \\ 
\midrule
STUDY SCENE3 pick up the white mug and place it to the right of the caddy & 0.0 & 0.0 & 0.05 \\ 
\midrule
STUDY SCENE4 pick up the book in the middle and place it on the cabinet shelf & 0.05 & 0.2 & 0.05 \\ 
\midrule
STUDY SCENE4 pick up the book on the left and place it on top of the shelf & 0.2 & 0.1 & 0.15 \\ 
\midrule
STUDY SCENE4 pick up the book on the right and place it on the cabinet shelf & 0.0 & 0.25 & 0.05 \\ 
\midrule
STUDY SCENE4 pick up the book on the right and place it under the cabinet shelf & 0.15 & 0.1 & 0.05 \\ 
\bottomrule
\end{tabular}
}
\end{table}



\clearpage


% For robot manipulation tasks, lower loss usually does not necessarily lead to higher success rates. This is due to the compounding error issue during the execution of a task~\citep{ross2011reduction}, where small errors during the trajectory accumulate over time to cause the final failure of the trajectory. Thus a lower averaged loss among the training data cannot react the actual shift situation encountered by the robot during its execution. Here we plot a comparison chart of losses and success rates of 3 lifelong learning methods on LIBERO-SPATIAL with BC-T policy in figure \ref{fig:loss_and_sr}. We can see that, if we choose the loss as our evaluation metrics, \ewc{} works better than \er{} and \packnet{} on almost half of the tasks. However, on success rates, \ewc{} works extremely worse than the other two methods. From another perspective, \er{} has high losses over almost all tasks but it still can achieve some success rates over many tasks. These phenomenon show verify our idea to use success rate as the evaluation metrics for robot manipulation tasks rather than losses. 



\subsection{Attention Visualization}
\label{appendix:attention}
It is also important to visualize the behavior of the robot and its attention maps during the completion of tasks in the lifelong learning process to give us intuition and qualitative feedback on the performance of different algorithms and architectures. We visualize the attention maps of learned policies with~\citet{Greydanus2017VisualizingAU} and compare them in different studies as in \ref{experiment} to see if the robot correctly pays attention to the right regions of interest in each task. 


\paragraph{Perturbation-based attention visualization:}
We use a perturbation-based method~\cite{Greydanus2017VisualizingAU} to extract attention maps from agents. Given an input image $I$, the method applies a Gaussian filter to a pixel location $(i,j)$ to blur the image partially, and produces the perturbed image $\Phi(I, i, j)$. Denote the learned policy as $\pi$ and the inputs to the spatial module (e.g., the last latent representation of resnet or ViT encoder) $\pi_u(I)$ for image $I$. Then we define the saliency score as the Euclidean distance between the latent representations of the original and the blurred images: 
\begin{equation}
    S_{\pi}(i,j) = \frac{1}{2} \bigg|\bigg|\pi_u(I) - \pi_u(\Phi(I, i, j))\bigg|\bigg|^2.
\end{equation}
Intuitively, $S_{\pi}(i,j)$ describes \emph{how much removing information from the region around location $(i, j)$ changes the policy}. In other words, a large $S_{\pi}(i,j)$ indicates that the information around pixel $(i,j)$ is important for the learning agent's decision-making. Instead of calculating the score for every pixel, \cite{Greydanus2017VisualizingAU} found that computing a saliency score for pixel $i$ mod 5 and $j$ mod 5 produced good saliency maps at lower computational costs for Atari games. The final saliency map $P$ is normalized as $P(i,j) = \frac{S_\pi(i,j)}{\sum_{i,j}S_\pi(i,j)}$.

We provide the visualization and our analysis on the following pages.

\clearpage


\textbf{\large Different Task Suites} \\

\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/fig21.png}
    \caption{Attention map comparison among different task suites with \er{} and \bct{}. Each row corresponds to a task suite.}
    % \caption{Attention map comparison among different task suites with \er{} and \bct{}. Each column corresponds to a task suite. For \liberogoal{}, we show the initial states, and for the others, we show intermediate states. The selected tasks are (from the top down): \liberoobject{}: \textit{pick up the cream cheese and place it in the basket}, \textit{pick up the tomato sauce and place it in the basket}, \textit{pick up the milk and place it in the basket}; \liberogoal{}: \textit{put the wine bottle on top of the cabinet}, \textit{put the bowl on top of the cabinet}, \textit{put the bowl on the plate}; \liberospatial{}: \textit{pick up the black bowl next to the ramekin and place it on the plate}, \textit{pick up the black bowl in the top drawer of the wooden cabinet and place it on the plate}, \textit{pick up the black bowl next to the cookie box and place it on the plate}; \liberolong{}: \textit{turn on the stove and put the moka pot on it}, \textit{pick up the book and place it in the back compartment of the caddy}, \textit{put both the alphabet soup and the cream cheese box in the basket}.}
    \label{fig:t1-attention}
\end{figure*}

\textcolor{purple}{\textit{Findings:}}
Figure \ref{fig:t1-attention} shows attention visualization for 12 tasks across 4 task suites (e.g., 3 tasks per suite). We observe that: 
\begin{enumerate}
    \item policies pay more attention to the robot arm and the target placement area than the target object. 
    \item sometimes the policy pays attention to task-irrelevant areas, such as the blank area on the table.
\end{enumerate}
These observations demonstrate that the learned policy use perceptual data for decision-making in a very different way from how humans do. The robot policies tends to spuriously correlate task-irrelevant features with actions, a major reason why the policies overfit to the tasks and do not generalize well across tasks.

% 2) Besides the target objects and regions, policies also pay much attention to some task-irrelevant areas such as the blank areas in the background and the table. This shows that a good attention area cannot directly lead to high success rates on manipulation tasks and vice versa. \bo{I do not understand the logic} \chongkai{I mean the irrelevant areas should not be paid attention to, which is not the result. Meanwhile, these tasks are successfully accomplished. So I say that good/bad attention areas cannot directly lead to high/low success rates.} Since the robot manipulation task consists of both the perception part and the action part, this result may tell us that even if we learn a very good visual perception module and get perfect representations of images, it is still not easy to learn a good policy module. 

\clearpage


\textbf{\large The Same Task over the Course of Lifelong Learning}\\

\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/fig22.png}
    \caption{Attention map of the same state of the task \textit{put both the alphabet soup and the tomato sauce in the basket} from \liberolong{} during lifelong learning. Each row visualizes how the attention maps change on the first task with one of the LL algorithms (\er{} and \packnet{}) and one of the neural architectures (\bct{} and \bcvilt{}). Initial policy is the policy that is trained on the first task. And all the following attention maps correspond to policies after training on the third, fifth, and the tenth tasks. 
    % We visualize how the attention maps change after training on the the third, fifth, and the tenth tasks.
    % We choose 2 LL algorithms (\er{} and \packnet{}) and 2 architectures (\bct{} and \bcvilt{}) and plot the attention maps on the initial states after the training of the first, third, fifth, and tenth tasks.
    }
    \label{fig:lifelong-progress-attention}
\end{figure*}

\textcolor{purple}{\textit{Findings:}}
Figure \ref{fig:lifelong-progress-attention} shows attention visualizations from policies trained with \er{} and \packnet{} using the architectures \bct{} and \bcvilt{} respectively. We observe that:
\begin{enumerate}
    \item The ViT visual encoder's attention is more consistent over time, while the ResNet encoder's attention map gradually dilutes.
    \item PackNet, as it splits the model capacity for different tasks, shows a more consistent attention map over the course of learning.
\end{enumerate}

\clearpage

\textbf{\large Different Lifelong Learning Algorithms} \\

\begin{figure*}[h!]
\centering
\includegraphics[width=0.8\textwidth]{figures/fig23.png}
\caption{Comparison of attention maps of different lifelong learning algorithms with \bct{} on \liberolong{}. Each row shows the same state of a task with different neural architectures. ``Task 5'' refers to the task \textit{put the white mug on the left plate and put the yellow and white mug on the right plate}. ``Task 10'' refers to the task \textit{put the yellow and white mug in the microwave and close it}. The second row shows the policy that is trained on task 10 and gets evaluated on task 5, showing the attention map differences in backward transfer.
% The first row shows the same state of the fifth task \textit{pick up the book and place it in the back compartment of the caddy} with the immediately learned policy. The second row shows the same task but on different states across lifelong learning algorithms and with the policy just learning the ninth task. The third row shows the ninth task \textit{put the yellow and white mug in the microwave and close it} with the immediately learned policy.
}
\label{fig:attn-algo}
\end{figure*}

\textcolor{purple}{\textit{Findings:}}
Figure \ref{fig:attn-algo} shows the attention visualization of three lifelong learning algorithms on \liberolong{} with \bct{} on two tasks (task 5 and task 10). The first and third rows show the attention of the policy on the same task it has just learned. While the second row shows the attention of the policy on the task it learned in the past. We observe that:

\begin{enumerate}
    \item \packnet{} shows more concentrated attention compared against \er{} and \ewc{} (usually just a single mode).
    \item \er{} shares similar attention map with \ewc{}, but \ewc{} performs much worse than \er{}. Therefore, attention can only assist the analysis but cannot be treated as a criterion for performance prediction.
\end{enumerate}

% We can see that: 1) \er{} and \ewc{} generally show less concentrated attention area than \packnet{}; 2) \er{} and \ewc{} can track the robot arm during the completion of the task but \packnet{} cannot do that; 3) Compared to \er{} and \packnet{}, \ewc{} forgets the correct attention area much faster of the previously learned task during the lifelong learning process; 4) Regarding the robot behaviors (not the attention maps), the policy learned by \ewc{} will always show the same behavior no matter which language instruction is given in the same environment (the last row of \ewc{}), which shows that \ewc{} almost has no lifelong learning ability during the LL process. 

\clearpage 

\textbf{\large Different Neural Architectures}\\

\begin{figure*}[h!]
\centering
\includegraphics[width=0.8\textwidth]{figures/fig24.png}
\caption{Comparison of attention maps of different neural architectures with \er{} on \liberolong{}. Each row shows the same state of a task with different neural architectures. ``Task 5'' refers to the task \textit{put the white mug on the left plate and put the yellow and white mug on the right plate}. ``Task 10'' refers to the task \textit{put the yellow and white mug in the microwave and close it}. The second row shows the policy that is trained on task 10 and gets evaluated on task 5, showing the attention map differences in backward transfer.
% The first row shows the same state of the second task \textit{turn on the stove and put the moka pot on it} with the immediately learned policy. The second row shows the same task but on different states across architectures and with the policy just learning the seventh task. The third row shows the seventh task \textit{put both the alphabet soup and the cream cheese box in the basket} with the immediately learned policy.
}
\label{fig:attn-architecture}
\end{figure*}

\textcolor{purple}{\textit{Findings:}}
Figure \ref{fig:attn-architecture} shows attention map comparisons of the three neural architectures on \liberolong{} with \er{} on two tasks (task 5 and task 10). We observe that:
\begin{enumerate}
    \item ViT has more concentrated attention than policies using ResNet.
    \item When ResNet forgets, the attention is changing smoothly (more diluted). But for ViT, when it forgets, the attention can completely shift to a different location.
    \item When ResNet is combined with LSTM or a temporal transformer, the attention hints at the "course of future trajectory". But we do not observe that when ViT is used as the encoder.
\end{enumerate}
% We show the visualization on 2 immediately learned tasks (the first and the third rows) and one previously learned task (the second row). We can see that the degree of attention concentration of the three architectures is ranked as \bcvilt{} $>$ \bct $>$ \bcrnn{}, which shows that transformer architectures can get more concentrated attention than ResNets and CNNs.

\clearpage

\textbf{\large Different Task Ordering} \\

\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/fig25.png}
    \caption{Attention map comparison among different orderings with \er{} and \bct{} on three selected tasks from \liberolong{}: \textit{put both the alphabet soup and the tomato sauce in the basket}, \textit{put the white mug on the left plate and put the yellow and white mug on the right plate}, and \textit{put the yellow and white mug in the microwave and close it}. Each row corresponds to a specific sequence of task ordering, and the caption of each attention map indicates the order of the task in that sequence.}
    % \textit{put both the cream cheese box and the butter in the basket} and \textit{pick up the book and place it in the back compartment of the caddy} from \liberolong{}.}
    \label{fig:t4-attention}
\end{figure*}

\textcolor{purple}{\textit{Findings:}}
Figure \ref{fig:t4-attention} shows attention map comparisons of three different task orderings. We show two immediately learned tasks from \liberolong{} trained with \er{} and \bct{}. We observe that:
\begin{enumerate}
    \item As expected, learning the same task at different positions in the task stream results in different attention visualization.
    \item There seems to be a trend that the policy has a more spread-out attention when it learns on tasks that are later in the sequence.
\end{enumerate}

% We can see that attention on the robot and the target location emerges in all task orderings, but attention on the target object only emerges in certain orderings.

\clearpage 
\textbf{\large With or Without Pretraining} \\

\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/fig26.png}
    \caption{Attention map comparison between models without/with pretrained models using \bct{} and different lifelong learning algorithms on three selected tasks from \liberolong{}.}
    \label{fig:t5-attention}
\end{figure*}
\clearpage

\textcolor{purple}{\textit{Findings:}}
Figure \ref{fig:t5-attention} shows attention map comparisons between models with/without pretrained models on \liberolong{} with \bct{} and all three LL algorithms. We observe that:
\begin{enumerate}
    \item With pretraining, the policies attend to task-irrelevant regions more easily than those without pretraining. 
    \item Some of the policies with pretraining have better attention to the task-relevant features than their counterparts without pertaining, but their performance remains lower (the last in the second row and the second in the fourth row). This observation, again, shows that there is no positive correlation between semantically meaningful attention maps and the policy's performance.
\end{enumerate}


% We select 2 tasks and show the initial states of immediately learned tasks (the first and the third columns) and the previously learned tasks (the second column). We can see that: 1) with pretraining, most policies lose attention to the target region/object (such as the basket and the book), and pay more attention to the task-irrelevant areas. This may come from that the pretrained 90 tasks vary on target object/region, but have limited kinds of background, thus during the pretraining process, the policy finds the common parts and choose to pay attention to the task-irrelevant areas; 2) after downstream fine-tuning, the policy may gradually regain the attention on the target place/object, as shown in the second column of \ewc{}; 3) pretraining significantly reduces the degree of attention concentration of \packnet{}, which may hurt its performance.

