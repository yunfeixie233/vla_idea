\mysection{Related Work}
\label{sec:related}
This section provides an overview of existing benchmarks for lifelong learning and robot learning. We refer the reader to Appendix \ref{appendix:llalgo} for a detailed review of lifelong learning algorithms.


\myparagraph{Lifelong Learning Benchmarks} Pioneering work has adapted standard vision or language datasets for studying LL. This line of work includes image classification datasets like MNIST~\citep{deng2012mnist}, CIFAR~\citep{krizhevsky2009learning}, and ImageNet~\citep{deng2009imagenet}; segmentation datasets like Core50~\citep{lomonaco2017core50}; and natural language understanding datasets like GLUE~\citep{wang2018glue} and SuperGLUE~\citep{sarlin2020superglue}. Besides supervised learning datasets, video game benchmarks (e.g., Atari~\citep{mnih2013playing}, XLand~\citep{team2021open}, and VisDoom~\citep{kempka2016vizdoom}) in reinforcement learning (RL) have also been used for studying LL. However, LL in standard supervised learning does not involve procedural knowledge transfer, while RL problems in games do not represent human activities. ContinualWorld~\citep{Woczyk2021ContinualWA} modifies the 50 manipulation tasks in MetaWorld for LL. CORA~\citep{powers2021cora} builds four lifelong RL benchmarks based on Atari, Procgen~\citep{cobbe2020leveraging}, MiniHack~\citep{samvelyan2021minihack}, and ALFRED~\citep{shridhar2020alfred}. 
F-SIOL-310~\citep{ayub2021f} and OpenLORIS~\citep{she2020openloris} are challenging real-world lifelong object learning datasets that are captured from robotic vision systems. Prior works have also analyzed different components in a LL agent~\citep{mirzadeh2022architecture,Woczyk2022DisentanglingTI,ermis2022memory}, but they do not focus on robot manipulation problems.


\myparagraph{Robot Learning Benchmarks} A variety of robot learning benchmarks have been proposed to address challenges in meta learning (MetaWorld~\citep{yu2020meta}), causality learning (CausalWorld~\citep{ahmed2020causalworld}), multi-task learning~\citep{james2020rlbench, li2023behavior}, policy generalization to unseen objects~\citep{mu2021maniskill, gu2023maniskill2}, and compositional learning~\citep{mendez2022composuite}. Compared to existing benchmarks in lifelong learning and robot learning, the task suites in \lb{} are curated to address the research topics of \lldm{}. The benchmark includes a large number of tasks based on everyday human activities that feature rich interactive behaviors with a diverse range of objects. Additionally, the tasks in \lb{} are procedurally generated, making the benchmark scalable and adaptable. Moreover, the provided high-quality human demonstration dataset in \lb{} supports and encourages learning efficiency.