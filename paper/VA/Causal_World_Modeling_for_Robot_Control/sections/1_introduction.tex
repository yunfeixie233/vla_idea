\section{Introduction}
\label{sec:introduction}

\begin{figure}[t]
    \centering              
    \includegraphics[width=\textwidth]{figures/teaser_v3.pdf}
    \caption{\method\textbf{: An Autoregressive World Model for Robotic Manipulation.}
    (1) \textbf{Pretraining:} \method is pretrained on diverse in-the-wild videos and robot action data, enabling strong generalization across scenes and objects.
    (2) \textbf{Comprehensive Evaluation:} We conduct extensive experiments on real-world tasks (long-horizon, deformable objects, and precision manipulation) and simulation benchmarks, significantly outperforming state-of-the-art methods including $\pi_{0.5}$.
    (3) \textbf{Versatile Capabilities:} Beyond policy learning, our model supports visual dynamics prediction and inverse dynamics inference from robot videos.
    (4) \textbf{Emergent Properties:} Our causal world modeling approach exhibits long-range temporal memory and strong few-shot adaptation ability.}
    \label{fig:teaser}
\end{figure}

Vision-Language-Action (VLA) models have emerged as a promising paradigm for general-purpose robotic manipulation~\cite{rt1,rt2,kim2024,pi0}, demonstrating impressive capabilities in grounding linguistic instructions into visual perceptions across diverse objects and unstructured environments.
However, beneath their apparent success lies a significant challenge: \emph{representation entanglement}.
Most existing VLAs adopt a feedforward paradigm that maps current observations to action sequences~\cite{diffusionpolicy,act}, requiring a single neural network to simultaneously learn visual scene understanding, physical dynamics, and motor control from a unified supervision signal.
This entanglement can create a bottleneck---the model must compress heterogeneous knowledge, ranging from high-dimensional visual semantics to low-dimensional motor commands, into a shared representation space.
This often leads to limited sample efficiency and suboptimal generalization.
Without explicit modeling of environmental evolution~\cite{dreamerv3,daydreamer,tdmpc2}, reactive policies may rely on pattern matching rather than a principled understanding of physical dynamics.

Recent attempts to bring world modeling into robotic policies span interactive neural simulators (e.g., UniSim~\cite{unisim}), chunk-based video-action diffusion models (e.g., UVA~\cite{uva} and UWM~\cite{uwm}), and offline video generators for subgoal synthesis (e.g. Gen2Act~\cite{gen2act}, Act2Goal~\cite{zhou2025act2goal}).
While conceptually appealing, these approaches face three primary limitations for effective closed-loop control.
First, \emph{the reactivity gap}: chunk/open-loop generation often rolls out long segments without incorporating real-time feedback, making it hard to adapt to disturbances.
Second, \emph{limited long-term memory}: chunk-wise generation can introduce inconsistencies over long horizons when history is not persistently cached.
Third, \emph{causality}: bidirectional attention within a segment allows future tokens to influence past predictions, which diverges from the causal nature of physical reality where the present depends only on the past.
These observations motivate an autoregressive formulation for robust closed-loop reasoning.




We propose \method, an \emph{autoregressive diffusion} world model that addresses these limitations through a unified video-action framework.
Unlike autoregressive language models that predict discrete tokens, our model operates in a continuous latent space via flow matching~\cite{flowmatching,rectifiedflow}, autoregressively generating chunks of video and action representations through iterative denoising.
While our approach conceptually separates visual dynamics prediction and action decoding~\cite{vidar,vpp}, the key architectural insight is to \emph{interleave} video and action tokens into a single autoregressive sequence.
Both modalities are jointly processed through a Mixture-of-Transformers (MoT) architecture~\cite{mot} with shared attention.
Within this unified autoregressive generation process, latent imagination and action inference occur jointly: at each autoregressive step, the model generates predicted future visual states through iterative denoising while simultaneously decoding the corresponding actions, allowing both streams to mutually condition on one another.
This integration, built upon a large-scale pretrained video diffusion backbone~\cite{wan2024video}, offers several advantages:
\emph{(i) Reactive AR loop}: because video and action tokens form a unified sequence, each autoregressive step allows the system to recalibrate based on the latest real-world observation, enabling timely adjustments to both the predicted future and motor commands;
\emph{(ii) Persistent context through KV-cache}: the cached key-value pairs preserve the interleaved video-action trajectory, providing a rich context that helps mitigate temporal drift;
\emph{(iii) Causal consistency}: causal attention masking over the unified sequence ensures that both predicted visual states and action commands are governed by preceding states, respecting the temporal arrow of physical dynamics.
By incorporating real-world observations at each step, this formulation helps mitigate the \emph{distribution drift} that often affects open-loop methods in long-horizon tasks.

A primary challenge in deploying large-scale autoregressive video-action models is inference latency; generating high-fidelity video tokens through iterative denoising is computationally intensive.
We address this through two complementary strategies.
First, we introduce \emph{Noisy History Augmentation}, a training scheme that enables \emph{partial denoising} at inference time.
The key insight is that action decoding does not always require pixel-perfect reconstruction; instead, it can rely on robust semantic structures.
By training the action decoder to predict from partially noisy latent representations, we significantly reduce the computational overhead while maintaining precise action prediction.
Second, we design an \emph{asynchronous coordination} pipeline that overlaps computation with execution: while the robot executes current actions, the world model predicts future visual states and plans subsequent sequences.
This parallelized architecture, combined with variable chunk-size training, facilitates high-frequency closed-loop control without compromising prediction quality.

We evaluate \method across diverse manipulation tasks in both simulation and real-world environments.
Our method demonstrates competitive performance compared to state-of-the-art VLA policies, particularly in long-horizon tasks requiring temporal consistency.
Our contributions are summarized as follows:

\begin{itemize}
    \item \textbf{Autoregressive Video-Action World Modeling:} We introduce an autoregressive diffusion framework that \emph{architecturally unifies} visual dynamics prediction and action inference within a single interleaved sequence while maintaining their \emph{conceptual distinction}. This formulation supports persistent memory through KV cache and causal consistency via attention masking.

    \item \textbf{Mixture-of-Transformers Architecture with Asynchronous Execution:} We design a dual-stream MoT architecture with asymmetric capacity and introduce a partial denoising strategy combined with asynchronous coordination to enable efficient robotic control.

     \item \textbf{Superior Long-Horizon and Precision Performance:}
    Extensive real-world and simulation experiments demonstrate consistent state-of-the-art performance, with particularly strong improvements on long-horizon and high-precision manipulation tasks. Our method also achieves significantly improved sample efficiency and strong generalization to novel scenes and object configurations. 

\end{itemize}
