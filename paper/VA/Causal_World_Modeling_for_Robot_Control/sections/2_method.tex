\section{Preliminary}


\subsection{Flow Matching}

Flow matching~\cite{flowmatching,rectifiedflow,cfm} is a continuous-time generative modeling framework that learns to transform a simple
source distribution (e.g., Gaussian noise) to a target data distribution through a continuous flow.
Given a data sample $x_1$ and a noise sample $\epsilon \sim \mathcal{N}(0, I)$, flow matching defines
a time-dependent vector field $v_s: \mathbb{R}^d \times [0,1] \to \mathbb{R}^d$ that describes the
instantaneous velocity of particles flowing from $\epsilon$ to $x_1$.
The trajectory $x^{(s)}$ evolves according to the ordinary differential equation (ODE):
\begin{equation}
\frac{dx^{(s)}}{ds} = v_s(x^{(s)}), \quad x^{(0)} = \epsilon \sim \mathcal{N}(0, I),
\end{equation}
where $s \in [0, 1]$ denotes the flow time.

The model is trained to predict this vector field by minimizing:
\begin{equation}
\mathcal{L}_{\text{FM}} = \mathbb{E}_{s, \epsilon, x_1} \left[ \| v_\theta(x^{(s)}, s) - \dot{x}^{(s)} \|^2 \right],
\end{equation}
where $\dot{x}^{(s)}$ is the true velocity along the interpolation path, typically defined as $x^{(s)} = (1-s)\epsilon + sx_1$,
giving $\dot{x}^{(s)} = x_1 - \epsilon$.

At inference, samples are generated by solving the learned ODE from $s=0$ to $s=1$:
\begin{equation}
x_1 = \epsilon + \int_0^1 v_\theta(x^{(s)}, s) \, ds.
\end{equation}


\subsection{Video Generation with Conditional Flow Matching}

Recent video generation models~\cite{sora,kling,veo,wan2024video} leverage flow matching to generate videos conditioned on text or images.
These models operate in the latent space of pretrained video autoencoders, where visual observations
are encoded as latent representations $z_t = E(o_t)$ using encoder $E$ (e.g., from video diffusion models).

Given a conditioning signal $c$ (text prompt or initial image), the flow matching model learns to
generate a sequence of latent video frames $\mathbf{z} = \{z_1, \ldots, z_T\}$ by predicting the vector field:
\begin{equation}
v_\theta(\mathbf{z}^{(s)}, s \mid c) = \frac{d}{ds} \mathbf{z}^{(s)},
\end{equation}
where $s \in [0, 1]$ is the flow time and $\mathbf{z}^{(s)}$ represents the latent video at flow step $s$.
The generation process starts from noise $\mathbf{z}^{(0)} = \epsilon \sim \mathcal{N}(0, I)$ and integrates the learned
vector field to obtain the final latent video $\mathbf{z}^{(1)}$, which is then decoded to pixel space.
This bidirectional generation framework enables flexible synthesis from text descriptions or seed images.



\begin{figure}[t]
    {\centering
    \includegraphics[width=\textwidth]{./figures/framework2.pdf}}

    \caption{\raggedright \textbf{Framework overview}: \method is conditioned by \textit{autoregressive diffusion} for unified \textit{video-action world modeling}. 
    We leverage a dual-stream \textit{Mixture-of-Transformers (MoT)} architecture that interleaves video and action tokens within a single sequence. 
    At each autoregressive step, the video stream (initialized from Wan2.2-5B) first predicts future latent visual states via \textit{flow matching}. 
    Then the action stream decodes corresponding actions through \textit{inverse dynamics} conditioning on the predicted visual transitions. 
    }
    \label{fig:overview}
\end{figure}

\section{Method}
\subsection{Problem Statement \& Approach Overview}

We study robotic manipulation as a sequential decision-making problem under partial observability.
At each timestep $t$, the agent receives a visual observation $o_t \in \mathcal{O}$ and executes
an action $a_t \in \mathcal{A}$, which induces a transition in the underlying physical world and
produces the next observation $o_{t+1}$.

\noindent\textbf{Vision-Language-Action (VLA) Policies.}
Most existing VLA policies learn a direct, reactive mapping from observation history to actions:
\begin{equation}
a_t \sim \pi_\theta(\cdot \mid o_t),
\end{equation}
through imitation learning on robot demonstration data.
While this end-to-end approach has shown impressive results, it suffers from a fundamental
coupling problem: the model must simultaneously learn visual scene understanding, physical dynamics,
and motor control from a single supervision signal of paired observations and actions.
This entanglement leads to poor sample efficiency and limited generalization, as the model struggles
to disentangle visual reasoning from action prediction without explicit dynamics modeling.

\noindent\textbf{Our Approach.}
Unlike VLA policies that directly learn action distributions, we adopt a world modeling perspective:
instead of learning $\pi(a_t \mid o_t)$, we predict how the visual world will evolve,
then infer actions based on these predictions.Â·
Our approach operates in two stages:
\begin{equation}
\begin{aligned}
&\text{(Stage 1) Visual dynamics prediction:} \quad &&o_{t+1} \sim p_\theta(\cdot \mid o_{\le t}), \\
&\text{(Stage 2) Inverse dynamics:} \quad &&a_t \sim g_\psi(\cdot \mid o_t, o_{t+1}).
\end{aligned}
\end{equation}
Stage 1 learns to predict future visual observations given observation history.
Stage 2 uses an inverse dynamics model to decode actions from desired visual transitions.
This decomposition enables Stage 1 to leverage large-scale video data for learning physical priors,
while Stage 2 only requires robot demonstrations to ground visual predictions in executable actions.

\noindent\textit{Method Overview.}
Figure~\ref{fig:overview} illustrates the details of our framework.
Our method consists of three key components, detailed in the following subsections:
\textit{(\S\ref{sec:world_model}) Autoregressive Video-Action World Modeling} describes how we
model visual dynamics in latent space and decode actions from predicted state transitions---this is the \emph{core formulation} of our approach;
\textit{(\S\ref{sec:architecture}) \methodname: Unified Architecture \& Training} presents our unified model
for video-action pretraining, including the architecture design and training objective---this is the \emph{instantiation} of our formulation;
\textit{(\S\ref{sec:inference}) Real-time Deployment \& Asynchronous Inference} introduces our deployment
strategy that enables real-time control through parallelized prediction and execution---this is the \emph{practical realization} for robotic control. 

\subsection{Autoregressive Video-Action World Modeling}
\label{sec:world_model}

Previous video world models either focus on open-ended video prediction~\cite{sora} or learn action-conditioned interactive environments~\cite{genie,genie2} primarily for game or simulation domains, which may not directly transfer to precise robotic manipulation.
To leverage rich visual dynamics priors from video data for robot manipulation, we propose a unified video-action world modeling framework that jointly models visual observations and robot actions within a single autoregressive process.
Unlike prior approaches that either decouple video prediction from action inference~\cite{moto,vpp} or rely on bidirectional diffusion within segments~\cite{uwm}, our method unifies video and action within a single \emph{causal autoregressive} framework, enabling persistent memory through KV cache and seamless integration of real-time observations.



\noindent\textbf{World Dynamics with Autoregressive Modeling.}
Recent world models for robotics often adopt bidirectional video generation approaches~\cite{unipi,dreamitate,gen2act,pwa} or learn interactive simulators~\cite{unisim}, which face fundamental limitations for closed-loop control.
Open-loop methods that generate entire long sequences in one shot incur prohibitive computational cost
and cannot incorporate real-time feedback for error correction.
Chunk-based diffusion methods that generate video segments sequentially~\cite{vidar,uwm} suffer from two critical issues:
(1) they lack persistent memory across chunks, as each chunk is generated independently without access to the full history, leading to temporal inconsistencies and drift over long horizons;
(2) the bidirectional attention within each chunk violates causality, preventing seamless integration with real-time observations during execution.

The physical world, however, is inherently causal and autoregressive: the present state depends only on the past, and we cannot observe the future before it occurs.
This fundamental property motivates our autoregressive world modeling approach, which offers three critical advantages over chunk-based diffusion for robotic control:
\textit{(1) Persistent Memory}: by explicitly conditioning on the complete observation history through causal attention and KV cache, the model maintains long-term context and temporal coherence across the entire trajectory, avoiding the ``amnesia'' problem of chunk-based methods;
\textit{(2) Causal Consistency}: the unidirectional dependency structure naturally aligns with closed-loop execution, where new observations can be seamlessly incorporated as they arrive;
\textit{(3) Efficiency}: chunk-wise prediction with parallel generation within each chunk balances computational efficiency with autoregressive flexibility, enabling high-frequency control with real-time error correction.

We formalize this as an autoregressive process: at each step, the world model predicts the next chunk of $K$ video frames using conditional flow matching:
\begin{equation}
o_{t+1:t+K} \sim p_\theta( \cdot \mid o_{\leq t}),
\end{equation}
where tokens within each chunk are generated in parallel via bidirectional attention, while maintaining causal structure across chunks.
This chunk-wise formulation balances generation efficiency with autoregressive flexibility for closed-loop correction.

\noindent\textbf{Video-Action State Encoding.}
Operating directly on pixel-level video observations is computationally prohibitive due to the high dimensionality and redundancy of raw visual data.
We leverage a causal video VAE~\cite{wan2024video} to compress visual observations into compact latent tokens $z_t = E(o_t \mid o_{<t}) \in \mathbb{R}^{N \times 4}$, where $N$ is the number of spatial tokens after passing into video VAE.
By conditioning on previous latent states, the encoder maintains temporal coherence while processing observations sequentially, naturally aligning with our autoregressive world modeling framework.
To align robot actions with visual tokens, we project action vectors to token embeddings $a_t \in \mathbb{R}^{D}$ via a lightweight MLP $\phi(\cdot)$ where $D$ is the dimension of the video token after patchfication, enabling unified interleaving of visual and action tokens as in prior approaches~\cite{vidar, motus}.


\noindent\textbf{Latent Video State Transition.}
While standard video generation models predict future frames based solely on visual history, robotic manipulation requires accounting for the embodiment's physical state and interaction with the environment.
During deployment, the robot's state evolves through continuous interaction: each action modifies the embodiment's configuration (e.g., gripper position, joint angles), which in turn influences how the scene evolves.

In many manipulation settings, actions encode absolute pose information (e.g., end-effector poses in world coordinates), so the action history $a_{<t}$ effectively captures the trajectory of the embodiment's configuration.
Conditioning on action history thus provides knowledge of how the robot has moved and interacted with objects, consistent with prior action-conditioned video/world models~\cite{unisim,vidar,uwm}.
We extend our autoregressive formulation to condition on both observation and action histories:
\begin{equation}
z_{t+1:t+K} \sim p_\theta( \cdot \mid z_{\leq t}, a_{<t}),
\label{eq:vdm}
\end{equation}
where $z_t$ is the latent visual state and $a_t$ is the action token.
This enables the world model to ground predictions in the embodiment's state, ensuring that predicted observations reflect the robot's physical interaction with the scene.

\noindent\textbf{Inverse Dynamics for Action Decoding.}
Once the world model predicts future visual states, we leverage these predictions to plan actions.
Rather than directly predicting actions from current observations, we employ an inverse dynamics model that infers actions by conditioning on desired future observations, enabling the policy to reason about \textit{what action leads to a desired visual outcome}.

However, simply conditioning on the current and next states $(z_t, z_{t+1})$ is insufficient for accurate action prediction.
The action history $a_{<t}$ encodes the embodiment's state trajectory for determining feasible actions, while the observation history $z_{<t}$ provides temporal context for multi-step interactions (e.g., whether an object was previously grasped).
We therefore formulate inverse dynamics as:
\begin{equation}
a_{t:t+K-1} \sim g_\psi(\cdot \mid \hat{z}_{t+1:t+K}, z_{\leq t}, a_{<t}),
\label{eq:idm}
\end{equation}
where the inverse dynamics model $g_\psi$ takes as input the predicted chunk of visual states $\hat{z}_{t+1:t+K}$ inferred by Eq.~\ref{eq:vdm}, observation history $z_{\leq t}$, and action history $a_{<t}$.
This mirrors recent IDM-based policies~\cite{unipi,onexwm,mimicvideo,vidar,tian2025} that leverage future targets to infer feasible actions while maintaining consistency with embodiment dynamics.






\subsection{\methodname: Unified Architecture \& Training}
\label{sec:architecture}

\noindent\textbf{Architecture.}
To jointly model video and action generation, we leverage a dual-stream diffusion transformer architecture that performs conditional flow matching for autoregressive prediction.
Our model consists of two parallel transformer backbones: a video stream initialized from Wan2.2-5B (a large-scale pretrained video generation model with dimension $d_v$~\cite{wan2024video}), and an action stream with same depth but significantly smaller width $d_a \ll d_v$.
This asymmetric design is motivated by the observation that action distributions are inherently simpler than visual data requiring fewer parameters to model effectively while maintaining expressive capacity for visual dynamics.


\noindent\textit{Video Sparsification.}
Video frames exhibit significant temporal redundancy, especially in robotic manipulation where scenes evolve gradually.
We sparsify the video sequence by temporally downsampling frames by a factor of $\tau=4$, reducing visual tokens while improving efficiency~\cite{motus}.
Since actions evolve at higher frequency than visual changes, we interleave the downsampled video tokens with action tokens in temporal order: for each video frame $o_t$, we associate $\tau$ consecutive actions $\{a_{t,1}, a_{t,2}, \ldots, a_{t,\tau}\}$, forming a unified sequence $[z_t, a_{t,1}, a_{t,2}, \ldots, a_{t,\tau}, z_{t+1}, \ldots]$ for joint modeling.
This design means that predicting $K$ video frames corresponds to generating $\tau K$ actions, enabling high-frequency control while maintaining efficient video generation.

\noindent\textit{Mixture-of-Transformer Block.} To enable interaction while preserving modality-specific feature spaces, we employ a Mixture-of-Transformers (MOT) architecture~\cite{motus,mot,bagel}, where video and action tokens are processed by separate transformer blocks at each layer, then fused via cross-modal attention~\cite{motus}.
At each layer, the video and action streams independently compute their query, key, and value matrices using separate QKV projection matrices, maintaining distinct feature spaces for each modality.
To align dimensions for cross-modal fusion, action tokens are first projected to the video dimension via a linear layer, participate in joint self-attention, then projected back to their original dimension via a residual connection that preserves the action-specific representations.
This MOT design allows video and action to mutually influence each other through attention while maintaining separate parameterizations, preventing interference between modality-specific feature representations.
For action decoding, the final action stream outputs are mapped to low-dimensional action vectors via a linear projection head.




\noindent\textit{Action Network Initialization.}
Proper initialization of the action stream is critical for training stability and convergence.
We find that training the action network from scratch leads to unstable optimization and slow convergence, as the action tokens' output distribution initially diverges significantly from the video distribution, disrupting the joint attention mechanism.
To address this, we initialize the action network weights by interpolating the pretrained video weights according to the action dimension, then apply a scaling factor $\alpha = \sqrt{d_v / d_a}$ to preserve output variance, where $d_v$ and $d_a$ are the video and action dimensions.
This initialization strategy ensures that action tokens start with output distributions comparable to video tokens, stabilizing early-stage training and accelerating convergence.

\noindent\textit{Variable Chunk Size Training.}
To enable flexible deployment, we randomly sample the chunk size $K$ from a predefined range during training.
By training with variable chunk sizes (e.g., $K \in [1, 8]$), the model learns to generate coherent predictions across different temporal horizons.
At inference time, this allows freely selecting the chunk size to balance computational efficiency and planning horizon---larger chunks reduce the number of autoregressive steps but require longer per-step computation, while smaller chunks enable more frequent closed-loop correction.
In our experiments, we use $K=4$ for deployment as a practical trade-off.



\noindent\textbf{Teacher Forcing for Unified Video-Action Training.}
In \S\ref{sec:world_model}, we formulated both visual dynamics prediction (Eq. 7) and inverse dynamics (Eq. 8) as autoregressive modeling problems, where each prediction conditions on the history of observations and actions.
This unified autoregressive formulation enables a natural training strategy: we can treat the interleaved video-action sequence as a single unified sequence and train the model using standard next-token prediction, analogous to language modeling in NLP~\cite{transformer}.

\begin{wrapfigure}{r}{0.45\textwidth}
    \centering
    \vspace{-10pt}
    \includegraphics[width=0.45\textwidth]{./figures/attention_mask2.pdf}
    \caption{ \raggedright\textbf{Teacher Forcing Attention Mask}: Causal attention mask for unified video-action pretraining. Each token can only attend to preceding tokens in the temporal sequence.}
    \label{fig:attention_mask}
    \vspace{-10pt}
\end{wrapfigure}


Specifically, given an episode with interleaved tokens, we train the model to predict each token conditioned on all preceding tokens in the sequence.
This is implemented via teacher forcing: during training, we use ground-truth tokens from the dataset as context for predicting subsequent tokens, rather than model-generated predictions.
The causal dependency structure is enforced through attention masking (Figure~\ref{fig:attention_mask})---each token can only attend to tokens that appear earlier in the temporal sequence.


Importantly, teacher forcing is particularly well-suited for robotic manipulation: unlike pure generative modeling where it leads to train-test distribution mismatch, robot policies naturally retrieve real-world observations during deployment, directly matching the training regime.
This formulation offers two key benefits: (1) unifying video and action prediction under a single training objective enables end-to-end learning of world dynamics and action inference; (2) by processing episodes in parallel with causal attention masking, we efficiently optimize both components across all timesteps in a single forward pass.




\noindent\textit{Noisy History Augmentation.}
The primary bottleneck during inference remains video token generation---the number of video tokens are much larger than action tokens, and each requires multiple denoising steps through the flow matching process.
To address this, we introduce a noise augmentation strategy during training that enables \emph{partial denoising} at test time.
The key insight is that action prediction does not require fully denoised video representations; the inverse dynamics model can learn to extract action-relevant information from partially noisy video states.
Specifically, during training, we randomly augment the video history $z_{\leq t}$ with noise following the same interpolation scheme as flow matching:
\begin{equation}
\tilde{z}_{\leq t} = \begin{cases}
(1 - s_{\text{aug}}) \epsilon + s_{\text{aug}} z_{\leq t}, & p = 0.5, \quad s_{\text{aug}} \in [0.5, 1], \; \epsilon \sim \mathcal{N}(0, I) \\
z_{\leq t}, & 1 - p = 0.5
\end{cases}
\end{equation}
This augmentation trains the action decoder to predict actions from partially noisy video representations.

At inference time, this enables a significant speedup: instead of fully denoising video tokens from $s=0$ to $s=1$, we only need to denoise to $s=0.5$, halving the number of denoising steps for video generation while maintaining action prediction quality.



\noindent\textbf{Training Objective.}
We jointly optimize both video and action using flow matching with the noisy history augmentation described above.
For video tokens $z_t$, the dynamics loss supervises velocity field prediction conditioned on (potentially noisy) history:
\begin{equation}
\mathcal{L}_{\mathrm{dyn}} = \mathbb{E}_{t, s, z_{t+1}, \epsilon} \left[ \| v_\theta(z_{t+1}^{(s)}, s ,  \tilde{z}_{\leq t}, a_{<t} | c) - \dot{z}_{t+1}^{(s)} \|^2 \right],
\end{equation}
where $s \in [0,1]$ is flow time, $z_{t+1}^{(s)} = (1-s)\epsilon + sz_{t+1}$ with $\epsilon \sim \mathcal{N}(0, I)$, $\dot{z}_{t+1}^{(s)} = z_{t+1} - \epsilon$, $\tilde{z}_{\leq t}$ is the augmented history (Eq.~10), and $c$ is the language instruction.
For action tokens $a_t$, the inverse dynamics loss conditions on current and next observations:
\begin{equation}
\mathcal{L}_{\mathrm{inv}} = \mathbb{E}_{t, s, a_t, \epsilon} \left[ \| v_\psi(a_t^{(s)}, s, \tilde{z}_{\leq t+1}, a_{<t} | c) - \dot{a}_t^{(s)} \|^2 \right],
\end{equation}
where $a_t^{(s)} = (1-s)\epsilon + sa_t$ with $\epsilon \sim \mathcal{N}(0, I)$, $\tilde{z}_{t}, \tilde{z}_{t+1}$ are the (potentially noisy) current and next video tokens, and $c$ is the language instruction.
The complete objective is $\mathcal{L} = \mathcal{L}_{\mathrm{dyn}} + \lambda \mathcal{L}_{\mathrm{inv}}$.




\subsection{Real-time Deployment \& Asynchronous Inference}
\label{sec:inference}

\noindent\textbf{KV Cache for Efficient Autoregressive Inference.}
Our autoregressive formulation naturally enables KV cache acceleration during inference.
Since each prediction step conditions on the history of observations and actions, we cache the key-value pairs from previous tokens to avoid redundant computation.
At each autoregressive step, only the new tokens (current observation and predicted actions) require full attention computation, while cached history tokens are reused.
Algorithm~\ref{alg:kv_inference} describes the complete inference procedure with KV cache.

\begin{algorithm}[t]
\caption{KV Cache Inference}
\label{alg:kv_inference}
\begin{algorithmic}[1]
\Require Initial observation $o_0$, chunk size $K$, KV cache $\mathcal{C}$
\State $z_0 \gets E(o_0)$, $\mathcal{C} \gets \{z_0\}$
\State $t \gets 0$

\MainLoop
    \State Sample $\epsilon \sim \mathcal{N}(0, I)$ \Comment{Generate video chunk (integrate to $s=0.5$)}
    \State $\tilde{z}_{t+1:t+K} \gets \epsilon + \int_0^{0.5} v_\theta(z^{(s)}_{t+1:t+K}, s \mid \mathcal{C}) \, ds$
   
    \State Sample $\epsilon \sim \mathcal{N}(0, I)$ \Comment{Generate action chunk (integrate to $s=1$)}
    \State $a_{t:t+K-1} \gets \epsilon + \int_0^{1} v_\psi(a^{(s)}_{t:t+K-1}, s \mid \tilde{z}_{t:t+K}, \mathcal{C}) \, ds$
    
    \For{$i = t$ \textbf{to} $t+K-1$}
        \State Execute $a_i$, receive $o_{i+1}$ \Comment{Execute and collect observations}
        \State $z_{i+1} \gets E(o_{i+1})$
    \EndFor
    
    \State $\mathcal{C} \gets \mathcal{C} \cup \{z_{t+1:t+K}, a_{t:t+K-1}\}$ \Comment{Update KV cache}
    \State $t \gets t + K$
\EndMainLoop %
\end{algorithmic}
\end{algorithm}






\noindent\textbf{Asynchronous Prediction and Execution.}
Despite the efficiency gains from KV cache and partial denoising, autoregressive prediction still incurs non-negligible latency that can violate real-time control requirements.
To address this, we introduce an asynchronous inference strategy that pipelines action prediction with execution, effectively hiding prediction latency.
We illustrate the difference between synchronous and asynchronous inference in \cref{fig:async}.

The key insight is to overlap computation with execution (\cref{fig:async}B): While the robot executes the current action chunk $a_t$, the model simultaneously predicts the subsequent action chunk $a_{t+1}$ conditioned on the most recent real observation $z_{t-1}$ (received after the execution of $a_{t-1}$).
For simplicity, we use $z_t$ to denote latent observations (ignoring the video VAE compression) instead of $o_t$ in this section.
We discard all history data before timestamp $t-1$ and use the hat notation $\hat{~}$ to mark predicted visual content. Consequently, the model's active context is limited to the executed action chunk $a_{t-1}$, the recent ground-truth observation $z_{t-1}$, the currently executing action $a_t$, and its corresponding visual forecast $\hat{z}_t$. A naive auto-regressive implementation (\cref{fig:async}B-1) is to store these tokens into the KV cache and predict $\hat{z}_{t+1}$. However, we observed that such a design frequently leads to open-loop degradation and trajectory drift. Because the video generative model inherently favors temporal smoothness, it tends to "continue" the hallucinated video $\hat{z}_t$ while ignoring the critical physical feedback provided by the real observation $z_{t-1}$, eventually causing the model to lose its capacity to react to the environment.

To mitigate this, we introduce a Forward Dynamics Model (FDM) grounded step into our inference pipeline (\cref{fig:async}B-2). Instead of relying on stale forecasts, we replace it by executing a forward dynamics pass: the model uses the recent feedback $z_{t-1}$ and "imagines" the resulting visual state $z_{t}$ after applying action $a_t$. By caching this feedback-grounded prediction instead of a stale forecast, we force the model to re-align with environmental feedback before predicting $z_{t+1}$. This design enhances our asynchronous algorithm into a robust closed-loop system, enabling the robot to effectively perceive and react to real-world changes. 

Algorithm~\ref{alg:async_inference} formalizes this asynchronous pipeline. During post training, we additionally incorporate a forward dynamics prediction loss:
\begin{equation}
\mathcal{L}_{\mathrm{fdm}} = \mathbb{E}_{t, s, \hat{z}_{t+1}, \epsilon} \left[ \| v_\psi(\tilde{z}_{t+1}, s , z_t, a_t, \tilde{z}_{<t}, \hat{a}_{<t} | c) - \dot{z}_{t+1}^{(s)} \|^2 \right],
\end{equation}



\begin{figure}[t]
    {\centering
    \includegraphics[width=\textwidth]{./figures/async.pdf}}

    \caption{\raggedright \textbf{Asynchronous pipeline design overview}: The traditional synchronous pipeline (A) suffers from delays caused by blocked computations, while the asynchronous pipeline (B) addresses this issue by enabling parallel computation and execution. However, a naive asynchronous implementation (B-1) relies on outdated visual predictions. In contrast, we improve and refine asynchronous prediction through forward dynamic prediction (B-2), which updates stale predictions with recent real-world observations.
    }
    \label{fig:async}
\end{figure}

\begin{algorithm}[t]
\caption{Asynchronous Inference and Execution}
\label{alg:async_inference}
\begin{algorithmic}[1] 
\Require Initial observation $o_0$, chunk size $K$, KV cache $\mathcal{C}$

\State $z_0 \gets E(o_0)$; $\mathcal{C} \gets \{z_0\}$
\State $\tilde{z}_{1:K}, a_{0:K-1} \gets \Func{Predict}(\mathcal{C})$ \Comment{Cold Start}
\State $\Var{ObsQueue} \gets \emptyset$ \Comment{Thread-safe queue for incoming real observations}
\State $t \gets 0$

\State \textbf{loop}

    \State \hspace{\algorithmicindent} \textbf{parallel}:
    
    \Statex \hspace{\algorithmicindent}\IndBranch \textbf{Branch A: Robot Execution}
    
    \State \hspace{\algorithmicindent}\IndCode \tikzmark{markA_start}\textbf{async} $\Func{Executor}(a_{t:t+K-1}, \Var{ObsQueue})$ \Comment{Execute pre-computed actions}\tikzmark{markA_end}
    
    \Statex \hspace{\algorithmicindent}\IndBranch \textbf{Branch B: Inference with FDM Grounding}
    
    \State \hspace{\algorithmicindent}\IndCode \tikzmark{markB_start}\textbf{if} $t > 0$ \textbf{then}
    
        \Statex \hspace{\algorithmicindent}\IndInner $o_{t-K+1:t} \gets \Var{ObsQueue.dequeue}()$ \Comment{Get real observation}
        \Statex \hspace{\algorithmicindent}\IndInner $z_{t-K+1:t} \gets E(o_{t-K+1:t})$
        \Statex \hspace{\algorithmicindent}\IndInner $\mathcal{C} \gets \mathcal{C} \cup \{z_{t-K+1,t}, a_{t-K:t-1}\}$ \Comment{Cache feedback}
        
    \Statex \hspace{\algorithmicindent}\IndCode \textbf{end if}
    
    \Statex \hspace{\algorithmicindent}\IndCode $\mathcal{C}_{\text{tmp}} \gets \mathcal{C} \cup \{a_{t:t+K-1}\}$ \Comment{Cache action being executed}
    \Statex \hspace{\algorithmicindent}\IndCode $z_{t+1:t+K} \gets \Func{FDM}(\mathcal{C}_{\text{tmp}})$ \Comment{Imagine visual outcome}
    \Statex \hspace{\algorithmicindent}\IndCode $\mathcal{C}_{\text{tmp}} \gets \mathcal{C}_{\text{tmp}} \cup \{z_{t+1:t+K}\}$ \Comment{Update cache}
    \Statex \hspace{\algorithmicindent}\IndCode $\tilde{z}_{t+K+1:t+2K}, a_{t+K:t+2K-1} \gets \Func{Predict}(\mathcal{C}_{\text{tmp}})$
    
    \Statex \hspace{\algorithmicindent}\IndCode $t \gets t + K$\tikzmark{markB_end}

\State \textbf{end loop}

\end{algorithmic}

\DrawScopeLine{markA_start}{markA_end}{0.8em}{2.2ex}{0.8ex}
\DrawScopeLine{markB_start}{markB_end}{0.8em}{2.2ex}{0.8ex}

\end{algorithm}






