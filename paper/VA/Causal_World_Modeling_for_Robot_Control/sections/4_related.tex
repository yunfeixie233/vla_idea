\section{Related Work}
\label{sec:related}

\noindent \textbf{Vision-Language-Action Policies}.
Recent advancements in Embodied AI have witnessed a paradigm shift toward large-scale Vision-Language-Action (VLA) policies. 
By leveraging web-scale knowledge and diverse robot demonstrations, models such as $\pi_{0.5}$~\cite{pi2025pi05}, GR-3~\cite{gr3}, and GR00T-N1~\cite{bjorck2025} achieve remarkable generalizability across various manipulation tasks without relying on hand-crafted rules, modular priors, or restricted action abstractions, enabling a more direct and expressive end-to-end mapping from perception to control.
These policies typically employ pre-trained Vision-Language Models (VLMs) as foundational backbones \cite{pi2025pi05,pi0,gr3,bjorck2025,rt2,kim2024,yang2025_1,xvla}, which provide superior cross-modal understanding and more generalizable action distributions compared to task-specific imitation policies like ACT \cite{act} or Diffusion Policy \cite{diffusionpolicy}.
Efforts have been further devoted to improving the deployability through lightweight backbones \cite{shukor2025,liu2025b,reuss2025}, efficient tokenization \cite{pertsch2025}, real-time inference \cite{rtc,black2025training,tang2025vlash}, or fine-tuning schemes \cite{kim2025,li2025simplevla,jing2025mixture}.
However, despite their prowess in semantic reasoning, a fundamental limitation persists: the pre-training objectives and data distributions of standard VLMs largely overlook the fine-grained system dynamics and low-level trajectories essential for precision manipulation. 
While supervised fine-tuning on expensively collected large-scale robot datasets allows these models to approximate the marginal action distribution \cite{droid,oxe,barreiros2025careful}, they remain deficient in capturing the underlying transition dynamics---specifically, how the physical state of the environment should evolve and will evolve.
Furthermore, most current VLA methods formulate control as a purely reactive mapping from instantaneous observations to actions. This approach inherently fails to account for the historical context necessary to resolve ambiguities in non-Markovian environments. Additionally, the static image-text pre-training inherent in VLMs fails to instill essential temporal priors. Even when augmented with memory modules~\cite{sridhar2025memer, li2025cronusvla,shi2025a}, such models remain unable to reason about the causal and sequential nature of physical interactions.
To address these shortcomings, recent research has pivoted towards generalist robot policies grounded in world models and generative video modeling \cite{onexwm, uva, uwm}. Our \method unifies autoregressive video prediction with action decoding, ensuring that robot control is explicitly conditioned on learned video dynamics. By integrating these two objectives, our framework maintains strict causal consistency, allowing the policy to synchronize physical execution with the predicted visual evolution of the environment.


\noindent \textbf{World Models for Robotic Control}.
Inspired by human reliance on intuitive physics to anticipate environmental changes, world models aim to facilitate effective planning by predicting future dynamics. Existing approaches are generally categorized into three groups based on their state representations. The first category operates in latent space \cite{watter2015embed, li2019propagation, li2024deformnet, shen2024action}, encoding task-relevant features into compact vectors to predict evolution via probabilistic \cite{li2024deformnet, wu2023daydreamer, lusch2018deep} or deterministic methods \cite{xu2019densephysnet, shen2024action}. The second category utilizes 3D point clouds \cite{shi2024robocraft, wang2023dynamic, sulsky1995application}, leveraging Graph Neural Networks (GNNs) to predict geometric evolution \cite{zhang2024adaptigraph, zhang2025particle}, which is particularly effective for manipulating deformable objects \cite{zhang2025particle, wang2023dynamic}. The third category focuses on 2D pixel space, directly predicting future keyframes or video sequences \cite{zhou2025act2goal, du2023learning, kim2026cosmos, zhou2024robodreamer}. Our work aligns with this third category. Within this domain, approaches range from co-training with video generation for representation learning \cite{uva, uwm, bu2025learning} to serving as simulators for policy learning or evaluation \cite{team2025evaluating}. Our research specifically targets methods that predict future frames during execution to condition action generation. However, prior video-conditioned methods predominantly rely on open-loop generation \cite{du2023learning, zhou2025act2goal}, presenting two significant challenges. First, the misalignment between generated videos and real-world dynamics, coupled with cumulative drift from execution errors, often leads to suboptimal performance. Second, the computational intensity of video generation imposes high latency, severely hindering real-time inference. Our method leverages KV Cache and causal masking to continuously update the model's memory with real-world observations. This effectively transitions the system to a closed-loop control mechanism, mitigating error accumulation in long-horizon tasks. Furthermore, we introduce a partial denoising strategy, enabling action generation from intermediate representations without waiting for fully denoised frames. 



