% \newpage

\section{Detailed Experiment Results}
\label{sec:appendix_posttrain_details}
Table~\ref{tab:performance_comparison} and Table~\ref{tab:real_success_rate_extended} present a detailed per-task comparison of our \modellarge{} and the Diffusion Policy baseline across our simulation benchmarks and real-world benchmarks, respectively. We train both models on datasets of varying sizes --- 30, 100, and 300 demonstrations for simulation benchmarks, and 10\% and full data for real-world benchmarks. As expected, performance improves steadily for both models with increasing dataset sizes.  Meanwhile, our model consistently outperforms the baseline across all benchmarks and dataset sizes, indicating better generalization and sample efficiency.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{assets/sim_posttraining.pdf}
    \caption{Average policy success rate on simulated manipulation tasks with varying numbers of demonstrations.}
    \label{fig:sim_posttraining}
\end{figure}

\begin{table}[t]
\centering
\footnotesize
\caption{\centering Simulation Evaluation Results with Models Trained with Different Dataset Sizes.}
\label{tab:performance_comparison}
\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{Task} & \multicolumn{3}{c}{Diffusion Policy} & \multicolumn{3}{c}{\modellarge{}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
& 30 demos & 100 demos & 300 demos & 30 demos & 100 demos & 300 demos \\
\midrule
\multicolumn{7}{l}{RoboCasa Kitchen (24 tasks, PnP = Pick-and-Place)} \\
Close Double Door & 1.7 & 26.5 & 60.8 & 0.0 & 43.1 & 74.5 \\
Close Drawer & 57.5 & 88.2 & 94.1 & 76.9 & 96.1 & 99.0 \\
Close Single Door & 21.7 & 46.1 & 72.6 & 49.1 & 67.7 & 83.3 \\
Coffee Press Button & 32.5 & 46.1 & 91.2 & 27.8 & 56.9 & 85.3 \\
Coffee Serve Mug & 6.7 & 28.4 & 66.7 & 3.7 & 34.3 & 72.6 \\
Coffee Setup Mug & 0.0 & 19.6 & 32.4 & 0.0 & 2.0 & 22.6 \\
Open Double Door & 0.0 & 9.8 & 18.6 & 0.0 & 12.8 & 14.7 \\
Open Drawer & 15.8 & 42.2 & 61.8 & 9.3 & 42.2 & 79.4 \\
Open Single Door & 36.7 & 42.2 & 57.8 & 20.4 & 54.9 & 58.8 \\
PnP from Cab to Counter & 2.5 & 4.9 & 9.8 & 0.9 & 3.9 & 19.6 \\
PnP from Counter to Cab & 0.0 & 2.9 & 10.8 & 1.9 & 6.9 & 36.3 \\
PnP from Counter to Microwave & 0.0 & 2.0 & 8.8 & 0.0 & 0.0 & 12.8 \\
PnP from Counter to Sink & 0.0 & 0.0 & 13.7 & 0.0 & 1.0 & 9.8 \\
PnP from Counter to Stove & 0.0 & 1.0 & 17.7 & 0.0 & 0.0 & 23.5 \\
PnP from Microwave to Counter & 0.0 & 2.0 & 11.8 & 0.0 & 0.0 & 15.7 \\
PnP from Sink to Counter & 4.2 & 8.8 & 42.2 & 0.0 & 5.9 & 33.3 \\
PnP from Stove to Counter & 1.7 & 2.9 & 23.5 & 0.0 & 0.0 & 29.4 \\
Turn Off Microwave & 63.3 & 53.9 & 52.0 & 47.2 & 57.8 & 70.6 \\
Turn Off Sink Faucet & 21.7 & 63.7 & 72.6 & 49.1 & 67.7 & 72.6 \\
Turn Off Stove & 5.0 & 10.8 & 19.6 & 4.6 & 15.7 & 26.5 \\
Turn On Microwave & 30.0 & 51.0 & 75.5 & 55.6 & 73.5 & 78.4 \\
Turn On Sink Faucet & 31.7 & 27.5 & 63.7 & 33.3 & 59.8 & 62.8 \\
Turn On Stove & 12.5 & 22.6 & 36.3 & 14.8 & 25.5 & 55.9 \\
Turn Sink Spout & 8.3 & 11.8 & 23.5 & 24.1 & 42.2 & 52.9 \\
\midrule
\textbf{RoboCasa Average} & \textbf{14.7} & \textbf{25.6} & \textbf{43.2} & \textbf{17.4} & \textbf{32.1} & \textbf{49.6} \\
\midrule
\multicolumn{7}{l}{DexMimicGen Cross-Embodiment Suite (9 tasks)} \\
Can Sort & 82.8 & 93.1 & 99.4 & 94.8 & 98.0 & 98.0 \\
Coffee & 35.5 & 68.1 & 79.7 & 44.9 & 79.4 & 73.5 \\
Pouring & 37.0 & 62.3 & 68.8 & 54.4 & 71.6 & 87.3 \\
Threading & 4.2 & 18.3 & 27.5 & 3.9 & 37.3 & 60.8 \\
Three Piece Assembly & 10.0 & 32.5 & 63.3 & 10.8 & 43.1 & 69.6 \\
Transport & 7.5 & 25.0 & 53.3 & 7.8 & 48.0 & 61.8 \\
Box Cleanup & 30.0 & 80.8 & 97.5 & 33.3 & 29.4 & 95.1 \\
Drawer Cleanup & 1.7 & 16.7 & 52.5 & 10.8 & 42.2 & 55.9 \\
Lift Tray & 5.0 & 25.0 & 73.3 & 5.8 & 77.5 & 65.7 \\
\midrule
\textbf{DexMG Average} & \textbf{23.7} & \textbf{46.9} & \textbf{68.4} & \textbf{29.6} & \textbf{58.5} & \textbf{74.2} \\
\midrule
\multicolumn{7}{l}{GR-1 Tabletop (24 Tasks)} \\
Cutting Board to Pot & 22.6 & 37.3 & 48.0 & 58.8 & 57.8 & 57.8 \\
Cutting Board to Basket & 19.6 & 42.2 & 29.4 & 43.1 & 61.8 & 56.9 \\
Cutting Board to Tiered Basket & 13.7 & 13.7 & 18.6 & 13.7 & 23.5 & 34.3 \\
Cutting Board to Pan & 28.4 & 48.0 & 57.8 & 67.7 & 65.7 & 68.6 \\
Cutting Board to Cardboard Box & 11.8 & 15.7 & 22.6 & 31.4 & 30.4 & 33.3 \\
Placemat to Bowl & 14.7 & 18.6 & 23.5 & 31.4 & 39.2 & 39.2 \\
Placemat to Plate & 15.7 & 23.5 & 37.3 & 33.3 & 37.3 & 49.0 \\
Placemat to Basket & 15.7 & 25.5 & 41.2 & 50.0 & 46.1 & 55.9 \\
Placemat to Tiered Shelf & 6.9 & 5.9 & 11.8 & 11.8 & 21.6 & 19.6 \\
Plate to Pan & 13.7 & 17.7 & 35.3 & 35.3 & 48.0 & 52.9 \\
Plate to Cardboard Box & 12.8 & 13.7 & 27.5 & 34.3 & 38.2 & 32.4 \\
Plate to Bowl & 15.7 & 18.6 & 31.4 & 41.2 & 42.2 & 34.3 \\
Plate to Plate & 25.5 & 39.2 & 61.8 & 72.6 & 85.3 & 68.6 \\
Tray to Tiered Shelf & 2.0 & 6.9 & 15.7 & 17.7 & 27.5 & 14.7 \\
Tray to Tiered Basket & 12.8 & 34.3 & 39.2 & 33.3 & 49.0 & 45.1 \\
Tray to Plate & 26.5 & 41.2 & 49.0 & 53.9 & 68.6 & 62.8 \\
Tray to Cardboard Box & 21.6 & 37.3 & 40.2 & 51.0 & 55.9 & 54.9 \\
Tray to Pot & 21.6 & 48.0 & 52.9 & 52.0 & 59.8 & 65.7 \\
Wine to Cabinet & 43.1 & 55.9 & 60.8 & 57.8 & 53.9 & 62.8 \\
Place Bottle to Cabinet & 40.2 & 62.8 & 60.8 & 60.8 & 81.4 & 74.5 \\
Place Milk to Microwave & 37.3 & 41.2 & 51.0 & 42.2 & 58.8 & 49.0 \\
Potato to Microwave & 17.7 & 30.4 & 41.2 & 30.4 & 26.5 & 34.3 \\
Cup to Drawer & 24.5 & 32.4 & 36.3 & 36.3 & 44.1 & 40.2 \\
Can to Drawer & 48.0 & 74.5 & 75.5 & 77.5 & 76.5 & 75.5 \\
\midrule
\textbf{GR-1 Average} & \textbf{21.3} & \textbf{32.7} & \textbf{40.4} & \textbf{43.2} & \textbf{50.0} & \textbf{49.3} \\
\bottomrule
\end{tabular}
\end{table}


\begin{table}[htbp]
\centering
\footnotesize
\caption{\centering Success rate on real-world tasks with the GR-1 humanoid robot.}
\label{tab:real_success_rate_extended}
\begin{tabular}{lcccc}
\toprule
 \multirow{2}{*}{Task} & \multicolumn{2}{c}{Diffusion Policy} & \multicolumn{2}{c}{\modellarge{}} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5}
 & 10\% Data & Full Data & 10\% Data & Full Data \\
\midrule
Tray to Plate & 0.0\% & 20.0\% & 40.0\% & 100.0\% \\
Cutting Board to Basket & 0.0\% & 30.0\% & 10.0\% & 100.0\% \\
Cutting Board to Pan & 0.0\% & 60.0\% & 60.0\% & 80.0\% \\
Plate to Bowl & 0.0\% & 40.0\% & 30.0\% & 100.0\% \\
Placemat to Basket & 10.0\% & 60.0\% & 40.0\% & 80.0\% \\
\midrule
\textbf{Pick-and-Place Seen Object Average} & 2.0\% & 42.0\% & 36.0\% & 92.0\% \\
\midrule
Tray to Plate & 0.0\% & 20.0\% & 30.0\% & 80.0\% \\
Cutting Board to Basket & 10.0\% & 20.0\% & 60.0\% & 60.0\% \\
Cutting Board to Pan & 0.0\% & 40.0\% & 40.0\% & 80.0\% \\
Plate to Bowl & 0.0\% & 20.0\% & 10.0\% & 40.0\% \\
Placemat to Basket & 10.0\% & 50.0\% & 30.0\% & 100.0\% \\
\midrule
\textbf{Pick-and-Place Unseen Object Average} & 4.0\% & 30.0\% & 34.0\% & 72.0\% \\
\midrule
\textbf{Pick-and-Place Average} & \textbf{3.0\%} & \textbf{36.0\%} & \textbf{35.0\%} & \textbf{82.0\%} \\
\midrule[1.5pt] % Thicker border for highlighted rows
White Drawer & 6.6\% & 36.4\% & 26.4\% & 79.9\% \\
Dark Cabinet & 0.0\% & 46.2\% & 86.6\% & 69.7\% \\
Wooden Chest & 36.4\% & 33.2\% & 72.9\% & 63.2\% \\
\midrule
\textbf{Articulated Average} & \textbf{14.3\%} & \textbf{38.6\%} & \textbf{62.0\%} & \textbf{70.9\%} \\
\midrule[1.5pt]
Machinery Packing & 20.0\% & 44.0\% & 8.0\% & 56.0\% \\
Mesh Cup Pouring & 0.0\% & 62.5\% & 65.0\% & 67.5\% \\
Cylinder Handover & 0.0\% & 76.5\% & 20.0\% & 86.6\% \\
\midrule
\textbf{Industrial Average} & \textbf{6.7\%} & \textbf{61.0\%} & \textbf{31.0\%} & \textbf{70.0\%} \\
\midrule[1.5pt]
Coordination Part 1 & 45.0\% & 65.0\% & 70.0\% & 80.0\% \\
Coordination Part 2 & 10.0\% & 60.0\% & 30.0\% & 85.0\% \\
\midrule
\textbf{Coordination Average} & \textbf{27.5\%} & \textbf{62.5\%} & \textbf{50.0\%} & \textbf{82.5\%} \\
\midrule[1.5pt] % Thicker border for highlighted rows
\textbf{Average} & \textbf{10.2\%} & \textbf{46.4\%} & \textbf{42.6\%} & \textbf{76.8\%} \\
\bottomrule
\end{tabular}
\end{table}


\section{Additional Qualitative Results}
We provide qualitative rollout examples of our pre-trained  and post-trained \modellarge{} model in \FIGREF{fig:gr1-pretraining-qualitative-examples} and \FIGREF{fig:gr1-posttraining-qualitative-examples}. Our model demonstrates strong language following abilities and generalization to unseen situations in bimanual manipulation tasks. %compared to the Diffusion Policy baseline.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{assets/gr1-pretraining-qualitative-examples.pdf}
    \caption{\textbf{Pre-training Qualitative Example.} While prompting the pretrained \modellarge{} model with a post-training task instruction, we even increase the difficulty by placing the apple to the left of both hands. Despite not having encountered this setup during training, the model successfully places the red apple into the basket via a two-handed handover, albeit with jerkier motion.}
    \label{fig:gr1-pretraining-qualitative-examples}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{assets/gr1-qualitative-examples.pdf}
    \caption{\textbf{Post-training Qualitative Example.} (Top) Post-trained \modellarge{} successfully places the cucumber into the basket, whereas the Diffusion Policy fails due to an inaccurate grasp. (Bottom) The post-trained model successfully picks the lemon from the cutting board and puts it into the pan while the Diffusion Policy remains stuck.}
    \label{fig:gr1-posttraining-qualitative-examples}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=.9\linewidth]{assets/appendix_dream.pdf}
    \caption{\textbf{More Examples of Neural Generated Trajectories.} We highlight 4 key capabilities of neural trajectories: (1) The first three rows shows an example of a multi-view trajectory generated for the post-training in RoboCasa; we achieve this by concatenating the views as a 4-grid video during training. (2) The following two rows show two consecutive sequences, where the initial frame of the latter is from the end of the former, highlighting the possibility of generating trajectories of tasks requiring composition of atomic tasks. (3)  The following two rows illustrate the capability of our models to generate trajectories with articulated objects and liquids, which are known to be very challenging in simulation. (4) The last row is generated from an in-painted initial frame, showcasing that we can generate more diverse videos without having to collect novel initial frames in the real-world and be bound by human labor. We use the red rectangles to indicate the initial frames.}
    \label{fig:appendix-dream}
\end{figure}

\section{Hyperparameters}
We report important hyperparameters used for the pre- and post-training phases in Table~\ref{tab:hyperparameters}. Overall, these two phases share the same values for most of the hyperparameters. 
%
For post-training, we use smaller batch sizes to avoid overfitting when fine-tuning in data-limited settings.
%

\begin{table}[htb]
\centering
\caption{\centering{Training hyperparameters. Pre- and post-training use the same hyperparameters unless specified.}}
\begin{tabular}{lll}
\toprule
\textbf{Hyperparameter} & \textbf{Pre-training Value} & \textbf{Post-training Value}\\
\midrule
Learning rate & 1e-4 \\
Optimizer & AdamW \\
Adam beta1 & 0.95 \\
Adam beta2 & 0.999 \\
Adam epsilon & 1e-8 \\
Weight decay & 1e-5 \\
LR scheduler & cosine \\
Warmup ratio & 0.05 \\
Batch size & 16,384 & 128 or 1024\\
Gradient steps &  200,000 & 20,000 -- 60,000 \\
Backbone's vision encoder & unfrozen \\
Backbone's text tokenizer & frozen \\
DiT & unfrozen \\
\bottomrule
\end{tabular}
\label{tab:hyperparameters}
\end{table}



\section{System Design}

\subsection{Dataset Formats}

Our training corpora build upon the LeRobot dataset format~\citep{cadene2024lerobot}, a widely adopted standard in the open-source robotics community. Developed by Hugging Face, LeRobot aims to lower the barrier to entry for robotics research by providing a standardized format for storing, sharing, and utilizing robot demonstration data. The format has gained significant traction due to its flexibility and the extensive collection of pretrained models and datasets available through the Hugging Face hub.

At its core, the LeRobot dataset format employs a combination of established file formats for efficient storage and access:

\begin{enumerate}
    \item \textbf{Tabular Data:} Robot states, actions, and metadata are stored in parquet files, which offer efficient columnar storage and fast data retrieval. This format enables quick filtering and slicing operations essential for training deep learning models.
    
    \item \textbf{Image and Video Data:} Visual observations are encoded as MP4 video files (or alternatively as PNG image sequences), with references stored in the parquet files. This approach significantly reduces storage requirements while maintaining data accessibility.
    
    \item \textbf{Metadata:} Dataset statistics, episode indices, and other metadata are stored in structured JSON files, providing machine-readable information about the dataset's characteristics.
\end{enumerate}

The format organizes demonstration data into episodes, with each frame containing synchronized observation and action pairs. Each observation typically includes camera imagery (\texttt{observation.images.*}) and robot state information (\texttt{observation.state}), while actions represent the control commands sent to the robot. This organization facilitates both imitation learning, where models learn to predict actions from observations, and reinforcement learning, where models learn to optimize for specific outcomes.


While the LeRobot format provides a solid foundation, our work with cross-embodiment data necessitated additional structure to support richer modality information and more sophisticated training regimes. We have extended the LeRobot format with the following constraints:

\begin{enumerate}
    \item \textbf{Modality configuration file:} We require a \texttt{modality.json} configuration file in the \texttt{meta} directory that explicitly defines the structure of state and action vectors, mapping each dimension to a semantic meaning and provides additional modality-specific information.
    
    \item \textbf{Fine-grained modality specification:} Unlike the standard LeRobot format, which treats state and action as monolithic vectors, our extension splits these vectors into semantically meaningful fields (\eg, end-effector position, orientation, gripper state), each with their own metadata including data types, ranges, and transformation specifications.
    
    \item \textbf{Multiple annotation support:} We extend the format to support multiple annotation types (\eg, task descriptions, validity flags, success indicators) within a single dataset, following the LeRobot convention of storing indices in the parquet file with the actual content in separate JSON files.
    
    \item \textbf{Rotation type specification:} Our format explicitly specifies the representation used for rotational data (\eg, quaternions, Euler angles, axis-angle), enabling proper handling of rotational transformations during training.
\end{enumerate}

Our extended format offers several key benefits for training VLA models:

\begin{enumerate}
    \item \textbf{Semantic clarity:} By explicitly defining the structure and meaning of each dimension in state and action vectors, our format enhances interpretability and reduces errors during data preprocessing and model training.
    
    \item \textbf{Flexible transformations:} The fine-grained modality specification enables sophisticated, field-specific normalization and transformation during training. For example, rotational data can be properly normalized and augmented according to its specific representation.
    
    \item \textbf{Multi-modal learning support:} The extended format naturally accommodates the diverse data types required for VLA models, including visual observations, state information, action commands, and language annotations, while maintaining clear relationships between these modalities.
    
    \item \textbf{Improved data validation:} The explicit structure enables more thorough validation of datasets, reducing the risk of training with malformed or inconsistent data.
    
    \item \textbf{Enhanced interoperability:} While adding constraints, our format maintains backward compatibility with the LeRobot ecosystem, allowing us to leverage existing tools and datasets while enabling more sophisticated modeling approaches.
\end{enumerate}

The extended format strikes a balance between standardization and flexibility, providing a clear structure for common robotics data while accommodating the specific needs of VLA models. This approach has proven valuable in our work, enabling more efficient training and improved model performance while maintaining compatibility with the broader robotics research community.

\subsection{Standardized Action Spaces}

For the above datasets, we make a \textbf{best-effort unification} of action and state spaces to ensure consistency across different embodiments and control modalities. Several key practices are applied to achieve this standardization:

\begin{enumerate}
    \item \textbf{End-effector rotation state normalization:} State end-effector rotations are converted to a \textit{6D rotation representation} to avoid singularities and discontinuities in traditional Euler angles.
    \item \textbf{End-effector rotation action standardization:} End-effector rotation actions are expressed in \textit{axis-angle representation}, providing a compact and smooth parameterization for rotation control.
    \item \textbf{State and action scaling:} \textit{Min-max normalization} is applied to joint states, joint actions, end-effector state positions, and end-effector action positions and rotations, ensuring uniform value ranges across different robots.
    \item \textbf{Consistent ordering:} The arrangement of state and action vectors follows a standardized sequence: \textit{end-effector rotation, end-effector position, and gripper closeness}, ordered from the \textit{left arm to the right arm} (if applicable).
\end{enumerate}

\section{Additional Training Details}
\paragraph{Auxiliary Object Detection Loss}

To enhance the model's spatial understanding, we introduce an auxiliary object detection loss during training. In addition to predicting actions, the model must also localize the object of interest based on the given language instruction. Specifically, for each frame in a trajectory segment, we annotate the bounding box of the target object using the OWL-v2 object detector~\citep{minderer2023owl}. We then compute the normalized center coordinates of the bounding box, $x_{gt}$, by dividing its $x$ and $y$ coordinates by the image width and height, respectively. 
To predict the 2D coordinates, we append a linear layer atop the final vision-language embedding tokens and optimize using a squared loss: $ L_{det} = \| \textbf{x}_{pred} - \textbf{x}_{gt} \|^2$.
Thus, the final loss is given by: $L = L_{fm} + L_{det}$.

\paragraph{Neural Trajectory Generation}
\label{sec:appendix_dream}
We finetune WAN2.1-I2V-14B~\citep{wan2.1} using LoRA~\citep{hu2022lora} on collected teleoperation trajectories. The trajectories are uniformly downsampled to 81 frames at 480P resolution for finetuning. The resulting image-to-video model generates neural trajectories that capture all possible ``counterfactual scenarios'' in the real world. To ensure quality, we filter out generated videos that do not accurately follow the given language instructions. Specifically, we sample 8 frames from each video and prompt a commercial-grade multimodal LLM to assess whether it adheres to the instructions. Videos that fail this criterion undergo re-captioning, with the videos downsampled to 16 frames at 256P resolution for this process.

\paragraph{IDM Model Training}
\label{sec:appendix_idm}
We train an inverse dynamics model (IDM) by conditioning on two images (current and the future frame) within a trajectory and train to generate action chunks between the two image frames. From preliminary experiments, we observed that adding state information or more image frames did not significantly improve the action prediction performance on the validation set. For the IDM model architecture, we use the Diffusion Transformer module (System 1) with SigLIP-2 vision embeddings and train with a flow-matching objective. We train the IDM model for each embodiment for 30K or 60K, depending on the size of the training set. After training, we pseudo-label the actions given the two images (with the same action horizon as training) for each step of the neural trajectories. 

\begin{figure}[thb!]
\centering
\includegraphics[width=0.95\textwidth]{assets/human_video.pdf}
% \vspace{-8em}
\caption{\textbf{Human Egocentric Video Dataset Samples.} We use seven human video datasets for pre-training. The images above show examples from each of the seven datasets with their corresponding language annotations.}
\label{fig:human_sample}
\end{figure}


\begin{table}[htbp]
\centering
\footnotesize
\caption{\centering Pre-training Dataset Statistics}
\begin{tabular}{llllll}
\toprule
Dataset & Length (Frames) & Duration (hr) & FPS & Camera View & Category \\ 
\midrule
GR-1 Teleop Pre-Training & 6.4M & 88.4 & 20 & Egocentric & Real robot \\
DROID (OXE) & 23.1M & 428.3 & 15 & Left, Right, Wrist & Real robot \\
RT-1 (OXE) & 3.7M & 338.4 & 3 & Egocentric & Real robot \\
Language Table (OXE) & 7.0M & 195.7 & 10 & Front-facing & Real robot \\
Bridge-v2 (OXE) & 2.0M & 111.1 & 5 & Shoulder, left, right, wrist & Real robot \\
MUTEX (OXE) & 362K & 5.0 & 20 & Wrist & Real robot \\
Plex (OXE) & 77K & 1.1 & 20 & Wrist & Real robot \\
RoboSet (OXE) & 1.4M & 78.9 & 5 & Left, Right, Wrist & Real robot \\
Agibot-Alpha & 213.8M & 1,979.4 & 30 & Egocentric, left, right & Real robot \\
RH20T-Robot & 4.5M & 62.5 & 20 & Wrist & Real robot \\
Ego4D & 154.4M & 2,144.7 & 20 & Egocentric & Human \\
Ego-Exo4D & 8.9M & 123.0 & 30 & Egocentric & Human \\
Assembly-101 & 1.4M & 19.3 & 20 & Egocentric & Human \\
HOI4D & 892K & 12.4 & 20 & Egocentric & Human \\
HoloAssist & 12.2M & 169.6 & 20 & Egocentric & Human \\
RH20T-Human & 1.2M & 16.3 & 20 & Egocentric & Human \\
EPIC-KITCHENS & 2.3M & 31.7 & 20 & Egocentric & Human \\
GR-1 Simulation Pre-Training & 125.5M & 1,742.6 & 20 & Egocentric & Simulation  \\
GR-1 Neural Videos & 23.8M & 827.3 & 8 & Egocentric & Neural-generated \\
\midrule
Total robot data & 262.3M & 3,288.8 & -- & -- & -- \\ 
Total human data & 181.3M & 2,517.0 & -- & -- & -- \\
Total simulation data & 125.5M & 1,742.6 & -- & -- & -- \\
Total neural data & 23.8M &  827.3 & -- & -- & -- \\
\bottomrule
Total & 592.9M & 8,375.7 & -- & -- & -- \\ 
\bottomrule
\end{tabular}
\label{tab:dataset_stats}
\end{table}

% \begin{figure}[h]
% % \centering
% \includegraphics[width=\textwidth]{assets/qual_sim.pdf}
% \caption{\textbf{Qualitative Results in Simulation.} We compare trajectories for \modelname{} and the DP baseline. The task is to turn the sink spout to the right. \modelname{} successfully completes the task, whereas the DP baselines instead attempt to move to the left.}
% \label{fig:qual_sim}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{assets/multiview_dream1.pdf}
%     \caption{Multi-view neural trajectories in simulation environments. We use a red rectangle to crop the conditioned initial frame.}
%     \label{fig:multiview-1}
% \end{figure}

% Please add the following required packages to your document preamble:
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}



% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{assets/multiview_dream2.pdf}
%     \caption{Multi-view neural trajectories in simulation environments. We use a red rectangle to crop the conditioned initial frame.}
%     \label{fig:multiview-2}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{assets/multiview_dream3.pdf}
%     \caption{Multi-view neural trajectories in simulation environments. We use a red rectangle to crop the conditioned initial frame.}
%     \label{fig:multiview-3}
% \end{figure}


% \begin{table}[]
% \begin{tabular}{lccc}
% \toprule
%          & \multicolumn{3}{c}{Dataset Size} \\
%          & low & medium & high \\ \midrule
% baseline &  14.72   &    25.61    &  43.22    \\
% groot    &  17.44   &    32.07    &  49.59  \\ \bottomrule
% \end{tabular}
% \end{table}