@article{atzmon2024edify,
  title={Edify Image: High-Quality Image Generation with Pixel Space Laplacian Diffusion Models},
  author={NVIDIA},
  journal={arXiv preprint arXiv:2411.07126},
  year={2024}
}

@article{nvidia2024edify3d,
  title={Edify 3D: Scalable High-Quality 3D Asset Generation},
  author={NVIDIA},
  journal={arXiv preprint arXiv:2411.07135},
  year={2024}
}
@inproceedings{robocasa2024,
  title={RoboCasa: Large-Scale Simulation of Everyday Tasks for Generalist Robots},
  author={Soroush Nasiriany and Abhiram Maddukuri and Lance Zhang and Adeet Parikh and Aaron Lo and Abhishek Joshi and Ajay Mandlekar and Yuke Zhu},
  booktitle={Robotics: Science and Systems (RSS)},
  year={2024}
}
@inproceedings{jiang2024dexmimicen,
      title     = {DexMimicGen: Automated Data Generation for Bimanual Dexterous Manipulation via Imitation Learning},
      author    = {Jiang, Zhenyu and Xie, Yuqi and Lin, Kevin and Xu, Zhenjia and Wan, Weikang and Mandlekar, Ajay and Fan, Linxi and Zhu, Yuke},
      journal   = {arXiv preprint arXiv:2410.24185},
      year      = {2024}
}

@inproceedings{mandlekar2023mimicgen,
    title={MimicGen: A Data Generation System for Scalable Robot Learning using Human Demonstrations},
    author={Mandlekar, Ajay and Nasiriany, Soroush and Wen, Bowen and Akinola, Iretiayo and Narang, Yashraj and Fan, Linxi and Zhu, Yuke and Fox, Dieter},
    booktitle={Conference on Robot Learning},
    year={2023}
}

@inproceedings{robomimic2021,
  title={What Matters in Learning from Offline Human Demonstrations for Robot Manipulation},
  author={Ajay Mandlekar and Danfei Xu and Josiah Wong and Soroush Nasiriany and Chen Wang and Rohun Kulkarni and Li Fei-Fei and Silvio Savarese and Yuke Zhu and Roberto Mart\'{i}n-Mart\'{i}n},
  booktitle={Conference on Robot Learning (CoRL)},
  year={2021}
}

@article{chi2024diffusionpolicy,
	author = {Cheng Chi and Zhenjia Xu and Siyuan Feng and Eric Cousineau and Yilun Du and Benjamin Burchfiel and Russ Tedrake and Shuran Song},
	title ={Diffusion Policy: Visuomotor Policy Learning via Action Diffusion},
	journal = {The International Journal of Robotics Research},
	year = {2024},
}

@article{eagle2,
  title={Eagle 2: Building Post-Training Data Strategies from Scratch for Frontier Vision-Language Models},
  author={Li, Zhiqi and Chen, Guo and Liu, Shilong and Wang, Shihao and VS, Vibashan and Ji, Yishen and Lan, Shiyi and Zhang, Hao and Zhao, Yilin and Radhakrishnan, Subhashree and others},
  journal={arXiv preprint arXiv:2501.14818},
  year={2025}
}

@article{tschannen2025siglip,
  title={{SigLIP 2}: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features},
  author={Tschannen, Michael and Gritsenko, Alexey and Wang, Xiao and Naeem, Muhammad Ferjad and Alabdulmohsin, Ibrahim and Parthasarathy, Nikhil and Evans, Talfan and Beyer, Lucas and Xia, Ye and Mustafa, Basil and others},
  journal={arXiv preprint arXiv:2502.14786},
  year={2025}
}

@article{allal2025smollm2,
  title={SmolLM2: When Smol Goes Big--Data-Centric Training of a Small Language Model},
  author={Allal, Loubna Ben and Lozhkov, Anton and Bakouch, Elie and Bl{\'a}zquez, Gabriel Mart{\'\i}n and Penedo, Guilherme and Tunstall, Lewis and Marafioti, Andr{\'e}s and Kydl{\'\i}{\v{c}}ek, Hynek and Lajar{\'\i}n, Agust{\'\i}n Piqueres and Srivastav, Vaibhav and others},
  journal={arXiv preprint arXiv:2502.02737},
  year={2025}
}

@article{yang2024cogvideox,
  title={Cogvideox: Text-to-video diffusion models with an expert transformer},
  author={Yang, Zhuoyi and Teng, Jiayan and Zheng, Wendi and Ding, Ming and Huang, Shiyu and Xu, Jiazheng and Yang, Yuanming and Hong, Wenyi and Zhang, Xiaohan and Feng, Guanyu and others},
  journal={arXiv preprint arXiv:2408.06072},
  year={2024}
}

@article{lin2024stiv,
  title={STIV: Scalable Text and Image Conditioned Video Generation},
  author={Lin, Zongyu and Liu, Wei and Chen, Chen and Lu, Jiasen and Hu, Wenze and Fu, Tsu-Jui and Allardice, Jesse and Lai, Zhengfeng and Song, Liangchen and Zhang, Bowen and others},
  journal={arXiv preprint arXiv:2412.07730},
  year={2024}
}

@article{xiang2024pandora,
  title={Pandora: Towards general world model with natural language actions and video states},
  author={Xiang, Jiannan and Liu, Guangyi and Gu, Yi and Gao, Qiyue and Ning, Yuting and Zha, Yuheng and Feng, Zeyu and Tao, Tianhua and Hao, Shibo and Shi, Yemin and others},
  journal={arXiv preprint arXiv:2406.09455},
  year={2024}
}

@article{ren2025videoworld,
  title={VideoWorld: Exploring Knowledge Learning from Unlabeled Videos},
  author={Ren, Zhongwei and Wei, Yunchao and Guo, Xun and Zhao, Yao and Kang, Bingyi and Feng, Jiashi and Jin, Xiaojie},
  journal={arXiv preprint arXiv:2501.09781},
  year={2025}
}

@article{brooks2024video,
  title={Video generation models as world simulators. 2024},
  author={Brooks, Tim and Peebles, Bill and Holmes, Connor and DePue, Will and Guo, Yufei and Jing, Li and Schnurr, David and Taylor, Joe and Luhman, Troy and Luhman, Eric and others},
  journal={URL https://openai. com/research/video-generation-models-as-world-simulators},
  volume={3},
  pages={1},
  year={2024}
}

@inproceedings{pixel_shuffle,
  title={Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network},
  author={Shi, Wenzhe and Caballero, Jose and Husz{\'a}r, Ferenc and Totz, Johannes and Aitken, Andrew P and Bishop, Rob and Rueckert, Daniel and Wang, Zehan},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  year={2016}
}

@inproceedings{peebles2023scalable,
  title={Scalable diffusion models with transformers},
  author={Peebles, William and Xie, Saining},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={4195--4205},
  year={2023}
}

@inproceedings{flowmatching,
  title={Flow Matching for Generative Modeling},
  author={Lipman, Yaron and Chen, Ricky TQ and Ben-Hamu, Heli and Nickel, Maximilian and Le, Matthew},
  booktitle={The Eleventh International Conference on Learning Representations}
}

@INPROCEEDINGS{action_chunking, 
    AUTHOR    = {Tony Z. Zhao AND Vikash Kumar AND Sergey Levine AND Chelsea Finn}, 
    TITLE     = {{Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware}}, 
    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, 
    YEAR      = {2023},
} 

@inproceedings{repa,
  title={Representation alignment for generation: Training diffusion transformers is easier than you think},
  author={Yu, Sihyun and Kwak, Sangkyung and Jang, Huiwon and Jeong, Jongheon and Huang, Jonathan and Shin, Jinwoo and Xie, Saining},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2025},
}

@inproceedings{
ye2025latent,
title={Latent Action Pretraining from Videos},
author={Seonghyeon Ye and Joel Jang and Byeongguk Jeon and Se June Joo and Jianwei Yang and Baolin Peng and Ajay Mandlekar and Reuben Tan and Yu-Wei Chao and Bill Yuchen Lin and Lars Liden and Kimin Lee and Jianfeng Gao and Luke Zettlemoyer and Dieter Fox and Minjoon Seo},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
}


@misc{open_x_embodiment_rt_x_2023,
title={Open {X-E}mbodiment: Robotic Learning Datasets and {RT-X} Models},
author = {{Open X-Embodiment Collaboration} and others},
howpublished  = {International Conference on Robotics and Automation},
year = {2024},
}

@misc{cadene2024lerobot,
    author = {Cadene, Remi and Alibert, Simon and Soare, Alexander and Gallouedec, Quentin and Zouitine, Adil and Wolf, Thomas},
    title = {LeRobot: State-of-the-art Machine Learning for Real-World Robotics in Pytorch},
    howpublished = "\url{https://github.com/huggingface/lerobot}",
    year = {2024}
}

@article{contributors2025agibotworld,
  title={{AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems}},
  author={AgiBot-World-Contributors and others},
  journal={arXiv preprint arXiv:2503.06669},
  year={2025}
}

@inproceedings{wang2023robogen,
  title={RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation},
  author={Wang, Yufei and Xian, Zhou and Chen, Feng and Wang, Tsun-Hsuan and Wang, Yian and Fragkiadaki, Katerina and Erickson, Zackory and Held, David and Gan, Chuang},
  booktitle={International Conference on Machine Learning},
  year={2024},
}


@inproceedings{grounding_dino,
  title={Grounding dino: Marrying dino with grounded pre-training for open-set object detection},
  author={Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Jiang, Qing and Li, Chunyuan and Yang, Jianwei and Su, Hang and others},
  booktitle={European Conference on Computer Vision},
  pages={38--55},
  year={2024},
  organization={Springer}
}

@inproceedings{moritz2018ray,
  title={Ray: A distributed framework for emerging $\{$AI$\}$ applications},
  author={Moritz, Philipp and Nishihara, Robert and Wang, Stephanie and Tumanov, Alexey and Liaw, Richard and Liang, Eric and Elibol, Melih and Yang, Zongheng and Paul, William and Jordan, Michael I and others},
  booktitle={13th USENIX symposium on operating systems design and implementation (OSDI 18)},
  pages={561--577},
  year={2018}
}

@misc{nvidia_osmo,
  author       = {NVIDIA},
  title        = {OSMO Platform},
  year         = {2025},
  url          = {https://developer.nvidia.com/osmo},
  note         = {Accessed: 2025-03-12}
}


% for related work section

% teleoperation
@inproceedings{zhang2017deep,
  title={Deep Imitation Learning for Complex Manipulation Tasks from Virtual Reality Teleoperation},
  author={Zhang, Tianhao and McCarthy, Zoe and Jow, Owen and Lee, Dennis and Goldberg, Ken and Abbeel, Pieter},
  year={2018},
  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},
}

@inproceedings{mandlekar2018roboturk,
  title={{RoboTurk: A Crowdsourcing Platform for Robotic Skill Learning through Imitation}},
  author={Mandlekar, Ajay and Zhu, Yuke and Garg, Animesh and Booher, Jonathan and Spero, Max and Tung, Albert and Gao, Julian and Emmons, John and Gupta, Anchit and Orbay, Emre and Savarese, Silvio and Fei-Fei, Li},
  booktitle={Conference on Robot Learning},
  year={2018}
}

@inproceedings{mandlekar2019scaling,
  title={Scaling robot supervision to hundreds of hours with roboturk: Robotic manipulation dataset through human reasoning and dexterity},
  author={Mandlekar, Ajay and Booher, Jonathan and Spero, Max and Tung, Albert and Gupta, Anchit and Zhu, Yuke and Garg, Animesh and Savarese, Silvio and Fei-Fei, Li},
  booktitle={2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={1048--1055},
  year={2019},
  organization={IEEE}
}

@article{mandlekar2020human,
  title={Human-in-the-loop imitation learning using remote teleoperation},
  author={Mandlekar, Ajay and Xu, Danfei and Mart{\'\i}n-Mart{\'\i}n, Roberto and Zhu, Yuke and Fei-Fei, Li and Savarese, Silvio},
  journal={arXiv preprint arXiv:2012.06733},
  year={2020}
}

@misc{wu2023gello,
    title={GELLO: A General, Low-Cost, and Intuitive Teleoperation Framework for Robot Manipulators},
    author={Philipp Wu and Yide Shentu and Zhongke Yi and Xingyu Lin and Pieter Abbeel},
    year={2023},
}

@article{aldaco2024aloha,
  title={ALOHA 2: An Enhanced Low-Cost Hardware for Bimanual Teleoperation},
  author={Aldaco, Jorge and Armstrong, Travis and Baruch, Robert and Bingham, Jeff and Chan, Sanky and Draper, Kenneth and Dwibedi, Debidatta and Finn, Chelsea and Florence, Pete and Goodrich, Spencer and others},
  journal={arXiv preprint arXiv:2405.02292},
  year={2024}
}

@inproceedings{fu2024mobile,
  title={Mobile aloha: Learning bimanual mobile manipulation with low-cost whole-body teleoperation},
  author={Fu, Zipeng and Zhao, Tony Z and Finn, Chelsea},
  booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)},
  year={2024},
}

@inproceedings{iyer2024open,
  title={OPEN TEACH: A Versatile Teleoperation System for Robotic Manipulation},
  author={Iyer, Aadhithya and Peng, Zhuoran and Dai, Yinlong and Guzey, Irmak and Haldar, Siddhant and Chintala, Soumith and Pinto, Lerrel},
  booktitle={CoRL 2024 Workshop on Mastering Robot Manipulation in a World of Abundant Data}
}

@inproceedings{dass2024telemoma,
  title={TeleMoMa: A Modular and Versatile Teleoperation System for Mobile Manipulation},
  author={Dass, Shivin and Ai, Wensi and Jiang, Yuqian and Singh, Samik and Hu, Jiaheng and Zhang, Ruohan and Stone, Peter and Abbatematteo, Ben and Mart{\'\i}n-Mart{\'\i}n, Roberto},
  booktitle={2nd Workshop on Mobile Manipulation and Embodied Intelligence at ICRA 2024},
  year={2024}
}

% large-scale datasets
@INPROCEEDINGS{ebert2021bridge, 
    AUTHOR    = {Frederik Ebert AND Yanlai Yang AND Karl Schmeckpeper AND Bernadette Bucher AND Georgios Georgakis AND Kostas Daniilidis AND Chelsea Finn AND Sergey Levine}, 
    TITLE     = {{Bridge Data: Boosting Generalization of Robotic Skills with Cross-Domain Datasets}}, 
    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, 
    YEAR      = {2022}, 
    ADDRESS   = {New York City, NY, USA}, 
    MONTH     = {June}, 
    DOI       = {10.15607/RSS.2022.XVIII.063} 
} 

@article{brohan2022rt,
  title={Rt-1: Robotics transformer for real-world control at scale},
  author={Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Dabis, Joseph and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and Hsu, Jasmine and others},
  journal={arXiv preprint arXiv:2212.06817},
  year={2022}
}

@inproceedings{ahn2022can,
  title={Do as i can, not as i say: Grounding language in robotic affordances},
  author={Brohan, Anthony and Chebotar, Yevgen and Finn, Chelsea and Hausman, Karol and Herzog, Alexander and Ho, Daniel and Ibarz, Julian and Irpan, Alex and Jang, Eric and Julian, Ryan and others},
  booktitle={Conference on Robot Learning},
  pages={287--318},
  year={2023},
  organization={PMLR}
}

@article{lynch2022interactive,
  title={Interactive language: Talking to robots in real time},
  author={Lynch, Corey and Wahid, Ayzaan and Tompson, Jonathan and Ding, Tianli and Betker, James and Baruch, Robert and Armstrong, Travis and Florence, Pete},
  journal={IEEE Robotics and Automation Letters},
  year={2023},
  publisher={IEEE}
}

@inproceedings{o2024open,
  title={Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0},
  author={O’Neill, Abby and Rehman, Abdul and Maddukuri, Abhiram and Gupta, Abhishek and Padalkar, Abhishek and Lee, Abraham and Pooley, Acorn and Gupta, Agrim and Mandlekar, Ajay and Jain, Ajinkya and others},
  booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)},
  year={2024},
}

@article{black2024pi_0,
  title={$\pi_0$: A Vision-Language-Action Flow Model for General Robot Control},
  author={Black, Kevin and Brown, Noah and Driess, Danny and Esmail, Adnan and Equi, Michael and Finn, Chelsea and Fusai, Niccolo and Groom, Lachy and Hausman, Karol and Ichter, Brian and others},
  journal={arXiv preprint arXiv:2410.24164},
  year={2024}
}

% simulation data generation

@article{james2020rlbench,
  title={Rlbench: The robot learning benchmark \& learning environment},
  author={James, Stephen and Ma, Zicong and Arrojo, David Rovick and Davison, Andrew J},
  journal={IEEE Robotics and Automation Letters},
  volume={5},
  number={2},
  pages={3019--3026},
  year={2020},
  publisher={IEEE}
}

@inproceedings{dalal2023imitating,
  title={Imitating Task and Motion Planning with Visuomotor Transformers},
  author={Dalal, Murtaza and Mandlekar, Ajay and Garrett, Caelan Reed and Handa, Ankur and Salakhutdinov, Ruslan and Fox, Dieter},
  booktitle={Conference on Robot Learning},
  pages={2565--2593},
  year={2023},
  organization={PMLR}
}

@inproceedings{gu2023maniskill2,
  title={ManiSkill2: A Unified Benchmark for Generalizable Manipulation Skills},
  author={Gu, Jiayuan and Xiang, Fanbo and Li, Xuanlin and Ling, Zhan and Liu, Xiqiang and Mu, Tongzhou and Tang, Yihe and Tao, Stone and Wei, Xinyue and Yao, Yunchao and others},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}

@inproceedings{ha2023scaling,
  title={Scaling up and distilling down: Language-guided robot skill acquisition},
  author={Ha, Huy and Florence, Pete and Song, Shuran},
  booktitle={Conference on Robot Learning},
  pages={3766--3777},
  year={2023},
  organization={PMLR}
}

@article{garrett2024skillmimicgen,
  title={SkillMimicGen: Automated Demonstration Generation for Efficient Skill Learning and Deployment},
  author={Garrett, Caelan and Mandlekar, Ajay and Wen, Bowen and Fox, Dieter},
  journal={arXiv preprint arXiv:2410.18907},
  year={2024}
}

@article{yang2025physics,
  title={Physics-Driven Data Generation for Contact-Rich Manipulation via Trajectory Optimization},
  author={Yang, Lujie and Suh, HJ and Zhao, Tong and Graesdal, Bernhard Paus and Kelestemur, Tarik and Wang, Jiuguang and Pang, Tao and Tedrake, Russ},
  journal={arXiv preprint arXiv:2502.20382},
  year={2025}
}

% generative models for data augmentation
@article{mandi2022cacti,
  title={CACTI: A Framework for Scalable Multi-Task Multi-Scene Visual Imitation Learning},
  author={Mandi, Zhao and Bharadhwaj, Homanga and Moens, Vincent and Song, Shuran and Rajeswaran, Aravind and Kumar, Vikash},
  journal={arXiv preprint arXiv:2212.05711},
  year={2022}
}

@article{yu2023scaling,
  title={Scaling robot learning with semantically imagined experience},
  author={Yu, Tianhe and Xiao, Ted and Stone, Austin and Tompson, Jonathan and Brohan, Anthony and Wang, Su and Singh, Jaspiar and Tan, Clayton and Peralta, Jodilyn and Ichter, Brian and others},
  journal={arXiv preprint arXiv:2302.11550},
  year={2023}
}

@article{chen2023genaug,
  title={GenAug: Retargeting behaviors to unseen situations via Generative Augmentation},
  author={Chen, Zoey and Kiami, Sho and Gupta, Abhishek and Kumar, Vikash},
  journal={arXiv preprint arXiv:2302.06671},
  year={2023}
}

@article{agarwal2025cosmos,
  title={Cosmos world foundation model platform for physical ai},
  author={Agarwal, Niket and Ali, Arslan and Bala, Maciej and Balaji, Yogesh and Barker, Erik and Cai, Tiffany and Chattopadhyay, Prithvijit and Chen, Yongxin and Cui, Yin and Ding, Yifan and others},
  journal={arXiv preprint arXiv:2501.03575},
  year={2025}
}

% human video

@inproceedings{grauman2024ego,
  title={Ego-exo4d: Understanding skilled human activity from first-and third-person perspectives},
  author={Grauman, Kristen and Westbury, Andrew and Torresani, Lorenzo and Kitani, Kris and Malik, Jitendra and Afouras, Triantafyllos and Ashutosh, Kumar and Baiyya, Vijay and Bansal, Siddhant and Boote, Bikram and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={19383--19400},
  year={2024}
}

@inproceedings{grauman2022ego4d,
  title={Ego4d: Around the world in 3,000 hours of egocentric video},
  author={Grauman, Kristen and Westbury, Andrew and Byrne, Eugene and Chavis, Zachary and Furnari, Antonino and Girdhar, Rohit and Hamburger, Jackson and Jiang, Hao and Liu, Miao and Liu, Xingyu and others},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={18995--19012},
  year={2022}
}

@inproceedings{goyal2017something,
  title={The" something something" video database for learning and evaluating visual common sense},
  author={Goyal, Raghav and Ebrahimi Kahou, Samira and Michalski, Vincent and Materzynska, Joanna and Westphal, Susanne and Kim, Heuna and Haenel, Valentin and Fruend, Ingo and Yianilos, Peter and Mueller-Freitag, Moritz and others},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={5842--5850},
  year={2017}
}

@inproceedings{damen2018scaling,
  title={Scaling egocentric vision: The epic-kitchens dataset},
  author={Damen, Dima and Doughty, Hazel and Farinella, Giovanni Maria and Fidler, Sanja and Furnari, Antonino and Kazakos, Evangelos and Moltisanti, Davide and Munro, Jonathan and Perrett, Toby and Price, Will and others},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={720--736},
  year={2018}
}

@inproceedings{miech2019howto100m,
  title={Howto100m: Learning a text-video embedding by watching hundred million narrated video clips},
  author={Miech, Antoine and Zhukov, Dimitri and Alayrac, Jean-Baptiste and Tapaswi, Makarand and Laptev, Ivan and Sivic, Josef},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={2630--2640},
  year={2019}
}

@article{sener2022assembly101,
    title = {Assembly101: A Large-Scale Multi-View Video Dataset for Understanding Procedural Activities},
    author = {F. Sener and D. Chatterjee and D. Shelepov and K. He and D. Singhania and R. Wang and A. Yao},
    journal = {CVPR 2022},
    year = {2022}
}

@inproceedings{fang2023rh20t,
    title = {RH20T: A Robotic Dataset for Learning Diverse Skills in One-Shot},
    author = {Fang, Hao-Shu and Fang, Hongjie and Tang, Zhenyu and Liu, Jirong and Wang, Junbo and Zhu, Haoyi and Lu, Cewu},
    booktitle = {RSS 2023 Workshop on Learning for Task and Motion Planning},
    year = {2023}
}

@InProceedings{wang2023holoassist,
    author    = {Wang, Xin and Kwon, Taein and Rad, Mahdi and Pan, Bowen and Chakraborty, Ishani and Andrist, Sean and Bohus, Dan and Feniello, Ashley and Tekin, Bugra and Frujeri, Felipe Vieira and Joshi, Neel and Pollefeys, Marc},
    title     = {HoloAssist: an Egocentric Human Interaction Dataset for Interactive AI Assistants in the Real World},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    year      = {2023},
}

@InProceedings{liu2022hoi4d,
    author    = {Liu, Yunze and Liu, Yun and Jiang, Che and Lyu, Kangbo and Wan, Weikang and Shen, Hao and Liang, Boqiang and Fu, Zhoujie and Wang, He and Yi, Li},
    title     = {HOI4D: A 4D Egocentric Dataset for Category-Level Human-Object Interaction},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {21013-21022}
}

@article{nair2022r3m,
  title={R3m: A universal visual representation for robot manipulation},
  author={Nair, Suraj and Rajeswaran, Aravind and Kumar, Vikash and Finn, Chelsea and Gupta, Abhinav},
  journal={arXiv preprint arXiv:2203.12601},
  year={2022}
}

@article{wu2023unleashing,
  title={Unleashing large-scale video generative pre-training for visual robot manipulation},
  author={Wu, Hongtao and Jing, Ya and Cheang, Chilam and Chen, Guangzeng and Xu, Jiafeng and Li, Xinghang and Liu, Minghuan and Li, Hang and Kong, Tao},
  journal={arXiv preprint arXiv:2312.13139},
  year={2023}
}

@article{karamcheti2023language,
  title={Language-driven representation learning for robotics},
  author={Karamcheti, Siddharth and Nair, Suraj and Chen, Annie S and Kollar, Thomas and Finn, Chelsea and Sadigh, Dorsa and Liang, Percy},
  journal={arXiv preprint arXiv:2302.12766},
  year={2023}
}

@article{bharadhwaj2024gen2act,
  title={Gen2act: Human video generation in novel scenarios enables generalizable robot manipulation},
  author={Bharadhwaj, Homanga and Dwibedi, Debidatta and Gupta, Abhinav and Tulsiani, Shubham and Doersch, Carl and Xiao, Ted and Shah, Dhruv and Xia, Fei and Sadigh, Dorsa and Kirmani, Sean},
  journal={arXiv preprint arXiv:2409.16283},
  year={2024}
}

@article{bharadhwaj2024track2act,
  title={Track2act: Predicting point tracks from internet videos enables diverse zero-shot robot manipulation},
  author={Bharadhwaj, Homanga and Mottaghi, Roozbeh and Gupta, Abhinav and Tulsiani, Shubham},
  journal={arXiv e-prints},
  pages={arXiv--2405},
  year={2024}
}

@article{ren2025motion,
  title={Motion Tracks: A Unified Representation for Human-Robot Transfer in Few-Shot Imitation Learning},
  author={Ren, Juntao and Sundaresan, Priya and Sadigh, Dorsa and Choudhury, Sanjiban and Bohg, Jeannette},
  journal={arXiv preprint arXiv:2501.06994},
  year={2025}
}


% Foundation model (as black box) + robotics

@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@article{driess2023palm,
  title={PaLM-E: An Embodied Multimodal Language Model},
  author={Driess, Danny and Xia, Fei and Sajjadi, Mehdi SM and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and others},
  journal={arXiv preprint arXiv:2303.03378},
  year={2023}
}

@inproceedings{zeroshot-llms-2022,
  title={Language models as zero-shot planners: Extracting actionable knowledge for embodied agents},
  author={Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
  booktitle={International conference on machine learning},
  pages={9118--9147},
  year={2022},
  organization={PMLR}
}

@inproceedings{saycan-2022,
  title={Do as i can, not as i say: Grounding language in robotic affordances},
  author={Brohan, Anthony and Chebotar, Yevgen and Finn, Chelsea and Hausman, Karol and Herzog, Alexander and Ho, Daniel and Ibarz, Julian and Irpan, Alex and Jang, Eric and Julian, Ryan and others},
  booktitle={Conference on robot learning},
  pages={287--318},
  year={2023},
  organization={PMLR}
}

@inproceedings{innermono-2022,
  title={Inner Monologue: Embodied Reasoning through Planning with Language Models},
  author={Huang, Wenlong and Xia, Fei and Xiao, Ted and Chan, Harris and Liang, Jacky and Florence, Pete and Zeng, Andy and Tompson, Jonathan and Mordatch, Igor and Chebotar, Yevgen and others},
  booktitle={6th Annual Conference on Robot Learning}
}

@inproceedings{code-as-policies-2022,
  title={Code as policies: Language model programs for embodied control},
  author={Liang, Jacky and Huang, Wenlong and Xia, Fei and Xu, Peng and Hausman, Karol and Ichter, Brian and Florence, Pete and Zeng, Andy},
  booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={9493--9500},
  year={2023},
  organization={IEEE}
}

@inproceedings{progprompt-2022,
  title={Progprompt: Generating situated robot task plans using large language models},
  author={Singh, Ishika and Blukis, Valts and Mousavian, Arsalan and Goyal, Ankit and Xu, Danfei and Tremblay, Jonathan and Fox, Dieter and Thomason, Jesse and Garg, Animesh},
  booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)},
  year={2023},
}

@article{groundeddecoding-2024,
  title={Grounded decoding: Guiding text generation with grounded models for embodied agents},
  author={Huang, Wenlong and Xia, Fei and Shah, Dhruv and Driess, Danny and Zeng, Andy and Lu, Yao and Florence, Pete and Mordatch, Igor and Levine, Sergey and Hausman, Karol and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={59636--59661},
  year={2023}
}


@article{text2motion-23,
  title={Text2Motion: from natural language instructions to feasible plans},
  author={Lin, Kevin and Agia, Christopher and Migimatsu, Toki and Pavone, Marco and Bohg, Jeannette},
  journal={Autonomous Robots},
  year={2023},
}

@inproceedings{huang2023voxposer,
  title={VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models},
  author={Huang, Wenlong and Wang, Chen and Zhang, Ruohan and Li, Yunzhu and Wu, Jiajun and Fei-Fei, Li},
  booktitle={Conference on Robot Learning},
  pages={540--562},
  year={2023},
  organization={PMLR}
}
% VLA

@inproceedings{rt1-2022,
    title={RT-1: Robotics Transformer for Real-World Control at Scale},
    author={Anthony	Brohan and  Noah Brown and  Justice Carbajal and  Yevgen Chebotar and  Joseph Dabis and  Chelsea Finn and  Keerthana Gopalakrishnan and  Karol Hausman and  Alex Herzog and  Jasmine Hsu and  Julian Ibarz and  Brian Ichter and  Alex Irpan and  Tomas Jackson and  Sally Jesmonth and  Nikhil Joshi and  Ryan Julian and  Dmitry Kalashnikov and  Yuheng Kuang and  Isabel Leal and  Kuang-Huei Lee and  Sergey Levine and  Yao Lu and  Utsav Malla and  Deeksha Manjunath and  Igor Mordatch and  Ofir Nachum and  Carolina Parada and  Jodilyn Peralta and  Emily Perez and  Karl Pertsch and  Jornell Quiambao and  Kanishka Rao and  Michael Ryoo and  Grecia Salazar and  Pannag Sanketi and  Kevin Sayed and  Jaspiar Singh and  Sumedh Sontakke and  Austin Stone and  Clayton Tan and  Huong Tran and  Vincent Vanhoucke and Steve Vega and  Quan Vuong and  Fei Xia and  Ted Xiao and  Peng Xu and  Sichun Xu and  Tianhe Yu and  Brianna Zitkovich},
    booktitle={arXiv preprint arXiv:2212.06817},
    year={2022}
}

% LLM/VLM finetuned for robotics


@inproceedings{rt22023arxiv,
    title={RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control},
    author={Anthony Brohan and Noah Brown and Justice Carbajal and Yevgen Chebotar and Xi Chen and Krzysztof Choromanski and Tianli Ding and Danny Driess and Avinava Dubey and Chelsea Finn and Pete Florence and Chuyuan Fu and Montse Gonzalez Arenas and Keerthana Gopalakrishnan and Kehang Han and Karol Hausman and Alex Herzog and Jasmine Hsu and Brian Ichter and Alex Irpan and Nikhil Joshi and Ryan Julian and Dmitry Kalashnikov and Yuheng Kuang and Isabel Leal  and Lisa Lee and Tsang-Wei Edward Lee and Sergey Levine and Yao Lu and Henryk Michalewski and Igor Mordatch and Karl Pertsch and Kanishka Rao and Krista Reymann and Michael Ryoo and Grecia Salazar and Pannag Sanketi and Pierre Sermanet and Jaspiar Singh and Anikait Singh and Radu Soricut and Huong Tran and Vincent Vanhoucke and Quan Vuong and Ayzaan Wahid and Stefan Welker and Paul Wohlhart and  Jialin Wu and Fei Xia and Ted Xiao and Peng Xu and Sichun Xu and Tianhe Yu and Brianna Zitkovich},
    booktitle={arXiv preprint arXiv:2307.15818},
    year={2023}
}

@misc{shentu2024lcb,
      title={From LLMs to Actions: Latent Codes as Bridges in Hierarchical Robot Control},
      author={Yide Shentu, Philipp Wu, Aravind Rajeswaran and Pieter Abbeel},
      year={2024},
}

@article{kim24openvla,
    title={OpenVLA: An Open-Source Vision-Language-Action Model},
    author={{Moo Jin} Kim and Karl Pertsch and Siddharth Karamcheti and Ted Xiao and Ashwin Balakrishna and Suraj Nair and Rafael Rafailov and Ethan Foster and Grace Lam and Pannag Sanketi and Quan Vuong and Thomas Kollar and Benjamin Burchfiel and Russ Tedrake and Dorsa Sadigh and Sergey Levine and Percy Liang and Chelsea Finn},
    journal = {arXiv preprint arXiv:2406.09246},
    year={2024},
} 

@inproceedings{
zheng2025tracevla,
title={Trace{VLA}: Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policies},
author={Ruijie Zheng and Yongyuan Liang and Shuaiyi Huang and Jianfeng Gao and Hal Daum{\'e} III and Andrey Kolobov and Furong Huang and Jianwei Yang},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
}

@misc{yang2025magma,
      title={Magma: A Foundation Model for Multimodal {AI} Agents}, 
      author={Jianwei Yang and Reuben Tan and Qianhui Wu and Ruijie Zheng and Baolin Peng and Yongyuan Liang and Yu Gu and Mu Cai and Seonghyeon Ye and Joel Jang and Yuquan Deng and Lars Liden and Jianfeng Gao},
      year={2025},
      eprint={2502.13130},
      archivePrefix={arXiv},
}




@article{cheang2024gr2vla,
title={GR-2: A Generative Video-Language-Action Model with Web-Scale Knowledge for Robot Manipulation},
author={Chi-Lam Cheang and Guangzeng Chen and Ya Jing and Tao Kong and Hang Li and Yifeng Li and Yuxiao Liu and Hongtao Wu and
Jiafeng Xu and Yichu Yang and Hanbo Zhang and Minzhao Zhu},
journal={arXiv preprint arXiv:2410.06158},
year={2024}
}

@article{wen2024tinyvla,
      title={TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation},
      author={Wen, Junjie and Zhu, Yichen and Li, Jinming and Zhu, Minjie and Wu, Kun and Xu, Zhiyuan and Cheng, Ran and Shen, Chaomin and Peng, Yaxin and Feng, Feifei and others},
      journal={arXiv preprint arXiv:2409.12514},
      year={2024}
    }

@article{ashish2017attention,
  title={Attention is all you need},
  author={Ashish, Vaswani},
  journal={Advances in neural information processing systems},
  volume={30},
  pages={I},
  year={2017}
}

@inproceedings{huang2023embodied,
  title={An Embodied Generalist Agent in 3D World},
  author={Huang, Jiangyong and Yong, Silong and Ma, Xiaojian and Linghu, Xiongkun and Li, Puhao and Wang, Yan and Li, Qing and Zhu, Song-Chun and Jia, Baoxiong and Huang, Siyuan},
  booktitle={Proceedings of the International Conference on Machine Learning (ICML)},
  year={2024}
}

@article{zhen20243dvla,
  author = {Zhen, Haoyu and Qiu, Xiaowen and Chen, Peihao and Yang, Jincheng and Yan, Xin and Du, Yilun and Hong, Yining and Gan, Chuang},
  title = {3D-VLA: 3D Vision-Language-Action Generative World Model},
  journal = {arXiv preprint arXiv:2403.09631},
  year = {2024},
}

@article{li2023vision,
  title={Vision-language foundation models as effective robot imitators},
  author={Li, Xinghang and Liu, Minghuan and Zhang, Hanbo and Yu, Cunjun and Xu, Jie and Wu, Hongtao and Cheang, Chilam and Jing, Ya and Zhang, Weinan and Liu, Huaping and others},
  journal={arXiv preprint arXiv:2311.01378},
  year={2023}
}
% Generalist robot policies (a form of VLA that's not pre-trained on VLMs)

@inproceedings{octo_2023,
    title={Octo: An Open-Source Generalist Robot Policy},
    author = {{Octo Model Team} and Dibya Ghosh and Homer Walke and Karl Pertsch and Kevin Black and Oier Mees and Sudeep Dasari and Joey Hejna and Charles Xu and Jianlan Luo and Tobias Kreiman and {You Liang} Tan and Lawrence Yunliang Chen and Pannag Sanketi and Quan Vuong and Ted Xiao and Dorsa Sadigh and Chelsea Finn and Sergey Levine},
    booktitle = {Proceedings of Robotics: Science and Systems},
    address  = {Delft, Netherlands},
    year = {2024},
}

% Flow
@article{liu2022flow,
  title={Flow straight and fast: Learning to generate and transfer data with rectified flow},
  author={Liu, Xingchao and Gong, Chengyue and Liu, Qiang},
  journal={arXiv preprint arXiv:2209.03003},
  year={2022}
}

@article{hu2024adaflow,
  title={AdaFlow: Imitation Learning with Variance-Adaptive Flow-Based Policies},
  author={Hu, Xixi and Liu, Bo and Liu, Xingchao and Liu, Qiang},
  journal={arXiv preprint arXiv:2402.04292},
  year={2024}
}

@inproceedings{
minderer2023owl,
title={Scaling Open-Vocabulary Object Detection},
author={Matthias Minderer and Alexey A. Gritsenko and Neil Houlsby},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
}

% instrumented human demos

@misc{kareer2024egomimicscalingimitationlearning,
      title={{EgoMimic}: Scaling Imitation Learning via Egocentric Video}, 
      author={Simar Kareer and Dhruv Patel and Ryan Punamiya and Pranay Mathur and Shuo Cheng and Chen Wang and Judy Hoffman and Danfei Xu},
      year={2024},
      eprint={2410.24221},
      archivePrefix={arXiv},
} 

@inproceedings{chi2024universal,
	title={Universal Manipulation Interface: In-The-Wild Robot Teaching Without In-The-Wild Robots},
	author={Chi, Cheng and Xu, Zhenjia and Pan, Chuer and Cousineau, Eric and Burchfiel, Benjamin and Feng, Siyuan and Tedrake, Russ and Song, Shuran},
	booktitle={Proceedings of Robotics: Science and Systems (RSS)},
	year={2024}
}

@article{seo2024legato,
        title={LEGATO: Cross-Embodiment Imitation Using a Grasping Tool},
        author={Seo, Mingyo and Park, H. Andy and Yuan, Shenli and Zhu, Yuke and
          and Sentis, Luis},
        journal={IEEE Robotics and Automation Letters (RA-L)},
        year={2025}
      }

@inproceedings{fang2024airexo,
  title =        {AirExo: Low-Cost Exoskeletons for Learning Whole-Arm Manipulation in the Wild},
  author =       {Fang, Hongjie and Fang, Hao-Shu and Wang, Yiming and Ren, Jieji and Chen, Jingjing and Zhang, Ruo and Wang, Weiming and Lu, Cewu},
  booktitle =    {2024 IEEE International Conference on Robotics and Automation (ICRA)},
  pages =        {15031--15038},
  year =         {2024},
  organization = {IEEE}
}

@inproceedings{NEURIPS2022_960a172b,
 author = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Cabi, Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob L and Borgeaud, Sebastian and Brock, Andy and Nematzadeh, Aida and Sharifzadeh, Sahand and Bi\'{n}kowski, Miko\l aj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Kar\'{e}n},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 title = {Flamingo: a Visual Language Model for Few-Shot Learning},
 year = {2022}
}

@software{Zakka_Mink_Python_inverse_2024,
  author = {Zakka, Kevin},
  license = {Apache-2.0},
  month = jul,
  title = {{Mink: Python inverse kinematics based on MuJoCo}},
  url = {https://github.com/kevinzakka/mink},
  version = {0.0.4},
  year = {2024}
}

@inproceedings{jiang2023vima,
  title={VIMA: robot manipulation with multimodal prompts},
  author={Jiang, Yunfan and Gupta, Agrim and Zhang, Zichen and Wang, Guanzhi and Dou, Yongqiang and Chen, Yanjun and Fei-Fei, Li and Anandkumar, Anima and Zhu, Yuke and Fan, Linxi},
  booktitle={Proceedings of the 40th International Conference on Machine Learning},
  pages={14975--15022},
  year={2023}
}

@article{wan2.1,
    title   = {Wan: Open and Advanced Large-Scale Video Generative Models},
    author  = {{Wan Team}},
    journal = {},
    year    = {2025}
}

@article{baker2022video,
  title={Video pretraining (vpt): Learning to act by watching unlabeled online videos},
  author={Baker, Bowen and Akkaya, Ilge and Zhokov, Peter and Huizinga, Joost and Tang, Jie and Ecoffet, Adrien and Houghton, Brandon and Sampedro, Raul and Clune, Jeff},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24639--24654},
  year={2022}
}

@book{kahneman2011thinking,
  title={Thinking, fast and slow},
  author={Kahneman, Daniel},
  year={2011},
}

@inproceedings{
hu2022lora,
title={Lo{RA}: Low-Rank Adaptation of Large Language Models},
author={Edward J Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2022},
}

@misc{qwen2.5-VL,
    title = {Qwen2.5-VL},
    url = {https://qwenlm.github.io/blog/qwen2.5-vl/},
    author = {{Qwen Team}},
    month = {January},
    year = {2025}
}

@article{hurst2024gpt,
  title={Gpt-4o system card},
  author={Hurst, Aaron and Lerer, Adam and Goucher, Adam P and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, AJ and Welihinda, Akila and Hayes, Alan and Radford, Alec and others},
  journal={arXiv preprint arXiv:2410.21276},
  year={2024}
}

@inproceedings{walke2023bridgedata,
    title={BridgeData V2: A Dataset for Robot Learning at Scale},
    author={Walke, Homer and Black, Kevin and Lee, Abraham and Kim, Moo Jin and Du, Max and Zheng, Chongyi and Zhao, Tony and Hansen-Estruch, Philippe and Vuong, Quan and He, Andre and Myers, Vivek and Fang, Kuan and Finn, Chelsea and Levine, Sergey},
    booktitle={Conference on Robot Learning (CoRL)},
    year={2023}
}

@misc{brohan2023rt1roboticstransformerrealworld,
      title={RT-1: Robotics Transformer for Real-World Control at Scale}, 
      author={Anthony Brohan and Noah Brown and Justice Carbajal and Yevgen Chebotar and Joseph Dabis and Chelsea Finn and Keerthana Gopalakrishnan and Karol Hausman and Alex Herzog and Jasmine Hsu and Julian Ibarz and Brian Ichter and Alex Irpan and Tomas Jackson and Sally Jesmonth and Nikhil J Joshi and Ryan Julian and Dmitry Kalashnikov and Yuheng Kuang and Isabel Leal and Kuang-Huei Lee and Sergey Levine and Yao Lu and Utsav Malla and Deeksha Manjunath and Igor Mordatch and Ofir Nachum and Carolina Parada and Jodilyn Peralta and Emily Perez and Karl Pertsch and Jornell Quiambao and Kanishka Rao and Michael Ryoo and Grecia Salazar and Pannag Sanketi and Kevin Sayed and Jaspiar Singh and Sumedh Sontakke and Austin Stone and Clayton Tan and Huong Tran and Vincent Vanhoucke and Steve Vega and Quan Vuong and Fei Xia and Ted Xiao and Peng Xu and Sichun Xu and Tianhe Yu and Brianna Zitkovich},
      year={2023},
      eprint={2212.06817},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2212.06817} 
}

@article{khazatsky2024droid,
    title   = {DROID: A Large-Scale In-The-Wild Robot Manipulation Dataset},
    author  = {Alexander Khazatsky and Karl Pertsch and Suraj Nair and Ashwin Balakrishna and Sudeep Dasari and Siddharth Karamcheti and Soroush Nasiriany and Mohan Kumar Srirama and Lawrence Yunliang Chen and Kirsty Ellis and Peter David Fagan and Joey Hejna and Masha Itkina and Marion Lepert and Yecheng Jason Ma and Patrick Tree Miller and Jimmy Wu and Suneel Belkhale and Shivin Dass and Huy Ha and Arhan Jain and Abraham Lee and Youngwoon Lee and Marius Memmel and Sungjae Park and Ilija Radosavovic and Kaiyuan Wang and Albert Zhan and Kevin Black and Cheng Chi and Kyle Beltran Hatch and Shan Lin and Jingpei Lu and Jean Mercat and Abdul Rehman and Pannag R Sanketi and Archit Sharma and Cody Simpson and Quan Vuong and Homer Rich Walke and Blake Wulfe and Ted Xiao and Jonathan Heewon Yang and Arefeh Yavary and Tony Z. Zhao and Christopher Agia and Rohan Baijal and Mateo Guaman Castro and Daphne Chen and Qiuyu Chen and Trinity Chung and Jaimyn Drake and Ethan Paul Foster and Jensen Gao and David Antonio Herrera and Minho Heo and Kyle Hsu and Jiaheng Hu and Donovon Jackson and Charlotte Le and Yunshuang Li and Kevin Lin and Roy Lin and Zehan Ma and Abhiram Maddukuri and Suvir Mirchandani and Daniel Morton and Tony Nguyen and Abigail O'Neill and Rosario Scalise and Derick Seale and Victor Son and Stephen Tian and Emi Tran and Andrew E. Wang and Yilin Wu and Annie Xie and Jingyun Yang and Patrick Yin and Yunchu Zhang and Osbert Bastani and Glen Berseth and Jeannette Bohg and Ken Goldberg and Abhinav Gupta and Abhishek Gupta and Dinesh Jayaraman and Joseph J Lim and Jitendra Malik and Roberto Martín-Martín and Subramanian Ramamoorthy and Dorsa Sadigh and Shuran Song and Jiajun Wu and Michael C. Yip and Yuke Zhu and Thomas Kollar and Sergey Levine and Chelsea Finn},
    year    = {2024},
}

@misc{lynch2022interactivelanguagetalkingrobots,
      title={Interactive Language: Talking to Robots in Real Time}, 
      author={Corey Lynch and Ayzaan Wahid and Jonathan Tompson and Tianli Ding and James Betker and Robert Baruch and Travis Armstrong and Pete Florence},
      year={2022},
      eprint={2210.06407},
      archivePrefix={arXiv},
}

@inproceedings{shah2023mutex,
	title        = {MUTEX: Learning Unified Policies from Multimodal Task Specifications},
	author       = {Rutav Shah and Roberto Mart{\'\i}n-Mart{\'\i}n and Yuke Zhu},
	year         = 2023,
	booktitle    = {7th Annual Conference on Robot Learning},
	å   = {https://openreview.net/forum?id=PwqiqaaEzJ}
}

@inproceedings{thomas2023plex,
    title={PLEX: Making the Most of the Available Data for Robotic Manipulation Pretraining},
    author={Garrett Thomas and Ching-An Cheng and Ricky Loynd and Felipe Vieira Frujeri and Vibhav Vineet and Mihai Jalobeanu and Andrey Kolobov},
    booktitle={CoRL},
    year={2023}
}

@inproceedings{kumar2023robo,

  title     = {RoboHive -- A Unified Framework for Robot Learning},
  author    = {Vikash Kumar, Rutav Shah, Gaoyue Zhou, Vincent Moens, Vittorio Caggiano, Jay Vakil, Abhishek Gupta, Aravind Rajeswaran},

  booktitle = {NeurIPS: Conference on Neural Information Processing Systems},

  year      = {2023},

  url       = {https://sites.google.com/view/robohive},

  eprint    = {https://arxiv.org/abs/2310.06828},   

}

@INPROCEEDINGS{roboset,
  author={Bharadhwaj, Homanga and Vakil, Jay and Sharma, Mohit and Gupta, Abhinav and Tulsiani, Shubham and Kumar, Vikash},
  booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={RoboAgent: Generalization and Efficiency in Robot Manipulation via Semantic Augmentations and Action Chunking}, 
  year={2024},
}