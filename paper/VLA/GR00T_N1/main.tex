\pdfoutput=1
\documentclass[10pt,logo,copyright]{nvidiatechreport}
\linespread{1.15}

\input{packages}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{common.tex}

\newcommand{\FIGREF}[1]{Fig.~\ref{#1}} % Fig. or Figure
\definecolor{darkred}{rgb}{0.7, 0.0, 0.0}
\newcommand{\checkthis}[1]{\textcolor{darkred}{#1}}

\newcommand{\red}[1]{\textcolor{red}{#1}}

\usepackage{pifont}
\newcommand{\cmark}{\ding{51}} % Tick mark
\newcommand{\xmark}{\ding{55}} % Cross mark
\usepackage{wrapfig}

\usepackage[nameinlink]{cleveref}
\crefname{equation}{Eq.}{Eqs.}
\crefname{figure}{Fig.}{Figs.}
\crefname{section}{Sec.}{Sec.}
\crefname{appendix}{App.}{App.}
\crefname{table}{Tab.}{Tabs.}
\crefname{algorithm}{Algo}{Algo}
\crefname{thm}{Thm}{Thm}
\Crefname{thm}{Thm}{Thm}
\crefname{prop}{Prop}{Prop}
\usepackage{longtable}

\newcommand{\ours}[0]{\modelname{}\xspace}
\newcommand{\ruijie}[1]{\textcolor[rgb]{0.12,0.55,1.0}{{{#1}}}}
\newcommand{\joel}[1]{\textcolor[rgb]{0.82,0.21,0.12}{{{#1}}}}


\newcommand{\crefnames}[3]{%
  \@for\next:=#1\do{%
    \expandafter\crefname\expandafter{\next}{#2}{#3}%
  }%
}

\title{\modelname{}: An Open Foundation Model for Generalist Humanoid Robots}

%\title{\modelname{}: A Foundation Model for Humanoid Robotics}


\author{NVIDIA\footnote{A detailed list of contributors and acknowledgments can be found in~\cref{sec:contributors} of this paper.}}

\begin{abstract}
General-purpose robots need a versatile body and an intelligent mind. Recent advancements in humanoid robots have shown great promise as a hardware platform for building generalist autonomy in the human world. A robot foundation model, trained on massive and diverse data sources, is essential for enabling the robots to reason about novel situations, robustly handle real-world variability, and rapidly learn new tasks. To this end, we introduce \modelname{}, an open foundation model for humanoid robots. \modelname{} is a Vision-Language-Action (VLA) model with a dual-system architecture. The vision-language module (System 2) interprets the environment through vision and language instructions. The subsequent diffusion transformer module (System 1) generates fluid motor actions in real time. Both modules are tightly coupled and jointly trained end-to-end. We train \modelname{} with a heterogeneous mixture of real-robot trajectories, human videos, and synthetically generated datasets.
%
We show that our generalist robot model \modelname{} outperforms the state-of-the-art imitation learning baselines on standard simulation benchmarks across multiple robot embodiments. Furthermore, we deploy our model on the Fourier GR-1 humanoid robot for language-conditioned bimanual manipulation tasks, achieving strong performance with high data efficiency.
%
\end{abstract}

\begin{document}

\maketitle

% \input{sections/teaser}

\abscontent

% \checkthis{Timeline: Initial draft due 5pm PT Tuesday (2/25).}
% \checkthis{If your names are in \textbf{bold}, it means that your assigned sections are required for the initial draft. }

\input{sections/intro}
\input{sections/model}
\input{sections/datasets}
\input{sections/experiments}
\input{sections/related}
\input{sections/conclusion}

\clearpage
\appendix
\input{appendix/contributors}
\input{appendix/swe}

\clearpage
\setcitestyle{numbers}
\bibliographystyle{plainnat}
\bibliography{main}

\end{document}