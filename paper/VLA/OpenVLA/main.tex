\documentclass{article}

\usepackage[preprint]{corl_2024} %




\usepackage{titlesec}
\titlespacing\section{0pt}{\parskip}{0pt}
\titlespacing\subsection{0pt}{0.5\parskip}{0pt}
\setlength\parskip{0.3em plus 0.1em minus 0.1em}



\usepackage[utf8]{inputenc} %
\usepackage[T1]{fontenc}    %
\usepackage{url}            %
\usepackage{amsfonts}       %
\usepackage{nicefrac}       %
\usepackage{microtype}      %
\usepackage{tabularx}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=orange,
	filecolor=magenta,      
	urlcolor=orange,
	citecolor=orange,
}
\usepackage[utf8]{inputenc} %
\usepackage[T1]{fontenc}    %
\usepackage{dsfont}
\usepackage{url}            %
\usepackage{amsfonts}       %
\usepackage{nicefrac}       %
\usepackage{color}
\usepackage[dvipsnames,table]{xcolor}
\usepackage{mathtools}
\usepackage{amsmath,amssymb}
\usepackage{bm}
\usepackage{siunitx}
\usepackage{wrapfig}
\sisetup{output-exponent-marker=\ensuremath{\mathrm{e}}}
\sisetup{detect-weight=true, detect-family=true}
\renewcommand{\floatpagefraction}{.95}%
\usepackage{algorithm}
\usepackage[noend]{algorithmic}
\usepackage{lipsum}
\usepackage{makecell}
\usepackage{cite}
\usepackage{subcaption}
\usepackage[capitalise, nameinlink]{cleveref}

\usepackage{footnote}
\makesavenoteenv{tabular}
\makesavenoteenv{table}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}


\definecolor{lightlightgray}{rgb}{0.9, 0.9, 0.9}
\definecolor[named]{skviolet}{HTML}{800080}


\usepackage{pifont}%
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\newcommand{\pl}[1]{\textcolor{red}{[PL: #1]}}

\input{math_commands.tex}


\newcommand{\todo}[1]{ \textcolor{red}{\bf #1}}
\newcommand{\needcite}[1]{ \textcolor{magenta}{\textbf{[Cite?]} #1}}

\newcommand{\KP}[1]{ \textcolor{blue}{K: \bf #1}}
\newcommand{\sk}[1]{ \textcolor{skviolet}{\textbf{[Sidd]: #1}}}
\newcommand{\russt}[1]{ \textcolor{orange}{RT: \bf #1}} %
\newcommand{\bb}[1]{ \textcolor{ForestGreen}{BB: \bf #1}}
\newcommand{\ie}{i.e., }
\newcommand{\eg}{e.g., }
\newcommand{\Skip}[1]{}

\usepackage{verbatim}
\usepackage{mdframed}
\mdfdefinestyle{codebox}{%
    backgroundcolor=gray!25, %
    linewidth=0pt, %
}
\usepackage{newfloat}
\DeclareFloatingEnvironment[
    fileext=los,
    listname={List of Listings},
    name=Listing,
    placement=tbhp,
    within=section,
]{listing}

\newcommand{\name}{OpenVLA}
\newcommand{\modelsize}{7B}
\newcommand{\ngpus}{64~A100}
\newcommand{\ntraindays}{14}
\newcommand{\trainlearningrate}{2e-5}
\newcommand{\batchsize}{2048}
\newcommand{\niterations}{150k}
\newcommand{\nepisodes}{970k}
\newcommand{\nepochs}{27}
\newcommand{\license}{MIT}
\newcommand{\website}{\url{https://openvla.github.io}}


\usepackage[font=small]{caption} %
\captionsetup[table]{name=Table}



\title{\name{}: \\An Open-Source Vision-Language-Action Model
\vspace{-0.3cm}
}



\author{
  \hspace{-1cm}Moo Jin Kim$^{\ast, 1}$ \quad Karl Pertsch$^{\ast, 1, 2}$ \quad Siddharth Karamcheti$^{\ast, 1, 3}$\\
  \hspace{-1.5cm}\textbf{Ted Xiao}$^{4}$ \;\; \textbf{Ashwin Balakrishna}$^{3}$ \;\; \textbf{Suraj Nair}$^{3}$ \;\; \textbf{Rafael Rafailov}$^{1}$ \;\; \textbf{Ethan Foster}$^{1}$ \;\; \textbf{Grace Lam}\\
  \hspace{-1.5cm}\textbf{Pannag Sanketi}$^{4}$ \;\; \textbf{Quan Vuong}$^{5,\dagger}$ \;\; \textbf{Thomas Kollar}$^{3}$ \;\; \textbf{Benjamin Burchfiel}$^{3}$ \;\; \textbf{Russ Tedrake}$^{3,6}$ \;\; \textbf{Dorsa Sadigh}$^{1}$\\
  \hspace{-1.5cm}\textbf{Sergey Levine}$^{2}$ \;\; \textbf{Percy Liang}$^{1}$ \;\; \textbf{Chelsea Finn}$^{1}$\\[0.3cm]
  \hspace{-2cm}\large\website
  \vspace{-0.5cm}
}


\begin{document}
\makeatletter
\let\@oldmaketitle\@maketitle%
\renewcommand{\@maketitle}{\@oldmaketitle%
  \begin{center}
  \captionsetup{type=figure}
  \includegraphics[width=\textwidth]{figures/openvla_teaser.pdf}
    \captionof{figure}{We present \name{}, a 7B-parameter open-source vision-language-action model (VLA), trained 
    on 970k robot episodes from the Open X-Embodiment dataset~\citep{open_x_embodiment_rt_x_2023}. \name{} sets a new state of the art for generalist robot manipulation policies. It supports controlling multiple robots out of the box and can be quickly adapted to new robot domains via parameter-efficient fine-tuning. The \name{} checkpoints and PyTorch training pipeline are fully open-source and models can be downloaded and fine-tuned from HuggingFace.
    } 
    \label{fig:teaser}
    \end{center}
}
\makeatother

\maketitle


\input{sections/00_abstract}
\input{sections/01_introduction}
\input{sections/02_related_work}
\input{sections/03_approach}
\input{sections/04_experiments}
\input{sections/05_conclusion}


\acknowledgments{
We are grateful to the Toyota Research Institute for providing significant funding and compute resources required to carry out this research. We also thank the Stanford Center for Research on Foundation Models for providing additional compute resources and Google DeepMind for alpha access to the RT-2-X API for our evaluations. We acknowledge additional support from Volkswagen, Physical Intelligence, ONR grants N00014-22-1-2621 and N00014-22-1-2293, the National Science Foundation through IIS-2246811, and DARPA ANSR.
}


\bibliography{bibref_definitions_long,bibtex}

\clearpage
\appendix
\input{sections/06_appendix.tex}

\end{document}
