
\section{Introduction}
\label{sec:intro}

A key weakness of learned policies for robotic manipulation is their inability to generalize beyond their training data: while existing policies trained for individual skills or language instructions have the capacity to extrapolate behaviors to new initial conditions such as object positions or lighting \citep{rt12022arxiv, chi2023diffusionpolicy}, they lack robustness to scene distractors or novel objects~\citep{xie2023decomposing, octo_2023} and struggle to execute unseen task instructions~\citep{walke2023bridgedata, rt22023arxiv}. Yet beyond robotics, existing foundation models for vision and language such as CLIP~\citep{radford2021clip}, SigLIP~\citep{zhai2023siglip}, and Llama~2~\citep{touvron2023llama2} are capable of these types of generalization and more, stemming from the priors captured by their Internet-scale pretraining datasets. While reproducing this scale of pretraining for robotics is still an open challenge 
--- even the largest robot manipulation datasets \citep{open_x_embodiment_rt_x_2023, khazatsky2024droid} only have 100K to 1M examples -- this imbalance suggests an opportunity: using existing foundation models for vision and language \textit{as a core building block} for training robotic policies that can generalize to objects, scenes, and tasks beyond their training data.

Towards this goal, existing work has explored integrating pretrained language and vision-language models for robotic representation learning \citep{nair2022r3m, Karamcheti2023LanguageDrivenRL, shridhar2022cliport} and as a component in modular systems for task planning and execution \citep{stone2023open, driess2023palm}. More recently, they have been used for directly learning vision-language-action models \citep[VLAs;][]{rt22023arxiv, open_x_embodiment_rt_x_2023, covariant_ai_2024, wayve_ai_2024} for control. VLAs provide a direct instantiation of using pretrained vision-and-language foundation models for robotics, directly fine-tuning visually-conditioned language models (VLMs) such as PaLI \citep{chen2022pali, chen2023pali3} to generate robot control actions. By building off of strong foundation models trained on Internet-scale data, VLAs such as RT-2 \citep{rt22023arxiv} demonstrate impressive robustness results, as well as an ability to generalize to novel objects and tasks, setting a new standard for generalist robot policies. Yet, there are two key reasons preventing the widespread use of existing VLAs: 1) current models \citep{rt22023arxiv, open_x_embodiment_rt_x_2023, covariant_ai_2024, wayve_ai_2024} are \textit{closed}, with limited visibility into model architecture, training procedures, and data mixture, and 2) existing works do not provide best practices for \textit{deploying and adapting VLAs} to new robots, environments, and tasks --- especially on commodity hardware (e.g., consumer-grade GPUs).
We argue that to develop a rich foundation for future research and development, robotics needs open-source, generalist VLAs that support effective fine-tuning and adaptation, akin to the existing ecosystem around open-source language models \citep{wolf-etal-2020-transformers, touvron2023llama, jiang2023mistral, team2024gemma}.

To this end, we introduce \name{}, a \modelsize{}-parameter open-source VLA that establishes a new state of the art 
for generalist robot manipulation policies.\footnote{\name{} uses multiple pretrained model components: SigLIP~\citep{zhai2023siglip} and DinoV2~\citep{oquab2023dinov2} vision encoders and a Llama~2~\citep{touvron2023llama2} language model backbone. For all three models, weights are open, but not their training data or code. We release training data, code and model weights for reproducing \name{} on top of these components.} \name{} consists of a pretrained visually-conditioned language model backbone that captures visual features at multiple granularities, fine-tuned on a large, diverse dataset of \nepisodes{}~robot manipulation trajectories from the Open-X Embodiment \citep{open_x_embodiment_rt_x_2023} dataset --- a dataset that spans a wide range of robot embodiments, tasks, and scenes. As a product of increased data diversity and new model components, \name{} outperforms the 55B-parameter RT-2-X model \citep{rt22023arxiv, open_x_embodiment_rt_x_2023}, the prior state-of-the-art VLA, by 16.5\% absolute success rate across 29 evaluation tasks on the WidowX and Google Robot embodiments. We additionally investigate \textit{efficient fine-tuning strategies for VLAs}, a new contribution not explored in prior work, across 7 diverse manipulation tasks spanning behaviors from object pick-and-place to cleaning a table. We find that fine-tuned \name{} policies clearly outperform fine-tuned pretrained policies such as Octo~\citep{octo_2023}. Compared to from-scratch imitation learning with diffusion policies \citep{chi2023diffusionpolicy}, fine-tuned \name{} shows substantial improvement on tasks involving grounding language to behavior in multi-task settings with multiple objects. Following these results, we are the first to demonstrate the effectiveness of compute-efficient fine-tuning methods leveraging low-rank adaptation \citep[LoRA;][]{hu2021lora} and model quantization \citep{dettmers2024qlora} to facilitate adapting \name{} models on consumer-grade GPUs instead of large server nodes without compromising performance.
As a final contribution, we open-source all models, deployment and fine-tuning notebooks, and the \name{} codebase for training VLAs at scale, with the hope that these resources enable future work exploring and adapting VLAs for robotics.



