\section{Related Work}
\label{sec:related_work}

\paragraph{Visually-Conditioned Language Models}
Visually-conditioned language models (VLMs), which are trained on Internet-scale data to generate natural language from input image(s) and language prompts, have been adopted for myriad applications from visual question answering~\citep{goyal2017making, hudson2019gqa, singh2019textvqa, bigham2010vizwiz} to object localization~\citep{kazemzadeh2014refcoco, yu2016refcoco}. One of the key advances fueling recent VLMs are model architectures that bridge features from pretrained vision encoders~\citep{radford2021clip, zhai2023siglip, oquab2023dinov2} with pretrained language models~\citep{touvron2023llama2, jiang2023mistral, google-gemma, microsoft-phi, bai2023qwen}, directly building on advances in both computer vision and natural language modelling to create powerful multimodal models. While early work explored various architectures for cross-attending between vision and language features~\citep{li2022blip, li2023blip2, dai2023instructblip, tan2019lxmert, laurencon2023obelics}, new open-source VLMs~\citep{liu2023llava, liu2023llavav15, chen2023pali3, karamcheti2024prismatic} have converged on a simpler ``patch-as-token'' approach, in which patch features from pretrained visual transformers are treated as tokens, and are then projected into the input space of a language model. This simplicity makes it easy to repurpose existing tools for training language models at scale for VLM training. We employ these tools in our work to scale VLA training, and specifically use VLMs from \citet{karamcheti2024prismatic} as our pretrained backbone, as they are trained from \textit{multi-resolution visual features}, fusing low-level spatial information from DINOv2 \citep{oquab2023dinov2} with higher-level semantics from SigLIP \citep{zhai2023siglip} to aid in visual generalization.

\paragraph{Generalist Robot Policies}
A recent trend in robotics works towards training multi-task ``generalist'' robot policies~\citep{kalashnikov2018qt,mtopt2021arxiv,ebert2021bridge,walke2023bridgedata,rt12022arxiv,ehsani2023imitating,bharadhwaj2023roboagent} on large diverse robot datasets~\citep{pinto2016supersizing,mandlekar2018roboturk,kalashnikov2018qt, gupta2018robot,dasari2019robonet,cabi2019, jang2022bc,walke2023bridgedata,rt12022arxiv, fang2023rh20t,bharadhwaj2023roboagent,khazatsky2024droid, open_x_embodiment_rt_x_2023}, spanning many different robot embodiments~\citep{devin2017learning,dasari2019robonet, hu2022know,yang2023polybot,reed2022a,salhotra2023bridging,radosavovic2023robot, shahGNMGeneralNavigation2023,bousmalis2023robocat,shah2023vint,open_x_embodiment_rt_x_2023,octo_2023,yang2024pushing}. Notably, Octo~\citep{octo_2023} trains a generalist policy that can control multiple robots out-of-the-box and allows for flexible fine-tuning to new robot setups. A key difference between these approaches and \name{} is the model architecture. Prior works like Octo typically compose pretrained components such as language embeddings or visual encoders with additional model components initialized from scratch~\citep{walke2023bridgedata, rt12022arxiv, octo_2023}, learning to ``stitch'' them together during the course of policy training. 
Unlike these works, \name{} adopts a more end-to-end approach, directly fine-tuning VLMs to generate robot actions by treating them as tokens in the language model vocabulary. Our experimental evaluation shows that this simple yet scalable pipeline substantially boosts performance and generalization ability over prior generalist policies. 


\paragraph{Vision-Language-Action Models}
A number of works have explored the use of VLMs for robotics, \eg for visual state representations~\citep{nair2022r3m, Karamcheti2023LanguageDrivenRL}, object detection~\citep{gadre2023cows}, high-level planning~\citep{driess2023palm}, and for providing a feedback signal~\citep{du2023vision, ma2023liv, zhang2023grounding, sontakke2024roboclip}. Others integrate VLMs directly into end-to-end visuomotor manipulation policies~\citep{shridhar2022cliport, stone2023open}, but incorporate significant structure into the policy architecture or require calibrated cameras, which limits their applicability. A number of recent works have explored similar recipes to ours and directly fine-tuned large pretrained VLMs for predicting robot actions~\citep{rt22023arxiv,open_x_embodiment_rt_x_2023,huang2023embodied,li2023vision,covariant_ai_2024,wayve_ai_2024,zhen20243dvla}. Such models are often referred to as vision-language-action models (VLAs), since they fuse robot control actions directly into VLM backbones. This has three key benefits: (1)~it performs alignment of pretrained vision and language components on a large, Internet-scale vision-language dataset, (2)~the use of a generic architecture, not custom-made for robot control, allows us to leverage the scalable infrastructure underlying modern VLM training~\citep{pytorch_amp,dao2023flashattention,zhao2023pytorch} and scale to training billion-parameter policies with minimal code modifications, and (3)~it provides a direct pathway for robotics to benefit from the rapid improvements in VLMs. Existing works on VLAs either focus on training and evaluating in single robot or simulated setups~\citep{huang2023embodied,li2023vision, dorka2024matters,zhen20243dvla} and thus lack generality, or are closed and do not support efficient fine-tuning 
to new robot setups~\citep{rt22023arxiv,open_x_embodiment_rt_x_2023,covariant_ai_2024,wayve_ai_2024}. 

Most closely related, RT-2-X~\citep{open_x_embodiment_rt_x_2023} trains a 55B-parameter VLA policy on the Open X-Embodiment
dataset and demonstrates state-of-the-art generalist manipulation policy performance. However, our work differs from RT-2-X in multiple important aspects: (1)~by combining a strong open VLM backbone with a richer robot pretraining dataset, \name{} outperforms RT-2-X in our experiments while being an order of magnitude smaller; (2)~we thoroughly investigate fine-tuning of \name{} models to new target setups, while RT-2-X does not investigate the fine-tuning setting; (3)~we are the first to demonstrate the effectiveness of modern parameter-efficient fine-tuning and quantization approaches for VLAs; and (4)~\name{} is the first generalist VLA that is open-source and thus supports future research on VLA training, data mixtures, objectives, and inference.

