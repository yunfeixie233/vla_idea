
\section{Discussion and Limitations}
\label{sec:conclusion}

In this work, we presented \name{}, a state-of-the-art, open-source vision-language-action model that obtains strong performance for cross-embodiment robot control out-of-the-box. We also demonstrated that \name{} can be easily adapted to new robot setups via parameter-efficient fine-tuning techniques.

The current \name{} model has several limitations. First, it currently only supports single-image observations. In reality, real-world robot setups are heterogeneous, with a wide range of possible sensory inputs~\citep{octo_2023}. Expanding \name{} to support multiple image and proprioceptive inputs as well as observation history is an important avenue for future work. Exploring the use of VLMs pretrained on \emph{interleaved} image and text data may facilitate such flexible-input VLA fine-tuning.

Secondly, improving the inference throughput of \name{} is critical to enable VLA control for high-frequency control setups such as ALOHA~\citep{zhao2023learning}, which runs at 50Hz. This will also enable testing VLAs on more dexterous, bi-manual manipulation tasks than what we investigated in this work. Exploring the use of action chunking or alternative inference-time optimization techniques such as speculative decoding~\citep{leviathan2023fast} offer potential remedies.

Additionally, there is room for further performance improvements. While \name{} outperforms prior generalist policies, it does not yet offer very high reliability on the tested tasks, typically achieving <90\% success rate. 

Finally, due to compute limitations, many VLA design questions remain underexplored: What effect does the size of the base VLM have on VLA performance? Does co-training on robot action prediction data and Internet-scale vision-language data substantially improve VLA performance? What visual features are best-suited for VLA models? We hope that the release of the \name{} model and codebase will enable the community to jointly investigate these questions.




