\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\textwidth]{images/teaserv1.pdf}
    \caption{\textbf{We present \methodname{}, a unified vision-language-action model.} Unlike prior VLA approaches that typically rely on an extra vision encoder to extract image features and generate only action outputs, \methodname{} represents vision, language, and action as discrete tokens within a unified autoregressive framework. This unified modeling paradigm enables multi-modal outputs and supports a wide range of tasks—such as text-supervised perception grounding, vision-supervised world modeling, and action-supervised policy learning—within a single architecture. The unified token-based design further allows \methodname{} to effectively leverage large-scale multimodal data, particularly video, for scalable and generalizable learning. \methodname{} achieves new state-of-the-art results on CALVIN, LIBERO, and SimplerEnv-Bridge, significantly surpassing existing methods.}
    \label{fig:teaser}
\end{figure}