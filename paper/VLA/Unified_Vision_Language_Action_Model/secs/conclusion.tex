\section{Conclusion}
In this paper, we present \methodname{}, a unified framework for vision–language–action modeling that bridges heterogeneous modalities through a shared token space and models them autoregressively. The proposed unified design facilitates deeper cross-modal integration and inherently supports flexible multimodal tasks. By leveraging a world model trained to capture dynamics and causality from videos, we observe significant improvements in downstream policy learning, both in terms of performance and efficiency. Extensive simulation experiments further demonstrate the model’s strong generalization ability, efficient policy learning, and broad applicability across diverse domains.
These findings highlight the great potential of our method as a new paradigm for vision–language–action modeling.
\vspace{-2mm}
\paragraph{Limitations and Future Work.}
Due to limited computational resources, our investigation into post-training scalability is still in its early stages. Nonetheless, initial results are promising and indicate potential for scaling to larger video datasets. Furthermore, while the unified multimodal framework exhibits strong capabilities in cross-modal learning, further research is needed to fully integrate it with reinforcement learning paradigms, enabling more robust and adaptive policy learning.