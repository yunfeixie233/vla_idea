\section{Introduction}
% background
Developing agents capable of perceiving, reasoning, and acting in the physical world has long been a central objective of artificial intelligence. Recent advances in vision-language-action (VLA) models~\cite{brohan2023rt, octo_2023, kim2024openvla, black2024pi_0}, grounded in the powerful generalization capabilities of vision-language models (VLMs)~\cite{peng2023kosmos, jaech2024openai,beyer2024paligemma, wang2024qwen2,guo2025deepseek}, have demonstrated impressive performance across a wide range of robotic manipulation tasks, and are increasingly being adapted to generalist humanoid robots~\cite{bjorck2025gr00t, ding2025humanoid} that demand broader embodied intelligence.
However, most existing VLA approaches~\cite{kim2024openvla,black2024pi_0} follow a language-centric paradigm: visual observations are first projected into a semantic space, and action policies are subsequently derived based on these representations. This late-fusion strategy, while beneficial for semantic understanding and generalization, limits the formation of deeply coupled cross-modal representations and impedes the learning of temporal and causal dependencies across the perception-action loop.
This raises a central question: \emph{Can vision, language, and action be jointly modeled within a unified representation space to facilitate tighter cross-modal integration and more effective policy learning?}

% challenges
While appealing in theory, unified modeling presents two key challenges. First, vision, language, and action are inherently heterogeneous modalities: vision comprises high-dimensional, continuous spatial signals; language conveys abstract, discrete semantics; and actions involve temporally ordered sequences with causal dependencies. 
Second, the perception-to-action pipeline is inherently dynamic and causal, yet existing VLA models~\cite{brohan2023rt, kim2024openvla, black2024pi_0} often adopt static, language-centric paradigms that merely learn the mapping from static image to action. These models fail to capture the dynamic nature of real-world interactions, thereby limiting their ability to leverage the rich temporal information from videos for training.

% our method
To address the above challenges, we introduce \methodname{}, a novel framework for unified vision–language–action learning.
As illustrated in Figure~\ref{fig:teaser}, we propose a unified framework that supports both \emph{multimodal} and \emph{multi-task} learning. At the modality level, vision, language, and action signals are all transformed into discrete tokens and modeled using a shared vocabulary. This unified token representation allows for joint learning across modalities, fostering deeper cross-modal understanding and integration.
Building upon the unified framework, we adopt an autoregressive, Markov chain-based sequence modeling approach, where observations and actions are interleaved. This structure naturally incorporates causal dependencies, enabling the model to reason over temporal dynamics rather than treating perception and action as isolated tasks. By integrating the world model paradigm during training, we leverage large-scale robotics videos for self-supervised learning, allowing the model to capture environment dynamics in a temporally consistent and causally grounded manner. Remarkably, we find that post-training with world models significantly enhances policy learning, particularly for long-horizon and out-of-distribution tasks.

% our findings
Experiments across multiple simulation benchmarks, including CALVIN~\cite{mees2022calvin}, LIBERO~\cite{liu2023libero}, and SimplerEnv~\cite{li2024evaluating}, demonstrating clear performance improvements over existing methods.
Our model incorporates world model learning during post-training, enabling it to effectively capture visual dynamics from large-scale videos. This strategy significantly enhances both data efficiency and training efficiency in downstream policy learning, and allows for rapid adaptation to novel robotic tasks. 
Beyond policy learning, we demonstrate the model’s multimodal output capabilities, including spatial reasoning and visual prediction, highlighting its versatility. 
Furthermore, we extend our approach to autonomous driving scenarios for broader applicability. These results underscore the potential of our unified VLA model as an alternative and promising direction for generalist embodied intelligence. 

Our contributions are summarized as follows:
\begin{itemize}
    \item We propose \methodname{}, the first unified vision–language–action (VLA) model that encodes vision, language, and action as discrete tokens within a shared vocabulary, jointly modeling them through autoregressive sequence learning. This approach offers a novel architecture alternative to the existing VLA paradigm, facilitating more integrated cross-modal modeling and enabling large-scale video-based training.
    \item Our unified sequence modeling framework supports a broad range of multimodal tasks. By investigating various post-training strategies, we demonstrate that world models can effectively learn temporal dynamics from video data, substantially enhancing performance and improving both data and training efficiency in downstream policy learning—particularly in long-horizon and out-of-distribution scenarios.
    \item Our model achieves state-of-the-art performance on several simulated benchmarks (CALVIN, LIBERO, and SimplerEnv-Bridge) and introduces an open-source VLA method supporting large-scale video training. We further explore its capabilities across various modalities, including spatial reasoning and video prediction, and demonstrate its effective transfer to driving scenarios, highlighting its potential for generalist embodied intelligence.
\end{itemize}