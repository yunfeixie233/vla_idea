\section{Related Works}
\subsection{Vision-Language-Action Models}
Recent vision-language-action (VLA) models have demonstrated strong task performance across diverse robots and tasks~\cite{brohan2023rt,vuong2023open,driess2023palm, kim2024openvla,zhen20243d, cheang2024gr, black2024pi_0, zheng2024tracevla, liu2025hybridvla, kim2025fine, intelligence2025pi_}.
These models leverage pre-trained vision-language models (VLMs) to enhance understanding and generalization, further fine-tuned on large-scale robotic datasets for low-level control.
Currently, VLA models can be categorized into two paradigms based on their output space: \emph{pure action prediction} and \emph{visual-guided action prediction}.

\textbf{Pure action prediction.} 
Recent efforts have extended vision-language models (VLMs) to incorporate action modalities, enabling direct action prediction from visual and language inputs. A prominent example is RT-2\cite{brohan2023rt}, which learns from both internet-scale and robotic data to generate discrete actions autoregressively, showcasing strong generalization and semantic grounding. Building upon this, RT-H\cite{belkhale2024rt} introduces hierarchical actions to facilitate data sharing across tasks. OpenVLA\cite{kim2024openvla} scales this paradigm with a 7B-parameter open-source model trained on 970k real-world demonstrations spanning diverse datasets. To enhance spatial reasoning, SpatialVLA\cite{qu2025spatialvla} integrates spatial representations into the action modeling process.
Beyond architecture scaling, new action modeling techniques have also emerged. $\pi_0$~\cite{black2024pi_0} incorporates flow matching to improve action learning efficiency, while FAST ~\cite{pertsch2025fast} introduces a unified frequency-domain formulation for discretizing actions.

\textbf{Visual-guided action prediction.}
These studies leverage the power of visual pretraining, typically based on a policy-as-video formulation, by predicting future visual signals and subsequently decoding them into actions.
SuSIE~\cite{black2023zero} predicts key future frames and derives actions through inverse dynamics. UniPi~\cite{du2023learning} generates videos from text instructions, extracting actions from the frames. GR series~\cite{wu2023unleashing,cheang2024gr,li2025gr} leverages video pretraining for general policy learning. PAD~\cite{guo2024prediction} uses diffusion models to simultaneously learn future images and actions. LAPA~\cite{ye2024latent} proposes to learn latent actions between images with VQ-VAE from action-free internet-scale videos. Track2Act~\cite{bharadhwaj2024track2act} extracts point tracks from diverse web videos to guide the learning of interaction plans.

Both approaches have their strengths and weaknesses. The first, focused on action prediction, integrates well with Vision-Language Models (VLMs) but lacks spatial understanding and visual prediction capabilities. The second, incorporating visual generation, requires separating generative and action prediction models, limiting the full potential of VLMs. Our work unifies these approaches, combining video generation pretraining with the strengths of VLMs to propose a native multimodal model with significant future potential.

\subsection{World Models for Robotics}
World models~\cite{ha2018world, hafner2019dream, lecun2022path} have gained widespread attention for their ability to capture and reason about the dynamics of the physical world. 
They have emerged as a cornerstone in a range of domains, including interactive video generation~\cite{ bruce2024genie, che2024gamegen}, autonomous driving~\cite{hu2023gaia, wang2024driving, wang2024drivedreamer, gao2024vista}, and robotics~\cite{du2023learning, wu2023daydreamer, yang2023learning}.
Recent advances in robotics increasingly focus on general-purpose controllable video generation to simulate realistic and diverse robot-environment interactions. Visual Foresight~\cite{finn2017deep} leverages action-conditioned video prediction with model-predictive control, enabling robots to plan manipulation tasks by forecasting future visual observations. UniSim~\cite{yang2023learning} builds a “universal simulator” trained on diverse visual datasets, capable of visualizing the effects of both high-level instructions (e.g., “open the drawer”) and low-level controls in novel scenes. RoboDreamer~\cite{zhou2024robodreamer} learns a compositional world model by factorizing video generation, facilitating the synthesis of novel action sequences. DREMA~\cite{barcellona2024dream} replicates scene dynamics and structure by integrating Gaussian Splatting with physics simulation. VLP~\cite{du2023video} enables long-horizon visual planning by combining text-to-video generation with vision-language models as heuristic evaluators. DayDreamer~\cite{wu2023daydreamer} extends Dreamer~\cite{hafner2019learning} to real-world robotic platforms, while UVA~\cite{li2025unified} proposes a joint video-action latent space to decouple video and action generation, achieving high accuracy and efficiency in policy inference. AdaWorld~\cite{gao2025adaworld} extracts latent actions from videos in a self-supervised manner and builds an autoregressive world model conditioned on these latent actions.


