VLA,,,,
model,link,libero inference support,vision_encoder,training data
openvla,https://github.com/openvla/openvla,yes,DINOv2 + SigLIP (fused 600M-param Prismatic VLM backbone),"Open X-Embodiment (970k episodes, ~27 datasets): Fractal/RT-1 12.7%, Kuka 12.7%, Bridge/BridgeV2 13.3%, BC-Z 7.5%, FMB 7.1%, Language Table 4.4%, Stanford Hydra 4.4%, Taco Play 3.0%, Roboturk 2.3%, Austin Sailor 2.2%, UTAustin Mutex 2.2%, Toto 2.0%, Austin Sirius 1.7%, DobbE 1.4%, Berkeley Autolab UR5 1.2%, IAMLab CMU Pickup Insert 0.9%, Viola 0.9%, NYU Franka Play 0.8%, Berkeley Fanuc 0.7%, Jaco Play 0.4%, Furniture Bench 2.4%, Austin Buds 0.2%, Berkeley Cable Routing 0.2%, CMU Stretch 0.2%, UCSD Kitchen <0.1%, DLR EDAN <0.1%, DROID 10% (removed last 1/3 of training)"
NORA,https://declare-lab.github.io/nora,yes (fine-tuned),Qwen-2.5-VL-3B native vision encoder,"Open X-Embodiment (970k real-world robot demos, same OXE mixture as OpenVLA including BridgeV2 and DROID subsets). 3B-param model trained ~3 weeks on 8xH100 (~4000 H100-GPU-hours), 1.1M gradient updates, batch size 256. Uses FAST+ action tokenizer."
UniVLA,https://robertwyq.github.io/univla.github.io,yes (fine-tuned),"Encoder-free: VQ tokenizer (from Emu3) with spatial compression factor 8, no ViT. 8.5B-param autoregressive Transformer (Emu3 backbone).","Post-training: 622K robot-centric videos from RT-1 (84K), BridgeV2 (28K), DROID (145K), Kuka (100K), TOTO (899), Taco Play (3242), FMB (7876), Berkeley Autolab UR5 (896), VIOLA (135), CMU Play Fusion (576), UTAustin Mutex (1500), CALVIN sim (22966), LIBERO sim (3386), ManiSkill2 sim (193K), SSV2 (220K, video-only). Post-trained 50K steps on 32 A100s. Fine-tuning on CALVIN/LIBERO/SimplerEnv. Uses FAST action tokenizer."
pi0,https://physicalintelligence.company/blog/pi0,no,"PaliGemma VLM (SigLIP ViT image encoder, Gemma 2B LLM). 3B VLM backbone + 300M action expert = 3.3B total.","~10000 hours of robot data. Pre-training mixture: 9.1% open-source (OXE, Bridge V2, DROID) + 90.9% proprietary dexterous manipulation data (903M timesteps: 106M single-arm, 797M dual-arm). 7 robot configurations, 68 tasks. Robots: UR5e, Bimanual UR5e, Franka, Bimanual Trossen, Bimanual ARX/AgileX, Mobile Trossen/ARX, Mobile Fibocom. Pre-trained 700K steps then post-trained per task (5-100+ hours per task). Uses flow matching for continuous actions at up to 50Hz."
pi0.5,https://pi.website/blog/pi05,no,"PaliGemma VLM (SigLIP ViT, Gemma 2B). Same 2B VLM backbone + 300M action expert as pi0.","Multi-source co-training: (MM) ~400 hours mobile manipulator data from ~100 homes; (ME) non-mobile multi-environment robot data from diverse homes; (CE) cross-embodiment lab data (extended OXE dataset from pi0); (HL) high-level subtask prediction annotations; (WD) web data (CapsFusion, COCO, Cambrian-7M, PixMo, VQAv2, object localization); (VI) verbal instruction demos (post-training only). Pre-trained 280K steps (discrete tokens) then post-trained 80K steps (adds flow matching action expert). 97.6% of pre-training examples are NOT from mobile manipulators."
VA,,,,
model,link,libero inference support,vision_encoder,training data
LingBot-VA,https://github.com/robbyant/lingbot-va,yes (fine-tuned),"Wan2.2 Causal VAE (4x16x16 compression) for video encoding + T5 text encoder (frozen) for instructions. Dual-stream MoT architecture: video stream (Wan2.2-5B, d=3072, 30 layers) + action stream (d=768, 30 layers, ~350M params). Total 5.3B params.","~16K hours of robot manipulation data from 6 sources: Agibot (large-scale mobile manipulator), RoboMind (multi-embodiment), InternData-A1 (sim-to-real), OXE/OpenVLA subset (multi-embodiment), UMI Data (human demos via universal manipulation interface), RoboCOIN (cross-embodiment bimanual). Plus internally collected demos. Pre-trained on 1.4T tokens. Post-trained per task with as few as 50 real-world demos. Uses flow matching for both video and action prediction."
